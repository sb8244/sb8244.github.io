<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Stephen Bussey's Software Engineering Blog</title>
  <id>https://stephenbussey.com</id>
  <link href="https://stephenbussey.com"/>
  <link href="https://stephenbussey.com/feed.xml" rel="self"/>
  <updated>2019-10-01T00:26:00-04:00</updated>
  <author>
    <name>Stephen Bussey</name>
  </author>
  <entry>
    <title>Improve Exq Writes With Pooling</title>
    <link rel="alternate" href="https://stephenbussey.com/2019/10/01/improve-exq-writes-with-pooling.html"/>
    <id>https://stephenbussey.com/2019/10/01/improve-exq-writes-with-pooling.html</id>
    <published>2019-10-01T00:26:00-04:00</published>
    <updated>2019-10-01T01:57:19-04:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;&lt;a href="https://hex.pm/packages/exq" target="_blank"&gt;Exq&lt;/a&gt; is a background job processing library written in Elixir. It uses Redis, via the
Redix library, to store and then retrieve jobs. In this post, we&amp;rsquo;ll look at the performance of writing jobs into Redis
via the &lt;code&gt;Exq.Enqueuer&lt;/code&gt; API. You&amp;rsquo;ll see several benchmarks that utilize a single &lt;code&gt;Enqueuer&lt;/code&gt;, a poolboy queue, and a named
process pool.&lt;/p&gt;

&lt;p&gt;The repo for the benchmark and sample application is at &lt;a href="https://github.com/sb8244/exq-throughput" target="_blank"&gt;https://github.com/sb8244/exq-throughput&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;The Problem&lt;/h2&gt;

&lt;p&gt;Background job processing libraries write their jobs into a persistent storage mechanism and then retrieve those jobs
in the future. If you&amp;rsquo;ve used Ruby, you may be familiar with Sidekiq. The act of writing to Redis is very fast, but there
can be overhead at multiple levels. If the overhead is too high, then writing jobs to Redis becomes slow and the application
may become backed up. This can lead to errors or even a loss of service, if acknowledged persistence of a job is required.&lt;/p&gt;

&lt;h2&gt;Types of Overhead&lt;/h2&gt;

&lt;p&gt;The most common overhead that I&amp;rsquo;ve seen is the backup of Redis commands being executed end-to-end serially. This happens
when you use a single connection to write to Redis, and can occur in any language. The issue arises because a single connection can only
send one command at a time. It must then wait for the response before another command can occur. Redis is single-threaded, so it may not
be obvious why this is an issue. The problem is that the network overhead is done serially in this type of system—each write has to
go over the network and back before the next starts.&lt;/p&gt;

&lt;p&gt;The following diagram shows the speed of three hypothetical Redis requests:&lt;/p&gt;

&lt;div style="text-align: center"&gt;
  &lt;img src="/images/ExqPool/redis_serialization.svg"
       alt="Redis single connection versus pooled connection. Pooled connection completes 3 requests much faster."
       height="375px" /&gt;
  &lt;p&gt;
    &lt;small&gt;&lt;i&gt;Redis single connection versus pooled connection. Pooled connection completes 3 requests much faster.&lt;/i&gt;&lt;/small&gt;
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Each connection sends a command that goes over the network to Redis, which processes the command. A response is returned
and also goes over the network. In the real-world, this network latency might be 1ms or less. However, the end result is that
the requests complete much faster when multiple commands can be simultaneously sent via multiple connections.&lt;/p&gt;

&lt;p&gt;Another type of overhead is the fact that an Elixir process handles messages serially. If a job is enqueued via a single
process, the same problem as a single connection emerges.&lt;/p&gt;

&lt;h2&gt;The Problem in Exq&lt;/h2&gt;

&lt;p&gt;Exq enqueues jobs through the &lt;code&gt;Exq.Enqueuer&lt;/code&gt; process. This is a single process that holds a single redis connection. Each enqueue
task goes through this one process, serially. If serial processes and single connections lead to less throughput, then this is
will limit the throughput of Exq enqueueing.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s move into what we can do about it, and then benchmarks.&lt;/p&gt;

&lt;h2&gt;Pool Processes to Increase Throughput&lt;/h2&gt;

&lt;p&gt;The solution to the problem above is to pool processes, so that multiple Redis commands can be sent to Redis in the same moment
of time. There are two main ways that I&amp;rsquo;ve done this in Elixir: poolboy and named pools.&lt;/p&gt;

&lt;h4&gt;Poolboy&lt;/h4&gt;

&lt;p&gt;&lt;a href="https://github.com/devinus/poolboy" target="_blank"&gt;Poolboy&lt;/a&gt; is a nifty Erlang library that can create a pool of any process you want. We could
pool &lt;code&gt;Exq.Enqueuer&lt;/code&gt; processes and then enqueue jobs by using the poolboy functions. Let&amp;rsquo;s see how we&amp;rsquo;d do that:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;defmodule ExqThroughput.Application do
  use Application

  def start(_type, _args) do
    children =
      [
        :poolboy.child_spec(:worker, poolboy_config())
      ]

    opts = [strategy: :one_for_one, name: ExqThroughput.Supervisor]
    Supervisor.start_link(children, opts)
  end

  def enqueuer_pool_size(), do: :erlang.system_info(:schedulers_online)

  def poolboy_config() do
    [
      {:name, {:local, :enqueuer}},
      {:worker_module, ExqThroughput.PooledEnqueuer},
      {:size, enqueuer_pool_size()}
    ]
  end
end

defmodule ExqThroughput.PooledEnqueuer do
  def start_link(_) do
    # Hack to make Exq happy with running
    num = :rand.uniform(100_000_000) + 100
    name = :&amp;quot;Elixir.Exq#{num}&amp;quot;
    Exq.Enqueuer.start_link(name: name)

    # We need to put the enqueuer instance into the pool
    {:ok, Process.whereis(:&amp;quot;#{name}.Enqueuer&amp;quot;)}
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There is a bit of a hack in the &lt;code&gt;PooledEnqueuer&lt;/code&gt; module to make Exq happy. There may be another way to get around this, but I went
for a quick solution for the purpose of this benchmark. There is also a bit of working around the Exq process tree to get access
directly to the Enqueuer process.&lt;/p&gt;

&lt;p&gt;We can now enqueue a job by first checking out the poolboy process:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;:poolboy.transaction(:enqueuer, fn pid -&amp;gt;
  Exq.enqueue(pid, &amp;quot;throughput_queue&amp;quot;, Worker, [])
end)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Named process pooling looks a bit different than this.&lt;/p&gt;

&lt;h4&gt;Named Processes&lt;/h4&gt;

&lt;p&gt;You can start multiple processes in Elixir and give them a name like &lt;code&gt;MyProcess1&lt;/code&gt;, &lt;code&gt;MyProcess2&lt;/code&gt;, etc. When you want to send a
message to the process, you would send a message to &lt;code&gt;:&amp;quot;Elixir.MyProcess#{:rand.uniform(2)}&amp;quot;&lt;/code&gt;. This is named process pooling, and is
conceptually very simple—this makes it easier to setup.&lt;/p&gt;

&lt;p&gt;We have to start the pool of processes in the application:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;defmodule ExqThroughput.Application do
  use Application

  def start(_type, _args) do
    children = named_enqueuer_pool(enqueuer_pool_size())
    opts = [strategy: :one_for_one, name: ExqThroughput.Supervisor]
    Supervisor.start_link(children, opts)
  end

  def enqueuer_pool_size(), do: :erlang.system_info(:schedulers_online)

  defp named_enqueuer_pool(count) do
    for i &amp;lt;- 1..count do
      name = :&amp;quot;Elixir.Exq#{i}&amp;quot;

      %{
        id: name,
        start: {Exq.Enqueuer, :start_link, [[name: name]]}
      }
    end
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can then enqueue work by directly using these processes:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;def named_enqueue() do
  num = :rand.uniform(ExqThroughput.Application.enqueuer_pool_size())
  Exq.enqueue(:&amp;quot;Elixir.Exq#{num}.Enqueuer&amp;quot;, &amp;quot;throughput_queue&amp;quot;, Worker, [])
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I love this approach due to its simplicity. Let&amp;rsquo;s see how all of the approaches stack up.&lt;/p&gt;

&lt;h2&gt;Benchmark&lt;/h2&gt;

&lt;p&gt;Benchee is used to benchmark three scenarios: single process, poolboy, named processes. Benchee is ran with various
parallelism amounts to simulate how you might run Exq in production. For example, if you are enqueueing from a web tier,
then your parallelism will be quite high. If you&amp;rsquo;re enqueueing from a single process, you would have no parallelism.&lt;/p&gt;

&lt;p&gt;The redis queues are cleaned up before/after each test. The Exq work processor is not running—this test is purely around
speed of enqueueing. These tests are all running locally, and Redis is not running through any type of virtualization. The performance
would be significantly different depending on how redis is setup and the network speed between your application and redis.&lt;/p&gt;

&lt;p&gt;When Benchee was run with a single runner, all of the approaches came out roughly the same. This is expected because we
won&amp;rsquo;t see parallelism benefits without multiple processes trying to enqueue.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Name                       ips        average  deviation         median         99th %
named enqueuer          9.05 K      110.52 μs    ±42.61%          99 μs         210 μs
poolboy enqueuer        8.73 K      114.51 μs    ±57.05%         102 μs         240 μs
default enqueuer        8.30 K      120.54 μs    ±51.87%         110 μs         249 μs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The difference with 6 parallel testers was quite different. We can see that the pool approaches have significantly higher
throughput:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;total ips is these numbers * 6
Name                       ips        average  deviation         median         99th %
poolboy enqueuer        4.40 K      227.14 μs    ±39.15%         216 μs         417 μs
named enqueuer          3.95 K      253.41 μs    ±45.96%         227 μs         605 μs
default enqueuer        1.05 K      954.02 μs    ±21.91%         951 μs     1446.13 μs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now for 12:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;total ips is these numbers * 12
Name                       ips        average  deviation         median         99th %
poolboy enqueuer        2.83 K      352.86 μs    ±26.97%         339 μs         655 μs
named enqueuer          2.78 K      359.24 μs    ±53.25%         302 μs        1004 μs
default enqueuer        0.84 K     1187.04 μs    ±21.96%        1121 μs     1882.19 μs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and 24:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;total ips is these numbers * 24
Name                       ips        average  deviation         median         99th %
named enqueuer          1.48 K      675.58 μs    ±66.26%      541.98 μs     2198.98 μs
poolboy enqueuer        1.06 K      942.92 μs    ±51.20%      845.98 μs     2470.98 μs
default enqueuer        0.34 K     2900.89 μs    ±19.05%     2765.98 μs     4482.25 μs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That one surprised me because the named enqueuer was significantly more performant. I tried it over 10 times and
consistently got the same results. The tests were run in different order each time.&lt;/p&gt;

&lt;p&gt;That disappeared for 48:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;total ips is these numbers * 48
Name                       ips        average  deviation         median         99th %
poolboy enqueuer        912.30        1.10 ms    ±30.56%        1.01 ms        2.35 ms
named enqueuer          896.40        1.12 ms    ±77.47%        0.86 ms        4.06 ms
default enqueuer        203.05        4.92 ms    ±18.66%        4.65 ms        8.84 ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Interpreting the Results&lt;/h2&gt;

&lt;p&gt;These results show, clearly, that pooling the &lt;code&gt;Exq.Enqueuer&lt;/code&gt; process significantly increases throughput. This might be
even more pronounced when Redis is accessed over the network.&lt;/p&gt;

&lt;p&gt;Each test increased the parallelism, and the gap between pooled and single got even larger. With 48 processes enqueueing jobs, the total
throughput per second is ~43,000 versus ~9,600. With 12 processes enqueueing jobs, the throughput per second is still ~33,000 versus
~10,000.&lt;/p&gt;

&lt;h2&gt;Action the Results&lt;/h2&gt;

&lt;p&gt;If you are using Exq in production, consider pooling the enqueuer processes to increase throughput capacity. You may also increase
your enqueue speeds even if you&amp;rsquo;re not at capacity. You can use any pooling approach you want, they are roughly the same and
have a substantial impact to throughput.&lt;/p&gt;

&lt;p&gt;Exq already has an open issue to discuss adding some type of parallelism to the enqueuer process. Thanks to my colleague Marco for
opening that issue and for letting me look at this problem with him.&lt;/p&gt;

&lt;h2&gt;The Book Plug&lt;/h2&gt;

&lt;p&gt;My book &amp;ldquo;Real-Time Phoenix: Build Highly Scalable Systems with Channels&amp;rdquo; is now in beta
through &lt;a href="http://bit.ly/rtp-exq" target="_blank"&gt;The Pragmatic Bookshelf&lt;/a&gt;. This book explores using Phoenix Channels, GenStage, and more to build
real-time applications in Elixir.&lt;/p&gt;

&lt;div style="text-align: center"&gt;
  &lt;a href="http://bit.ly/rtp-exq" target="_blank"&gt;
    &lt;img src="/images/sbsockets.jpg" alt="Real-Time Phoenix by The Pragmatic Bookshelf" height="300px" style="border: 1px solid #ccc" /&gt;
  &lt;/a&gt;
&lt;/div&gt;
</content>
  </entry>
  <entry>
    <title>Lessons Learned from Shipping PushEx</title>
    <link rel="alternate" href="https://stephenbussey.com/2019/08/30/lessons-learned-from-shipping-pushex.html"/>
    <id>https://stephenbussey.com/2019/08/30/lessons-learned-from-shipping-pushex.html</id>
    <published>2019-08-30T19:55:00-04:00</published>
    <updated>2019-08-30T21:46:28-04:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;It&amp;rsquo;s been a while since I announced the initial open-source release of PushEx. We had a few small challenges to solve
in order to roll it out, and then I let it bake for several months to ensure that it is stable in production. It has
been running for several months now at production scale, and I cut the first official release of it
on &lt;a href="https://hex.pm/packages/push_ex" target="_blank"&gt;hex.pm&lt;/a&gt; today. These are some of the challenges and lessons learned from running
it in production.&lt;/p&gt;

&lt;h2&gt;The project was a big success&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s start with the good news.
Our goal with the project was to replace our existing push provider, in order to have more control over an important
part of our tech stack. By &amp;ldquo;push provider&amp;rdquo;, I mean a service that sends data from servers to clients in real-time.
We needed to replace multiple JavaScript clients and we needed to ensure that the
pushes were getting queued up from our servers properly.&lt;/p&gt;

&lt;p&gt;This process went very well, due to careful planning and testing on our end. We ran both systems in parallel
for some time, and slowly ramped the number of pushes from 0 to 100%. Throughout this process, we pushed data to clients
but did not use it there. We became confident enough to consume the data on the clients through a slow rollout process.
At this point, the connections to our old provider dropped.&lt;/p&gt;

&lt;p&gt;We were able to avoid downtime or problems throughout this process by very carefully monitoring the application. We
stopped the rollout at the first sign of trouble, and then resumed once we understood the root cause.&lt;/p&gt;

&lt;h2&gt;The order of process shutdown matters&lt;/h2&gt;

&lt;p&gt;We encountered a large number of errors during the deployment process of the application. This never occurred during
normal operation, so it seemed a bit odd. The root cause of the errors was that the application would still receive
requests as it was shutting down. This caused messages to try to push to clients, but the processes involved in that
were already shut down. This can happen with a Supervisor structure like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[
  PushExWeb.Endpoint,
  PushEx.Pipeline
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the pipeline shuts down while the Endpoint is still online, web requests will encounter errors. A better layout
might look like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[
  PushEx.Pipeline,
  PushExWeb.Endpoint,
  PushExWeb.ConnectionDrainer
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to gain control over the shutdown process of PushEx, I had to make changes to the application supervision tree. I separated
the &lt;a href="https://github.com/pushex-project/pushex/blob/675c986acac5a06486d7b47e7b34e8289fda2d3b/lib/push_ex/application.ex#L10" target="_blank"&gt;starting of PushEx&amp;rsquo;s core system&lt;/a&gt;
from the &lt;a href="https://github.com/pushex-project/pushex/blob/675c986acac5a06486d7b47e7b34e8289fda2d3b/lib/push_ex/supervisor.ex#L11" target="_blank"&gt;web portion&lt;/a&gt;.
These changes allow the entire web stack to be gracefully go offline (complete with connection draining) before the data
pipeline goes offline.&lt;/p&gt;

&lt;p&gt;This leads into the next problem&amp;mdash;how to prevent data loss during application shutdown.&lt;/p&gt;

&lt;h2&gt;Connection draining is critical&lt;/h2&gt;

&lt;p&gt;Connection draining allows a web server to gracefully wait for connections to close before it proceeds with shutting down more of the application.
HTTP connection draining shuts down the listener process that would accept new connections.&lt;/p&gt;

&lt;p&gt;Several layers of draining are required for PushEx to gracefully exit. They are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/pushex-project/pushex/blob/675c986acac5a06486d7b47e7b34e8289fda2d3b/lib/push_ex_web/channels/socket_drainer.ex" target="_blank"&gt;Socket draining&lt;/a&gt; (sockets should gracefully disconnect)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pushex-project/pushex/blob/675c986acac5a06486d7b47e7b34e8289fda2d3b/lib/push_ex/supervisor.ex#L23" target="_blank"&gt;Ranch connection draining&lt;/a&gt; (new web requests shouldn&amp;rsquo;t be handled)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pushex-project/pushex/blob/675c986acac5a06486d7b47e7b34e8289fda2d3b/lib/push_ex/push/drainer.ex" target="_blank"&gt;Push Pipeline draining&lt;/a&gt; (data in
flight should be given a chance of delivery)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You&amp;rsquo;ll notice that Socket draining is implemented separately from the connection draining. This is because the connection draining API stops
the listener from accepting new connections only. If a connection is already established, it has to be manually killed or shut down.&lt;/p&gt;

&lt;p&gt;This process was complex to get right, but now we can reboot servers without errors and without losing data. All of our data
drains out within the 30 second limit, so your mileage would vary if that&amp;rsquo;s not the case.&lt;/p&gt;

&lt;h2&gt;Big topics affect performance&lt;/h2&gt;

&lt;p&gt;We have a few large push topics in our application. It can be costly for Phoenix Tracker to have a large number of joins in a very short period of time.
The solution for PushEx is to allow certain topics to not use Tracker. When an ignored topic
is &lt;a href="https://github.com/pushex-project/pushex/blob/62efb769e6bb3472f4b4635e9e98a5f6f1c346f7/lib/push_ex_web/channels/push_tracker.ex#L32" target="_blank"&gt;tracked&lt;/a&gt;
or &lt;a href="https://github.com/pushex-project/pushex/blob/62efb769e6bb3472f4b4635e9e98a5f6f1c346f7/lib/push_ex_web/channels/push_tracker.ex#L43" target="_blank"&gt;questioned&lt;/a&gt;,
it always responds with &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The likelihood that a large topic has 0 connected clients is close to 0, so it is acceptable to treat it like it always has clients. We don&amp;rsquo;t
want to treat small topics as if they always have connected clients, or we&amp;rsquo;d do more work than necessary.&lt;/p&gt;

&lt;h2&gt;Keep-Alive to dramatically increase API throughput&lt;/h2&gt;

&lt;p&gt;I was a bit shocked when I implemented the server API calls in our Ruby app. We were used to 20ms calls to our old provider, but the new
one was hitting 200ms! We traced the root cause to DNS slowness when establishing the connection. The solution was to utilize Keep-Alive
connection pooling.&lt;/p&gt;

&lt;p&gt;The Keep-Alive header makes it so that the connection is not closed after a response is sent. The connection is open to accept more requests
and can do so without the overhead of establishing a connection. This dramatically increased the throughput of our Ruby servers to the
PushEx API endpoint. We didn&amp;rsquo;t quite hit the 20ms goal, but it was close enough to call it a win.&lt;/p&gt;

&lt;p&gt;I did hit a snag here. The connection draining for Keep-Alive connections suffers from the same problem as WebSockets do&amp;mdash;they
don&amp;rsquo;t close when the listener is closed. I had to do a bit of hackery to set a global value indicating that the Keep-Alive connection should
be &lt;a href="https://github.com/pushex-project/pushex/blob/675c986acac5a06486d7b47e7b34e8289fda2d3b/lib/push_ex_web/controllers/push_controller.ex#L32" target="_blank"&gt;closed on the next request&lt;/a&gt;.
This is a hack that I wish I didn&amp;rsquo;t have to do, but it did have the desired impact.&lt;/p&gt;

&lt;h2&gt;Wrapping Up&lt;/h2&gt;

&lt;p&gt;There were lots of challenges in rolling out the first major PushEx release, but the end result is solid. We&amp;rsquo;re running at pretty high throughput
on a low number of small servers (2GB + 2VCPU for this app) without issue.&lt;/p&gt;

&lt;p&gt;The snags we hit are not unique to our application. When building an Elixir application, you should consider both the startup and shutdown order
of your process tree. Use connection draining to avoid new connections being made to a server that is in the process of shutting down. Leverage
Keep-Alive headers for server-to-server API requests, especially if the throughput is high. These things do come with tradeoffs, however, so
your mileage may vary.&lt;/p&gt;

&lt;h2&gt;The Book Plug&lt;/h2&gt;

&lt;p&gt;My book &amp;ldquo;Real-Time Phoenix: Build Highly Scalable Systems with Channels&amp;rdquo; is now in beta
through &lt;a href="https://pragprog.com/book/sbsockets/real-time-phoenix" target="_blank"&gt;The Pragmatic Bookshelf&lt;/a&gt;. This book captures the victories and struggles that
we face when building real-time applications in Elixir.&lt;/p&gt;

&lt;div style="text-align: center"&gt;
  &lt;a href="https://pragprog.com/book/sbsockets/real-time-phoenix" target="_blank"&gt;
    &lt;img src="/images/sbsockets.jpg" alt="Real-Time Phoenix by The Pragmatic Bookshelf" height="300px" style="border: 1px solid #ccc" /&gt;
  &lt;/a&gt;
&lt;/div&gt;
</content>
  </entry>
  <entry>
    <title>Elixir Makes Testing Hard Things Easy</title>
    <link rel="alternate" href="https://stephenbussey.com/2019/06/08/elixir-makes-testing-hard-things-easy.html"/>
    <id>https://stephenbussey.com/2019/06/08/elixir-makes-testing-hard-things-easy.html</id>
    <published>2019-06-08T01:59:00-04:00</published>
    <updated>2019-07-02T13:07:46-04:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;I love Elixir. If we&amp;rsquo;ve talked at a conference or you work with me, you know this. I constantly find small nuggets
when working with Elixir that simply bring joy to me. One of these nuggets recently emerged that I thought worth
sharing.&lt;/p&gt;

&lt;h1&gt;Backstory&lt;/h1&gt;

&lt;p&gt;I&amp;rsquo;m working on some learning material right now for real-time system development. One of the key aspects of a section
is about how important measurement is. Without measurement, how do we have any confidence that our running system
is healthy?&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m using StatsD for this section because it&amp;rsquo;s fairly well-adopted and is conceptually simple to understand. However,
I kept bumping into small inefficiencies in my flow. An example of this is that I use a Node program called &lt;code&gt;statsd-logger&lt;/code&gt;
to view my metrics in development. It felt a bit strange to have a reader install this Node program to view logs in our
Elixir app.&lt;/p&gt;

&lt;p&gt;The same problem came up today when I was writing tests for the StatsD metrics. I would typically use Mockery to test that
Statix, my client, is correctly called. If I went down this path, I&amp;rsquo;d be putting a lot on the reader in terms of understanding
what the heck a mock is and when to use it. Ideally, we&amp;rsquo;ll have 1 line of code max additional setup.&lt;/p&gt;

&lt;p&gt;Okay, enough of the backstory. I wanted to fix these two very specific problems. Let&amp;rsquo;s get into the more interesting bits.&lt;/p&gt;

&lt;h1&gt;Elixir is Many Programs&lt;/h1&gt;

&lt;p&gt;Elixir runs processes in a way that makes them feel like little independent programs. We can spin up tens of thousands of these
programs without any issue at all. We can even spin up programs that we may consider expensive, such as a webserver.&lt;/p&gt;

&lt;p&gt;The real jewel of these many programs is that they are given a simple and efficient way to communicate with each other, message
passing. To take the web server example to the extreme, we could spin up 5 different servers on 5 ports and interface with them
through our browser. Behind the scenes, these &amp;ldquo;servers&amp;rdquo; would be in the same Elixir VM and could be passing messages back and forth
nearly freely.&lt;/p&gt;

&lt;p&gt;This is conceptually interesting, to me, because it means that traditionally complex or standalone applications can be run in the
same program (very easily) and can share information.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s see how this applies to StatsD.&lt;/p&gt;

&lt;h1&gt;Just run a Server&lt;/h1&gt;

&lt;p&gt;What if we just ran a StatsD server in our Elixir application? It could listen on a port and accept messages. We could do whatever
we wanted with these messages, like log them out or store them. Let&amp;rsquo;s do that in a few lines of code:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;$ iex
iex(1)&amp;gt; :gen_udp.open(8126, active: true)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;ve started a &amp;ldquo;UDP server&amp;rdquo; in our terminal. Let&amp;rsquo;s send it some messages (open new terminal):&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;# Your mileage may vary, fingers crossed
$ echo -n &amp;quot;hello:1&amp;quot; | nc -4u -w0 localhost 8126
$ echo -n &amp;quot;hello:2&amp;quot; | nc -4u -w0 localhost 8126
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s go back to the original iex terminal:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;iex(2)&amp;gt; flush()
{:udp, #Port&amp;lt;0.5&amp;gt;, {127, 0, 0, 1}, 61814, &amp;#39;hello:1&amp;#39;}
{:udp, #Port&amp;lt;0.5&amp;gt;, {127, 0, 0, 1}, 61515, &amp;#39;hello:2&amp;#39;}
:ok
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Woah! We received the UDP packets as messages to our process. That&amp;rsquo;s super neat to me because of how simple it was (due to
the great backing work of Erlang/OTP.)&lt;/p&gt;

&lt;p&gt;So what if this was the solution to problem 1 (statsd-logger)? We could start a UDP server and log out messages. I threw together
a quick library for that purpose called &lt;a href="https://github.com/sb8244/statsd_logger" target="_blank"&gt;StatsDLogger&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;That worked out nicely. What about testing, though? Let&amp;rsquo;s take what we just wrote and apply it to tests.&lt;/p&gt;

&lt;h1&gt;Test Anything Easily&lt;/h1&gt;

&lt;p&gt;Testing can sometimes be hard. Integration testing across multiple servers is usually hard. What level do we start to mock at? We can&amp;rsquo;t
actually run all of our services all of the time without having very slow tests. To help solve this, we can take what we just did and apply it to
tests. There is a key question though, how do we actually write an assertion against our server?&lt;/p&gt;

&lt;p&gt;In Elixir, message passing is champion. If you have a process&amp;rsquo;s ID, you can send it a message and it will go into that process&amp;rsquo;s mailbox.
With ExUnit, this mailbox can be asserted against by using &lt;code&gt;assert_receive&lt;/code&gt;. This means that if we can send our test process
a message, we can write a test for it.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s slightly harder to boil this concept down into an example as simple as the last. Let&amp;rsquo;s look at some code I wrote to
handle this for StatsD:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;  def handle(:valid, name, value, opts) do
    pid = Keyword.fetch!(opts, :send_to)
    send(pid, {:statsd_recv, name, value})
  end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can handle a metric by sending a message to a pid. Now the question is, &amp;ldquo;which pid?&amp;rdquo; The process that calls &lt;code&gt;start_link&lt;/code&gt;
can be referenced with &lt;code&gt;self()&lt;/code&gt;. This means we can do something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;  def start_link(opts) do
    opts = Keyword.merge([send_to: self()], opts)
    GenServer.start_link(__MODULE__, opts)
  end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can put all of this together to write a test like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;  test &amp;quot;valid / invalid messages are handled&amp;quot; do
    StatsDLogger.start_link(port: @port, formatter: :send)

    send_event(&amp;quot;a:1&amp;quot;)
    send_event(&amp;quot;a:2|c&amp;quot;)
    send_event(&amp;quot;invalid&amp;quot;)

    assert_receive {:statsd_recv, &amp;quot;a&amp;quot;, &amp;quot;1&amp;quot;}
    assert_receive {:statsd_recv, &amp;quot;a&amp;quot;, &amp;quot;2|c&amp;quot;}
    assert_receive {:statsd_recv_invalid, &amp;quot;invalid&amp;quot;}
  end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Elixir has made testing this multi-server integration a piece of cake.&lt;/p&gt;

&lt;h1&gt;In the Wild&lt;/h1&gt;

&lt;p&gt;I&amp;rsquo;m definitely not finding any new technique here. The reason I love Elixir is that gems like this are all over open-source
libraries. The one that comes to mind here is &lt;a href="https://github.com/PSPDFKit-labs/bypass" target="_blank"&gt;Bypass&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Bypass works by starting an anonymous HTTP server that runs a function when it&amp;rsquo;s invoked. That is a great example of &amp;ldquo;start a
server&amp;rdquo; to solve how to test that an application makes the right request.&lt;/p&gt;

&lt;p&gt;Phoenix Channels are a great example of &amp;ldquo;send a message&amp;rdquo; in the wild. When you write &lt;code&gt;assert_push&lt;/code&gt;, you&amp;rsquo;re actually just
&lt;a href="https://github.com/phoenixframework/phoenix/blob/master/lib/phoenix/test/channel_test.ex#L481" target="_blank"&gt;checking for a specific message&lt;/a&gt;.
The Channel test is wired up so that the test process becomes the &amp;ldquo;socket transport.&amp;rdquo; This allows it to be at the edge of the system
and capture a lot of test for very little code.&lt;/p&gt;

&lt;h1&gt;Wrapping Up&lt;/h1&gt;

&lt;p&gt;If you&amp;rsquo;re here, thanks for making it through. I love Elixir because of small things like this that would actually be huge in some
other languages I&amp;rsquo;ve worked with. If you&amp;rsquo;re looking for a way to test complex code, try out message passing as a potential solution.&lt;/p&gt;

&lt;h1&gt;Shameless Plug for ElixirConf&lt;/h1&gt;

&lt;p&gt;My colleague Grant and I are going to give a great training session at ElixirConf Tuesday course. You
can find more details at &lt;a href="https://elixirconf.com/2019/training-classes/2" target="_blank"&gt;https://elixirconf.com/2019/training-classes/2&lt;/a&gt;. We&amp;rsquo;ll
be focused on writing real-time systems, and will specifically be leveraging a lot of real world lessons from doing this at
decent scale on several pieces of our SaaS app. Come check it out!&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Useful Elixir Patterns from Real-world Side Project</title>
    <link rel="alternate" href="https://stephenbussey.com/2019/05/10/useful-elixir-patterns-from-real-world-side-project.html"/>
    <id>https://stephenbussey.com/2019/05/10/useful-elixir-patterns-from-real-world-side-project.html</id>
    <published>2019-05-10T13:00:00-04:00</published>
    <updated>2019-07-02T13:07:45-04:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;I believe that one of the best ways to push new practices is to work on a real-world project that we
can afford to experiment on. We can push the boundary in the toy project while still seeing the results
of the decisions in a production environment with real users. I&amp;rsquo;ve been fortunate to be able to ship
several applications like this at SalesLoft. The latest one that we&amp;rsquo;ll look at today is our internal OKR (Objective-Key Result) app that we use for goal tracking and alignment.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll walk through this project and see several different patterns that I like. Some of these patterns
have been used in several different projects of mine and have held up well. Others were new experiments
that I hope to use again in the future.&lt;/p&gt;

&lt;p&gt;The source code for the application can be found on &lt;a href="https://github.com/sb8244/okr_app_pub" target="_blank"&gt;Github&lt;/a&gt;. The useful
part is the code and tests, less so the specific functionality.&lt;/p&gt;

&lt;h1&gt;Context Usage&lt;/h1&gt;

&lt;p&gt;Contexts, at least as I use them, are all about establishing an application domain and keeping
the interfaces small and useful for other parts of the system to use. I&amp;rsquo;ve done this in the past but
really pushed it for this project by having no function usage other than top level modules. This
pattern allows us to have a well-defined interface for the application, which makes it simpler for both
us and others to modify the code and understand the consequences.&lt;/p&gt;

&lt;p&gt;As an example, the following would not be acceptable:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;MyApp.SomeContext.ASpecificQuery.execute(params)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Instead, I would prefer this:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;MyApp.SomeContext.a_query(params)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All of my contexts live under the &lt;code&gt;OkrApp&lt;/code&gt; module. You can see a list of them in the &lt;a href="https://github.com/sb8244/okr_app_pub/tree/master/lib/okr_app" target="_blank"&gt;okr_app folder&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I started off the application by writing code directly in the context. Some of this is still seen because
I didn&amp;rsquo;t go back through and change everything once I solidified on what I wanted. In the end, I found
that &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/analytics.ex#L5" target="_blank"&gt;function delegates&lt;/a&gt;
allowed the context to be very easy to digest and I could write tests for the underlying modules in distinct
files.&lt;/p&gt;

&lt;p&gt;The hardest part of this pattern, that I haven&amp;rsquo;t figured out yet, is how to handle Ecto schemas between
contexts. Sometimes, a context needs to leak out. For example, a &lt;code&gt;User&lt;/code&gt; schema may need to be referenced
by the &lt;code&gt;AnalyticsEvent&lt;/code&gt; schema (as seen &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/analytics/analytics_event.ex#L11" target="_blank"&gt;here&lt;/a&gt;).
I found this acceptable because I wasn&amp;rsquo;t invoking logic outside of the context interface.&lt;/p&gt;

&lt;p&gt;The advantage of Context came out when I built the &lt;a href="https://github.com/sb8244/okr_app_pub/tree/master/lib/okr_app/mailer" target="_blank"&gt;mailer&lt;/a&gt;.
I found it very simple to add in this new code without worrying about anything breaking. I was also to extract concepts
specific to the mailer (such as &lt;code&gt;Recipient&lt;/code&gt;) rather than relying on generic modules prone to change such as &lt;code&gt;User&lt;/code&gt;.&lt;/p&gt;

&lt;h1&gt;Isolated Logic (Context) for SCIM&lt;/h1&gt;

&lt;p&gt;One of the requirements for this project was SCIM (System for Cross-domain Identity Management). We use Okta internally and
they have a lot of docs on how to write a SCIM integration. I didn&amp;rsquo;t want the details of SCIM leaking into the system too
heavily. I was able to leverage a SCIM Context to achieve this goal in a way that I am pretty satisfied with.&lt;/p&gt;

&lt;p&gt;SCIM has all of its web functionality extracted into a module called &lt;code&gt;Scim.Web&lt;/code&gt;. &lt;code&gt;Phoenix.Router&lt;/code&gt; is used to provide the
appropriate endpoint definitions, but the integration happens in a simple &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/scim/web/plug.ex#L13" target="_blank"&gt;Plug&lt;/a&gt;
which is used by &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app_web/router.ex#L43" target="_blank"&gt;&lt;code&gt;OkrAppWeb.Router&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The entire &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/scim/web/users_controller.ex#L9" target="_blank"&gt;SCIM controller&lt;/a&gt; leverages
a behavior passed into it. This is the &lt;a href="https://en.wikipedia.org/wiki/Strategy_pattern" target="_blank"&gt;strategy pattern&lt;/a&gt; at work.
I didn&amp;rsquo;t create a specific &lt;code&gt;Behaviour&lt;/code&gt; requirement for this module but probably should have, because it has an expected
interface.&lt;/p&gt;

&lt;p&gt;The actual SCIM application integration happens in &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/users_scim.ex" target="_blank"&gt;UsersScim&lt;/a&gt;.
This code is a bit ugly since I didn&amp;rsquo;t clean it up too much, but it&amp;rsquo;s nice that the implementation doesn&amp;rsquo;t leak into my
application API nor does it leak into other contexts.&lt;/p&gt;

&lt;h1&gt;ListQuery Module&lt;/h1&gt;

&lt;p&gt;I really like Ecto&amp;rsquo;s API for querying data, but I have found it cumbersome to build up dynamic queries from API params easily.
I wrote a &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/query/list_query.ex" target="_blank"&gt;&lt;code&gt;ListQuery&lt;/code&gt;&lt;/a&gt; some time ago that I brought into this project.&lt;/p&gt;

&lt;p&gt;The ListQuery is used to take provided &lt;code&gt;params&lt;/code&gt; and &lt;code&gt;opts&lt;/code&gt; and turns it into an Ecto compatible query. Sometimes advanced
queries are required and I devised a small way to do that by stripping certain &lt;code&gt;params&lt;/code&gt; and then reapplying them. You can
see that &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/objectives/store/cycle_store.ex#L11" target="_blank"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m able to leverage a &lt;code&gt;ListQuery&lt;/code&gt; powered function in &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app_web/controllers/api/objective_link_controller.ex#L11" target="_blank"&gt;various controllers&lt;/a&gt;
by passing in the user params. If you use this pattern in an environment with non-friendly users, you should probably sanitize
the input to not leak any information to the client.&lt;/p&gt;

&lt;h1&gt;SimpleEctoStore Module&lt;/h1&gt;

&lt;p&gt;My contexts use &lt;code&gt;defdelegate&lt;/code&gt; to send queries to a different module. I called the module a &amp;ldquo;store&amp;rdquo; and found myself
&lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/objectives.ex#L13" target="_blank"&gt;delegating to them often&lt;/a&gt;.
Writing the same code again and again in the store became a bit cumbersome. Often, I would be writing the most
simple code possible with just the module name changed.&lt;/p&gt;

&lt;p&gt;My solution to this copy/paste problem was to build a macro powered &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/store/simple_ecto_store.ex" target="_blank"&gt;&lt;code&gt;SimpleEctoStore&lt;/code&gt;&lt;/a&gt;.
Each usage of &lt;code&gt;SimpleEctoStore&lt;/code&gt; involved passing in the schema module as well as what methods were desired to be
pulled in. You can see this &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/objectives/store/objective_link_store.ex#L2" target="_blank"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This allowed me to remove the boiler plate for a large number of ecto stores. If I used a concept like stores again,
I would definitely repeat this one.&lt;/p&gt;

&lt;h1&gt;Things that didn&amp;rsquo;t work as well&lt;/h1&gt;

&lt;p&gt;One of the biggest pain points I had was defining how I wanted my JSON API to work. At work, I follow the philosophy of very focused
endpoints that don&amp;rsquo;t preload many models together. This is for performance and flexibility reasons. Doing that in this project
made it much more complex for me and was going to take time that I didn&amp;rsquo;t want to spend. The end result here is that I
have a massive endpoint called &amp;ldquo;Okr&amp;rdquo; that &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app_web/serializers/okr.ex" target="_blank"&gt;embeds many associations&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The preload for my Okr endpoint is &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/objectives/okr_preloader.ex" target="_blank"&gt;pretty gnarly&lt;/a&gt; as
well. However, I was thrilled to be able to pass queries into the preload clauses which means that I removed the risk of
accidentally leaking data that is queried out of the system but then not handled properly by me in Elixir.&lt;/p&gt;

&lt;p&gt;Another thing that I still feel awkward about is a context&amp;rsquo;s schema referencing a different context&amp;rsquo;s schema. I wanted to
remove this but couldn&amp;rsquo;t figure out how to do so in a way I liked.&lt;/p&gt;

&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;Contexts allowed for a focused API between parts of an application. Using them saved me some mental gymnastics as I was
developing the application. I was particularly happy with how I was able to isolate the sort of whacky SCIM API via
focused context modules.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ListQuery&lt;/code&gt; and &lt;code&gt;SimpleEctoStore&lt;/code&gt; both made my life easier while defining my ecto based queries and stores. These are generically
applicable modules that could be brought into other applications if needed.&lt;/p&gt;

&lt;p&gt;I am really happy overall with how this project turned out. It is easy to read through nearly 6 months after I&amp;rsquo;ve originally
built it and has been running without any Elixir issues since that time. I&amp;rsquo;ll be applying these patterns to future projects,
for sure, and hope you are able to get some value from it!&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Thanks for reading! I&amp;rsquo;ll be teaching a class at ElixirConf US about building scalable real-time systems in Elixir. I&amp;rsquo;ve been
focused a lot on this topic so I&amp;rsquo;m pretty thrilled to share what I&amp;rsquo;ve learned. I believe that registration will open
soon.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Monolith to Microservice Without Downtime — A Production Story</title>
    <link rel="alternate" href="https://stephenbussey.com/2019/03/01/monolith-to-microservice-without-downtime-a-production-story.html"/>
    <id>https://stephenbussey.com/2019/03/01/monolith-to-microservice-without-downtime-a-production-story.html</id>
    <published>2019-03-01T17:35:00-05:00</published>
    <updated>2019-07-02T13:07:45-04:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;I am cross-posting for an article that I published on &lt;a href="https://medium.com/salesloft-engineering/monolith-to-microservice-without-downtime-a-production-story-652c9b82f03e" target="_blank"&gt;SalesLoft&amp;rsquo;s Medium blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Rather than copying the content over to this blog, I&amp;rsquo;m going to point you at the above Medium post. However, here are the lessons learned from the process:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Migrating from monolith to microservice is not easy. In fact, it involved a lot of nuance in the coding and planning that had to be a forethought and not an afterthought.&lt;/li&gt;
&lt;li&gt;Stability for customers is the most important part of a refactor like this; our product exists to serve our customers and not to be a microservice-powered entity.&lt;/li&gt;
&lt;li&gt;Finding the seam in the monolith is important when it comes to stability. If your new code, which shouldn’t be running until it’s ready, causes old requests to fail…that is a problem.&lt;/li&gt;
&lt;li&gt;The timeline was significantly longer than I anticipated. This is important because we have to prioritize refactoring old code against developing new features and fixing bugs. On the flip side…&lt;/li&gt;
&lt;li&gt;The slower timeline helped us ensure rock solid stability. We had zero stability concern on release day, which turned out to be like any other normal day.&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  <entry>
    <title>Distributed In-Memory Caching in Elixir</title>
    <link rel="alternate" href="https://stephenbussey.com/2019/01/29/distributed-in-memory-caching-in-elixir.html"/>
    <id>https://stephenbussey.com/2019/01/29/distributed-in-memory-caching-in-elixir.html</id>
    <published>2019-01-29T17:04:00-05:00</published>
    <updated>2019-07-02T13:07:46-04:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;I&amp;rsquo;ve been working with implementing our PushEx server at SalesLoft (a bit later than I thought I&amp;rsquo;d have time to) and one
of the challenges has been to map user identities in our authentication token to a legacy identity that we use in our
push channels. This is as simple as an HTTP call, but it is also something that could potentially burst very hard and cause
a large number of downstream HTTP calls. In order to help alleviate this, the identity -&amp;gt; secondary identity will be cached.&lt;/p&gt;

&lt;p&gt;I would typically just throw a cache in redis or memcached, but I really wanted to reach for something simpler in this situation.
My goals for this project are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;No external databases introduced (there are none currently on our PushEx implementation)&lt;/li&gt;
&lt;li&gt;Caching can be persistent between deploys (we roll our deploys so some pods are always online)&lt;/li&gt;
&lt;li&gt;Caching is conceptually simple in case it needs debugged by another engineer in the future&lt;/li&gt;
&lt;li&gt;Identity mappings will be stable between services, so time variations are not required. However:&lt;/li&gt;
&lt;li&gt;Caching will use a TTL to prevent really stale values from being used&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I&amp;rsquo;m going to walk through the different options I looked at and what I ended up on. The options I passed on aren&amp;rsquo;t necessarily bad,
they just ended up being operationally or conceptually more difficult than I needed.&lt;/p&gt;

&lt;h1&gt;Swarm&lt;/h1&gt;

&lt;p&gt;There was a great talk in the Atlanta Elixir Meetup recently about using Swarm to manage processes across a cluster. I really liked
a few things that the presentation demonstrated:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Processes are stable such that restarting a node with the same identity will put the process back on that node&lt;/li&gt;
&lt;li&gt;Processes are capable of handing themselves off to another node&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My initial plan was to put Cachex or another store in sharded processes that are distributed across the cluster. However, I soon
realized that passing off this ets state may be fairly complex and so I ended up looking at the cache as a simple in-process memory
map. That is actually fine with me, but some other things didn&amp;rsquo;t work out for it.&lt;/p&gt;

&lt;p&gt;The biggest issue that I ran into was that process hand-off became fairly complex to implement. I never actually got that working
properly after ~4 hours of work on it. Another bigger issue is that there is no process state replication. This means that my shard
values would have to pass themselves off fully to another node between the time that the node is told to shut down and the near future.
If the node was forcibly killed before then, the data would be lost and the cache would be re-populated.&lt;/p&gt;

&lt;p&gt;I ended up moving on from this to trying out the next solution because it seemed like I was getting myself into a solution I didn&amp;rsquo;t need.
That will be a theme in this blog post: there is nothing particularly wrong with technology X, but the trade-offs it brings may be more
than worth it for the particular use case I&amp;rsquo;m working with.&lt;/p&gt;

&lt;h1&gt;Delta CRDT&lt;/h1&gt;

&lt;p&gt;After talking with my colleague Dan about this topic (he will be presenting on distributed state at Lonestar Elixir Conf and it will be awesome),
he suggested looking at DeltaCRDT as a potential solution. I really liked this library for a few reasons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;State is replicated across the cluster, so shutdowns are not frantic&lt;/li&gt;
&lt;li&gt;CRDT gives a lot of benefit around time variation in the cluster (node A has a different value than node B at a point in time)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We ended up getting a working solution that used the DeltaCRDT library. Our code looked something like this (don&amp;rsquo;t use this code):&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;defmodule SalesloftPusher.AccountLookupCache.Monitor do
  use GenServer

  def start_link(_) do
    GenServer.start_link(__MODULE__, [], name: __MODULE__)
  end

  def init(_) do
    :net_kernel.monitor_nodes(true)

    {neighbors, []} = :rpc.multicall(Node.list, Process, :whereis, [AccountLookupCache])
    DeltaCrdt.add_neighbours(AccountLookupCache, neighbors)

    {:ok, []}
  end

  # Callbacks

  def handle_info({:nodeup, node}, state) do
    handle_info({:retrynodeup, node, 0}, state)
  end

  def handle_info({:retrynodeup, node, count}, state) do
    pid = :rpc.call(node, Process, :whereis, [AccountLookupCache])

    if pid == nil do
      IO.puts &amp;quot;Node is up, but app not booted, retry = #{count}&amp;quot;
      Process.send_after(self(), {:retrynodeup, node, count + 1}, 500)
    else
      IO.puts &amp;quot;Node is now up #{node} #{inspect(pid)}&amp;quot;
      DeltaCrdt.add_neighbours(AccountLookupCache, [pid])
    end

    {:noreply, state}
  end

  def handle_info({:nodedown, _node}, state) do
    {:noreply, state}
  end
end

usage:

DeltaCrdt.read(AccountLookupCache)
DeltaCrdt.mutate(AccountLookupCache, :add, [&amp;quot;1111/2&amp;quot;, &amp;quot;4&amp;quot;])
DeltaCrdt.read(AccountLookupCache)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a pretty slick library and the setup was fairly simple. This code may have some edge cases in it but we ran into
some performance issues with larger data sets and moved onto another solution. While we did end up moving on from it, the
author has been working hard on a refactor to improve the speed of the data structure. What&amp;rsquo;s he is doing is way beyond my
understanding of CRDTs and is pretty inspiring open-source work.&lt;/p&gt;

&lt;p&gt;This issue got me thinking and I realized that I didn&amp;rsquo;t need a lot of the benefits of the CRDT. I wanted replication across
the cluster, but my values are stable and so time variations won&amp;rsquo;t be a factor. I would most likely have stuck with DeltaCRDT
at this point if that was a factor, but I ended up moving onto my current solution.&lt;/p&gt;

&lt;h1&gt;Cachex + pg2 Replication&lt;/h1&gt;

&lt;p&gt;My final solution involves a tried and true solution around Cachex + pg2. I&amp;rsquo;ve &lt;a href="/2018/02/17/pg2-basics-use-process-groups-for-orchestration-across-a-cluster.html" target="_blank"&gt;written about pg2&lt;/a&gt;
in the past and have used it successfully in production on several projects. It essentially lets us
place our cache processes in a group across the cluster and reference the remote cache processes as a pid list.&lt;/p&gt;

&lt;p&gt;The solution presented below utilizes Cachex for all local set/get/stat, and passes messages containing
sets to the cluster using &lt;code&gt;send&lt;/code&gt;. When the cache process starts it notifies all neighbors in pg2
that it would like a dump of the state and then writes that into Cachex. Here are some strengths
and weaknesses of the solution:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;+ Boot based (rather than shutdown) based replication so that nodes do not lose data when they go down&lt;/li&gt;
&lt;li&gt;+ Cachex for local cache management (so we get all of that library&amp;rsquo;s benefits)&lt;/li&gt;
&lt;li&gt;+ Efficient writing and loading of an export. In testing it took less than 3 seconds for a 1,000,000 key cache locally (higher across network)&lt;/li&gt;
&lt;li&gt;- The entire cluster being down will cause cache data loss and an increase in misses&lt;/li&gt;
&lt;li&gt;- There is no consensus on what the right cache is, it&amp;rsquo;s best attempt&lt;/li&gt;
&lt;li&gt;- Possible flood of binary across network on boot with many nodes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The biggest disadvantage is the last one and I think it will be fixed before I take this code into production. It
is a purely effort based blocker (I need to write the code) and conceptually will work just fine.&lt;/p&gt;

&lt;h1&gt;The Code&lt;/h1&gt;

&lt;p&gt;The code is on a gist due to being fairly long: &lt;a href="https://gist.github.com/sb8244/371335946d444bd8c5786571cacef4d6" target="_blank"&gt;The Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The end result is a straightforward set/get interface which handles all of the distribution and caching.
I did a few basic performance tests of the system by throwing 10k, 50k, 500k, 1000k k/v
pairs into the cache and seeing how it performed. Writes and distribution were incredibly fast and rebooting
the application caused cache availability within a few seconds, well before the app would finish booting for
kubernetes health checks. There was one caveat I noticed which is that memory usage spiked while loading
the dumps from remote servers. I believe that the best solution here will involve me changing to a solution
that collects the size of each remote cache and selects the top 1 or 2 sized caches. That will prevent N
servers from sending full cache dumps and only 1 or 2.&lt;/p&gt;

&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;In summary, three different potential solutions were evaluated for this distributed caching problem. While
the first two options utilize great libraries and would be possible to build on, the trade-offs were too much
for the simplicity of my needs.&lt;/p&gt;

&lt;p&gt;When working on a software project like this, considering what your exact needs are is important and may actually
lead you away from the typical libraries into a different solution. It may seem obvious for many, but it is very
easy to get caught up in the libraries that we&amp;rsquo;re hearing about rather than what is best for our particular use case.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Thanks for reading! I&amp;rsquo;ll be speaking at &lt;a href="https://lonestarelixir.com/2019/" target="_blank"&gt;Lonestar ElixirConf&lt;/a&gt; about bringing Elixir to production,
looking at both human and tech challenges in doing so.&lt;/p&gt;
</content>
  </entry>
</feed>

<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Stephen Bussey's Software Engineering Blog</title>
  <id>https://stephenbussey.com</id>
  <link href="https://stephenbussey.com"/>
  <link href="https://stephenbussey.com/feed.xml" rel="self"/>
  <updated>2019-12-30T13:28:00-05:00</updated>
  <author>
    <name>Stephen Bussey</name>
  </author>
  <entry>
    <title>Verifying Queries with Ecto's prepare_query Callback</title>
    <link rel="alternate" href="https://stephenbussey.com/2019/12/30/verifying-queries-with-ecto-s-prepare-query.html"/>
    <id>https://stephenbussey.com/2019/12/30/verifying-queries-with-ecto-s-prepare-query.html</id>
    <published>2019-12-30T13:28:00-05:00</published>
    <updated>2020-04-30T21:25:00-04:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;You can use many different techniques to build and scale Software as a Service applications. One technique that is very popular is
to use a single database for multiple paying customers. This multi-tenant approach to SaaS works well for many people, but there are
a few dangers to look out for. The biggest danger is the risk of cross-tenant data leaking. I consider this the worst possible scenario
for a multi-tenant SaaS application, even beyond a full system outage.&lt;/p&gt;

&lt;p&gt;In this post, we&amp;rsquo;re going to look at a technique to guarantee that cross-tenant leaks don&amp;rsquo;t happen in an Elixir application. We&amp;rsquo;ll be
looking at Ecto&amp;rsquo;s new(ish) &lt;code&gt;prepare_query&lt;/code&gt; callback and how it can be used to inspect (almost) every query that goes through your
application. I&amp;rsquo;ll discuss how I test drove a query inspector to inspect every query for tenancy.&lt;/p&gt;

&lt;p&gt;The ultimate goal of this post is to serve as a light reference for how I navigated the &lt;code&gt;Ecto.Query&lt;/code&gt; struct to implement
the tenancy enforcer. The biggest challenge that I faced was understanding what went into the structure and how to walk it.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s first look at what &lt;code&gt;prepare_query&lt;/code&gt; is.&lt;/p&gt;

&lt;h2&gt;Ecto&amp;rsquo;s &lt;code&gt;prepare_query&lt;/code&gt; Callback&lt;/h2&gt;

&lt;p&gt;The &lt;a href="https://hexdocs.pm/ecto/Ecto.Repo.html#c:prepare_query/3" target="_blank"&gt;&lt;code&gt;prepare_query&lt;/code&gt;&lt;/a&gt; callback was introduced in September of 2019. You
can define a function (&lt;code&gt;prepare_query&lt;/code&gt;) in your &lt;code&gt;Application.Repo&lt;/code&gt; module. The function is invoked before a query is executed, and you
are provided the full query structure as well as some metadata.&lt;/p&gt;

&lt;p&gt;With &lt;code&gt;prepare_query&lt;/code&gt;, you can inspect the query or even modify it. The Ecto documentation gives an example where a Repo filters out
&amp;ldquo;soft deleted&amp;rdquo; records unless the user is an admin. It looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;# From https://hexdocs.pm/ecto/Ecto.Repo.html#c:prepare_query/3

@impl true
def prepare_query(_operation, query, opts) do
  if opts[:admin] do
    {query, opts}
  else
    query = from(x in query, where: is_nil(x.deleted_at))
    {query, opts}
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can use this to detect whether a query has tenancy set correctly. We won&amp;rsquo;t actually modify the query in this post, due to
concerns I have with how that approach removes multi-tenancy awareness.&lt;/p&gt;

&lt;p&gt;In order to get started, we&amp;rsquo;ll need to define a function that shows us the &lt;code&gt;Ecto.Query&lt;/code&gt; structure. Let&amp;rsquo;s do that next.&lt;/p&gt;

&lt;h2&gt;Understanding Ecto Query Structure from Inspect Protocol&lt;/h2&gt;

&lt;p&gt;In order to get started with a runnable example, I modified
&lt;a href="https://github.com/sb8244/ecto_tenancy_enforcer/blob/master/test/support/tenancy/repo.ex" target="_blank"&gt;&lt;code&gt;test/support/tenancy/repo.ex&lt;/code&gt;&lt;/a&gt;
by replacing &lt;code&gt;prepare_query&lt;/code&gt; with the following empty function.&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;  def prepare_query(_operation, query, opts) do
    IO.inspect query
    {query, opts}
  end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I can then run the test &lt;code&gt;&amp;quot;valid tenancy is only condition&amp;quot;&lt;/code&gt; with &lt;code&gt;mix test test/integration/prepare_test.exs:26&lt;/code&gt;, and see the following
output:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;➜  ecto_tenancy_enforcer git:(master) ✗ mix test test/integration/prepare_test.exs:26
#Ecto.Query&amp;lt;from s0 in &amp;quot;schema_migrations&amp;quot;, lock: &amp;quot;FOR UPDATE&amp;quot;,
 select: type(s0.version, :integer)&amp;gt;
Including tags: [line: &amp;quot;26&amp;quot;]
Excluding tags: [:test]

#Ecto.Query&amp;lt;from c0 in Tenancy.Company, where: c0.tenant_id == 1, select: c0&amp;gt;
.

Finished in 0.2 seconds
43 tests, 0 failures, 42 excluded
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s not quite helpful, because the &lt;code&gt;Ecto.Query&lt;/code&gt; is printed out in text form. We can add &lt;code&gt;structs: false&lt;/code&gt; to the &lt;code&gt;IO.inspect&lt;/code&gt; call
and we get a different result.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;%{
  __struct__: Ecto.Query,
  aliases: %{},
  assocs: [],
  combinations: [],
  distinct: nil,
  from: %{
    __struct__: Ecto.Query.FromExpr,
    as: nil,
    hints: [],
    prefix: nil,
    source: {&amp;quot;companies&amp;quot;, Tenancy.Company}
  },
  group_bys: [],
  havings: [],
  joins: [],
  limit: nil,
  lock: nil,
  offset: nil,
  order_bys: [],
  prefix: nil,
  preloads: [],
  select: %{
    __struct__: Ecto.Query.SelectExpr,
    expr: {:&amp;amp;, [], [0]},
    fields: nil,
    file: &amp;quot;/Users/stephenbussey/src/ecto_tenancy_enforcer/deps/ecto/lib/ecto/query/planner.ex&amp;quot;,
    line: 814,
    params: [],
    take: %{}
  },
  sources: nil,
  updates: [],
  wheres: [
    %{
      __struct__: Ecto.Query.BooleanExpr,
      expr: {:==, [],
       [
         {{:., [], [{:&amp;amp;, [], [0]}, :tenant_id]}, [], []},
         %{
           __struct__: Ecto.Query.Tagged,
           tag: nil,
           type: {0, :tenant_id},
           value: 1
         }
       ]},
      file: &amp;quot;/Users/stephenbussey/src/ecto_tenancy_enforcer/test/integration/prepare_test.exs&amp;quot;,
      line: 27,
      op: :and,
      params: []
    }
  ],
  windows: [],
  with_ctes: nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alright, now we have the actual struct that we can work with. We are capable of writing a query, inspecting the struct, and then figuring
out how to walk / enforce that query. If we continued with TDD at this point, we&amp;rsquo;d eventually hit a snag. &lt;code&gt;Ecto.Query&lt;/code&gt; represents
referenced tables (joins, some wheres) with a positional index system. However, the structure doesn&amp;rsquo;t include
the list of positional references&amp;mdash;you have to build it yourself.&lt;/p&gt;

&lt;p&gt;This led to a bit of a pickle, because it&amp;rsquo;s not documented anywhere and is considered an internal query structure. However, we know
of at least one place that knows how to resolve a positional index into a table / module name (Ecto.Query inspect). Let&amp;rsquo;s find that
and see what it&amp;rsquo;s doing.&lt;/p&gt;

&lt;p&gt;A search for &lt;code&gt;defimpl&lt;/code&gt; in Ecto brings us to &lt;a href="https://github.com/elixir-ecto/ecto/blob/b3ee240e91deecfd6d8727946bfc8ed5de752e4f/lib/ecto/query/inspect.ex#L21" target="_blank"&gt;this line&lt;/a&gt;,
which is the implementation of &lt;code&gt;Inspect&lt;/code&gt; for &lt;code&gt;Ecto.Query&lt;/code&gt;. In particular, the following code is of interest to us:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;  defp to_list(query) do
    names =
      query
      |&amp;gt; collect_sources
      |&amp;gt; generate_letters
      |&amp;gt; generate_names
      |&amp;gt; List.to_tuple()

    ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With this lead, it&amp;rsquo;s possible to use these same functions to create a positional index lookup. You can see a finished example of this
in &lt;a href="https://github.com/sb8244/ecto_tenancy_enforcer/blob/master/lib/ecto_tenancy_enforcer/source_collector.ex" target="_blank"&gt;&lt;code&gt;EctoTenancyEnforcer.SourceCollector&lt;/code&gt;&lt;/a&gt;. I modified this to return the schema module name, rather than a letter or table name, so that I could use my
&lt;code&gt;enforced_schemas&lt;/code&gt; option to check if a referenced table needs checked or not.&lt;/p&gt;

&lt;p&gt;We have about as much as we&amp;rsquo;re going to get out of Ecto. At this point, it&amp;rsquo;s possible to jump in and start coding a query verifier to
do whatever you want. I found myself still a bit lost at this point, but I did know what I wanted to pass / fail. I&amp;rsquo;ll walk through
the steps I took to test drive a query enforcer.&lt;/p&gt;

&lt;h2&gt;Test Driving EctoTenancyEnforcer&lt;/h2&gt;

&lt;p&gt;I don&amp;rsquo;t practice test driven development often, but I find it valuable when I have no clue what the solution is going to be, but I
know what I want it to look like. That is the case in a query enforcer, because it&amp;rsquo;s easy to write out queries that should pass and
fail into a test suite. I can then go one-by-one to make the incorrect ones fail, while still having the valid ones pass.&lt;/p&gt;

&lt;p&gt;I started with a set of very simple tests, like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;test &amp;quot;no filters at all&amp;quot; do
  assert_raise(TenancyViolation, fn -&amp;gt;
    Repo.all(Company)
  end)
end

test &amp;quot;valid tenancy is only condition&amp;quot; do
  valid = from c in Company, where: c.tenant_id == 1
  assert Repo.all(valid) |&amp;gt; length == 1
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I started with these to build some simple confidence. Once they were passing, I listed out about 40 queries that I knew should work
or not work. I chipped my way through these and eventually found the patterns in the &lt;code&gt;Ecto.Query&lt;/code&gt; struct that led to fairly clean code to
walk them.&lt;/p&gt;

&lt;p&gt;The final result can be seen in the &lt;a href="https://github.com/sb8244/ecto_tenancy_enforcer/blob/master/lib/ecto_tenancy_enforcer/query_verifier.ex" target="_blank"&gt;&lt;code&gt;EctoTenancyEnforcer.QueryVerifier&lt;/code&gt;&lt;/a&gt; module. I&amp;rsquo;m sure that there are cases I missed in my TDD, but I&amp;rsquo;m happy enough with this to
use it in production applications. I&amp;rsquo;ll add new tests as cases are encountered in the wild.&lt;/p&gt;

&lt;h2&gt;Wrapping Up&lt;/h2&gt;

&lt;p&gt;Ecto&amp;rsquo;s &lt;code&gt;prepare_query&lt;/code&gt; callback is incredibly powerful for query inspection and modification. It&amp;rsquo;s a bit dense to get started with,
due to the &lt;code&gt;Ecto.Query&lt;/code&gt; structure being undocumented, but TDD helped me out significantly. The &lt;code&gt;Ecto.Query&lt;/code&gt; walking is undocumented,
but I&amp;rsquo;m anticipating stability in the Query structure going into the future. That may or may not pan out, but I think it&amp;rsquo;s a decent
bet.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re looking for tenancy enforcement in Ecto queries, try out &lt;a href="https://github.com/sb8244/ecto_tenancy_enforcer" target="_blank"&gt;EctoTenancyEnforcer&lt;/a&gt;.
You can refer to this repo as a complete example of query enforcement.&lt;/p&gt;

&lt;h2&gt;The Book Plug&lt;/h2&gt;

&lt;p&gt;My book &amp;ldquo;Real-Time Phoenix: Build Highly Scalable Systems with Channels&amp;rdquo; is now in beta
through &lt;a href="http://bit.ly/rtp-exq" target="_blank"&gt;The Pragmatic Bookshelf&lt;/a&gt;. This book explores using Phoenix Channels, GenStage, and more to build
real-time applications in Elixir. The first draft has been completed for a little bit and the book should be in production by
February, with print coming at the end of the production process.&lt;/p&gt;

&lt;div style="text-align: center"&gt;
  &lt;a href="http://bit.ly/rtp-ecto-tenancy" target="_blank"&gt;
    &lt;img src="/images/sbsockets.jpg" alt="Real-Time Phoenix by The Pragmatic Bookshelf" height="300px" style="border: 1px solid #ccc" /&gt;
  &lt;/a&gt;
&lt;/div&gt;
</content>
  </entry>
  <entry>
    <title>Improve Exq Writes With Pooling</title>
    <link rel="alternate" href="https://stephenbussey.com/2019/10/01/improve-exq-writes-with-pooling.html"/>
    <id>https://stephenbussey.com/2019/10/01/improve-exq-writes-with-pooling.html</id>
    <published>2019-10-01T00:26:00-04:00</published>
    <updated>2020-04-30T21:25:00-04:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;&lt;a href="https://hex.pm/packages/exq" target="_blank"&gt;Exq&lt;/a&gt; is a background job processing library written in Elixir. It uses Redis, via the
Redix library, to store and then retrieve jobs. In this post, we&amp;rsquo;ll look at the performance of writing jobs into Redis
via the &lt;code&gt;Exq.Enqueuer&lt;/code&gt; API. You&amp;rsquo;ll see several benchmarks that utilize a single &lt;code&gt;Enqueuer&lt;/code&gt;, a poolboy queue, and a named
process pool.&lt;/p&gt;

&lt;p&gt;The repo for the benchmark and sample application is at &lt;a href="https://github.com/sb8244/exq-throughput" target="_blank"&gt;https://github.com/sb8244/exq-throughput&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;The Problem&lt;/h2&gt;

&lt;p&gt;Background job processing libraries write their jobs into a persistent storage mechanism and then retrieve those jobs
in the future. If you&amp;rsquo;ve used Ruby, you may be familiar with Sidekiq. The act of writing to Redis is very fast, but there
can be overhead at multiple levels. If the overhead is too high, then writing jobs to Redis becomes slow and the application
may become backed up. This can lead to errors or even a loss of service, if acknowledged persistence of a job is required.&lt;/p&gt;

&lt;h2&gt;Types of Overhead&lt;/h2&gt;

&lt;p&gt;The most common overhead that I&amp;rsquo;ve seen is the backup of Redis commands being executed end-to-end serially. This happens
when you use a single connection to write to Redis, and can occur in any language. The issue arises because a single connection can only
send one command at a time. It must then wait for the response before another command can occur. Redis is single-threaded, so it may not
be obvious why this is an issue. The problem is that the network overhead is done serially in this type of system—each write has to
go over the network and back before the next starts.&lt;/p&gt;

&lt;p&gt;The following diagram shows the speed of three hypothetical Redis requests:&lt;/p&gt;

&lt;div style="text-align: center"&gt;
  &lt;img src="/images/ExqPool/redis_serialization.svg"
       alt="Redis single connection versus pooled connection. Pooled connection completes 3 requests much faster."
       height="375px" /&gt;
  &lt;p&gt;
    &lt;small&gt;&lt;i&gt;Redis single connection versus pooled connection. Pooled connection completes 3 requests much faster.&lt;/i&gt;&lt;/small&gt;
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Each connection sends a command that goes over the network to Redis, which processes the command. A response is returned
and also goes over the network. In the real-world, this network latency might be 1ms or less. However, the end result is that
the requests complete much faster when multiple commands can be simultaneously sent via multiple connections.&lt;/p&gt;

&lt;p&gt;Another type of overhead is the fact that an Elixir process handles messages serially. If a job is enqueued via a single
process, the same problem as a single connection emerges.&lt;/p&gt;

&lt;h2&gt;The Problem in Exq&lt;/h2&gt;

&lt;p&gt;Exq enqueues jobs through the &lt;code&gt;Exq.Enqueuer&lt;/code&gt; process. This is a single process that holds a single redis connection. Each enqueue
task goes through this one process, serially. If serial processes and single connections lead to less throughput, then this is
will limit the throughput of Exq enqueueing.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s move into what we can do about it, and then benchmarks.&lt;/p&gt;

&lt;h2&gt;Pool Processes to Increase Throughput&lt;/h2&gt;

&lt;p&gt;The solution to the problem above is to pool processes, so that multiple Redis commands can be sent to Redis in the same moment
of time. There are two main ways that I&amp;rsquo;ve done this in Elixir: poolboy and named pools.&lt;/p&gt;

&lt;h4&gt;Poolboy&lt;/h4&gt;

&lt;p&gt;&lt;a href="https://github.com/devinus/poolboy" target="_blank"&gt;Poolboy&lt;/a&gt; is a nifty Erlang library that can create a pool of any process you want. We could
pool &lt;code&gt;Exq.Enqueuer&lt;/code&gt; processes and then enqueue jobs by using the poolboy functions. Let&amp;rsquo;s see how we&amp;rsquo;d do that:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;defmodule ExqThroughput.Application do
  use Application

  def start(_type, _args) do
    children =
      [
        :poolboy.child_spec(:worker, poolboy_config())
      ]

    opts = [strategy: :one_for_one, name: ExqThroughput.Supervisor]
    Supervisor.start_link(children, opts)
  end

  def enqueuer_pool_size(), do: :erlang.system_info(:schedulers_online)

  def poolboy_config() do
    [
      {:name, {:local, :enqueuer}},
      {:worker_module, ExqThroughput.PooledEnqueuer},
      {:size, enqueuer_pool_size()}
    ]
  end
end

defmodule ExqThroughput.PooledEnqueuer do
  def start_link(_) do
    # Hack to make Exq happy with running
    num = :rand.uniform(100_000_000) + 100
    name = :&amp;quot;Elixir.Exq#{num}&amp;quot;
    Exq.Enqueuer.start_link(name: name)

    # We need to put the enqueuer instance into the pool
    {:ok, Process.whereis(:&amp;quot;#{name}.Enqueuer&amp;quot;)}
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There is a bit of a hack in the &lt;code&gt;PooledEnqueuer&lt;/code&gt; module to make Exq happy. There may be another way to get around this, but I went
for a quick solution for the purpose of this benchmark. There is also a bit of working around the Exq process tree to get access
directly to the Enqueuer process.&lt;/p&gt;

&lt;p&gt;We can now enqueue a job by first checking out the poolboy process:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;:poolboy.transaction(:enqueuer, fn pid -&amp;gt;
  Exq.enqueue(pid, &amp;quot;throughput_queue&amp;quot;, Worker, [])
end)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Named process pooling looks a bit different than this.&lt;/p&gt;

&lt;h4&gt;Named Processes&lt;/h4&gt;

&lt;p&gt;You can start multiple processes in Elixir and give them a name like &lt;code&gt;MyProcess1&lt;/code&gt;, &lt;code&gt;MyProcess2&lt;/code&gt;, etc. When you want to send a
message to the process, you would send a message to &lt;code&gt;:&amp;quot;Elixir.MyProcess#{:rand.uniform(2)}&amp;quot;&lt;/code&gt;. This is named process pooling, and is
conceptually very simple—this makes it easier to setup.&lt;/p&gt;

&lt;p&gt;We have to start the pool of processes in the application:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;defmodule ExqThroughput.Application do
  use Application

  def start(_type, _args) do
    children = named_enqueuer_pool(enqueuer_pool_size())
    opts = [strategy: :one_for_one, name: ExqThroughput.Supervisor]
    Supervisor.start_link(children, opts)
  end

  def enqueuer_pool_size(), do: :erlang.system_info(:schedulers_online)

  defp named_enqueuer_pool(count) do
    for i &amp;lt;- 1..count do
      name = :&amp;quot;Elixir.Exq#{i}&amp;quot;

      %{
        id: name,
        start: {Exq.Enqueuer, :start_link, [[name: name]]}
      }
    end
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can then enqueue work by directly using these processes:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;def named_enqueue() do
  num = :rand.uniform(ExqThroughput.Application.enqueuer_pool_size())
  Exq.enqueue(:&amp;quot;Elixir.Exq#{num}.Enqueuer&amp;quot;, &amp;quot;throughput_queue&amp;quot;, Worker, [])
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I love this approach due to its simplicity. Let&amp;rsquo;s see how all of the approaches stack up.&lt;/p&gt;

&lt;h2&gt;Benchmark&lt;/h2&gt;

&lt;p&gt;Benchee is used to benchmark three scenarios: single process, poolboy, named processes. Benchee is ran with various
parallelism amounts to simulate how you might run Exq in production. For example, if you are enqueueing from a web tier,
then your parallelism will be quite high. If you&amp;rsquo;re enqueueing from a single process, you would have no parallelism.&lt;/p&gt;

&lt;p&gt;The redis queues are cleaned up before/after each test. The Exq work processor is not running—this test is purely around
speed of enqueueing. These tests are all running locally, and Redis is not running through any type of virtualization. The performance
would be significantly different depending on how redis is setup and the network speed between your application and redis.&lt;/p&gt;

&lt;p&gt;When Benchee was run with a single runner, all of the approaches came out roughly the same. This is expected because we
won&amp;rsquo;t see parallelism benefits without multiple processes trying to enqueue.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Name                       ips        average  deviation         median         99th %
named enqueuer          9.05 K      110.52 μs    ±42.61%          99 μs         210 μs
poolboy enqueuer        8.73 K      114.51 μs    ±57.05%         102 μs         240 μs
default enqueuer        8.30 K      120.54 μs    ±51.87%         110 μs         249 μs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The difference with 6 parallel testers was quite different. We can see that the pool approaches have significantly higher
throughput:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;total ips is these numbers * 6
Name                       ips        average  deviation         median         99th %
poolboy enqueuer        4.40 K      227.14 μs    ±39.15%         216 μs         417 μs
named enqueuer          3.95 K      253.41 μs    ±45.96%         227 μs         605 μs
default enqueuer        1.05 K      954.02 μs    ±21.91%         951 μs     1446.13 μs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now for 12:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;total ips is these numbers * 12
Name                       ips        average  deviation         median         99th %
poolboy enqueuer        2.83 K      352.86 μs    ±26.97%         339 μs         655 μs
named enqueuer          2.78 K      359.24 μs    ±53.25%         302 μs        1004 μs
default enqueuer        0.84 K     1187.04 μs    ±21.96%        1121 μs     1882.19 μs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and 24:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;total ips is these numbers * 24
Name                       ips        average  deviation         median         99th %
named enqueuer          1.48 K      675.58 μs    ±66.26%      541.98 μs     2198.98 μs
poolboy enqueuer        1.06 K      942.92 μs    ±51.20%      845.98 μs     2470.98 μs
default enqueuer        0.34 K     2900.89 μs    ±19.05%     2765.98 μs     4482.25 μs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That one surprised me because the named enqueuer was significantly more performant. I tried it over 10 times and
consistently got the same results. The tests were run in different order each time.&lt;/p&gt;

&lt;p&gt;That disappeared for 48:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;total ips is these numbers * 48
Name                       ips        average  deviation         median         99th %
poolboy enqueuer        912.30        1.10 ms    ±30.56%        1.01 ms        2.35 ms
named enqueuer          896.40        1.12 ms    ±77.47%        0.86 ms        4.06 ms
default enqueuer        203.05        4.92 ms    ±18.66%        4.65 ms        8.84 ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Interpreting the Results&lt;/h2&gt;

&lt;p&gt;These results show, clearly, that pooling the &lt;code&gt;Exq.Enqueuer&lt;/code&gt; process significantly increases throughput. This might be
even more pronounced when Redis is accessed over the network.&lt;/p&gt;

&lt;p&gt;Each test increased the parallelism, and the gap between pooled and single got even larger. With 48 processes enqueueing jobs, the total
throughput per second is ~43,000 versus ~9,600. With 12 processes enqueueing jobs, the throughput per second is still ~33,000 versus
~10,000.&lt;/p&gt;

&lt;h2&gt;Action the Results&lt;/h2&gt;

&lt;p&gt;If you are using Exq in production, consider pooling the enqueuer processes to increase throughput capacity. You may also increase
your enqueue speeds even if you&amp;rsquo;re not at capacity. You can use any pooling approach you want, they are roughly the same and
have a substantial impact to throughput.&lt;/p&gt;

&lt;p&gt;Exq already has an open issue to discuss adding some type of parallelism to the enqueuer process. Thanks to my colleague Marco for
opening that issue and for letting me look at this problem with him.&lt;/p&gt;

&lt;h2&gt;The Book Plug&lt;/h2&gt;

&lt;p&gt;My book &amp;ldquo;Real-Time Phoenix: Build Highly Scalable Systems with Channels&amp;rdquo; is now in beta
through &lt;a href="http://bit.ly/rtp-exq" target="_blank"&gt;The Pragmatic Bookshelf&lt;/a&gt;. This book explores using Phoenix Channels, GenStage, and more to build
real-time applications in Elixir.&lt;/p&gt;

&lt;div style="text-align: center"&gt;
  &lt;a href="http://bit.ly/rtp-exq" target="_blank"&gt;
    &lt;img src="/images/sbsockets.jpg" alt="Real-Time Phoenix by The Pragmatic Bookshelf" height="300px" style="border: 1px solid #ccc" /&gt;
  &lt;/a&gt;
&lt;/div&gt;
</content>
  </entry>
  <entry>
    <title>Lessons Learned from Shipping PushEx</title>
    <link rel="alternate" href="https://stephenbussey.com/2019/08/30/lessons-learned-from-shipping-pushex.html"/>
    <id>https://stephenbussey.com/2019/08/30/lessons-learned-from-shipping-pushex.html</id>
    <published>2019-08-30T19:55:00-04:00</published>
    <updated>2020-04-30T21:25:00-04:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;It&amp;rsquo;s been a while since I announced the initial open-source release of PushEx. We had a few small challenges to solve
in order to roll it out, and then I let it bake for several months to ensure that it is stable in production. It has
been running for several months now at production scale, and I cut the first official release of it
on &lt;a href="https://hex.pm/packages/push_ex" target="_blank"&gt;hex.pm&lt;/a&gt; today. These are some of the challenges and lessons learned from running
it in production.&lt;/p&gt;

&lt;h2&gt;The project was a big success&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s start with the good news.
Our goal with the project was to replace our existing push provider, in order to have more control over an important
part of our tech stack. By &amp;ldquo;push provider&amp;rdquo;, I mean a service that sends data from servers to clients in real-time.
We needed to replace multiple JavaScript clients and we needed to ensure that the
pushes were getting queued up from our servers properly.&lt;/p&gt;

&lt;p&gt;This process went very well, due to careful planning and testing on our end. We ran both systems in parallel
for some time, and slowly ramped the number of pushes from 0 to 100%. Throughout this process, we pushed data to clients
but did not use it there. We became confident enough to consume the data on the clients through a slow rollout process.
At this point, the connections to our old provider dropped.&lt;/p&gt;

&lt;p&gt;We were able to avoid downtime or problems throughout this process by very carefully monitoring the application. We
stopped the rollout at the first sign of trouble, and then resumed once we understood the root cause.&lt;/p&gt;

&lt;h2&gt;The order of process shutdown matters&lt;/h2&gt;

&lt;p&gt;We encountered a large number of errors during the deployment process of the application. This never occurred during
normal operation, so it seemed a bit odd. The root cause of the errors was that the application would still receive
requests as it was shutting down. This caused messages to try to push to clients, but the processes involved in that
were already shut down. This can happen with a Supervisor structure like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[
  PushExWeb.Endpoint,
  PushEx.Pipeline
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the pipeline shuts down while the Endpoint is still online, web requests will encounter errors. A better layout
might look like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[
  PushEx.Pipeline,
  PushExWeb.Endpoint,
  PushExWeb.ConnectionDrainer
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to gain control over the shutdown process of PushEx, I had to make changes to the application supervision tree. I separated
the &lt;a href="https://github.com/pushex-project/pushex/blob/675c986acac5a06486d7b47e7b34e8289fda2d3b/lib/push_ex/application.ex#L10" target="_blank"&gt;starting of PushEx&amp;rsquo;s core system&lt;/a&gt;
from the &lt;a href="https://github.com/pushex-project/pushex/blob/675c986acac5a06486d7b47e7b34e8289fda2d3b/lib/push_ex/supervisor.ex#L11" target="_blank"&gt;web portion&lt;/a&gt;.
These changes allow the entire web stack to be gracefully go offline (complete with connection draining) before the data
pipeline goes offline.&lt;/p&gt;

&lt;p&gt;This leads into the next problem&amp;mdash;how to prevent data loss during application shutdown.&lt;/p&gt;

&lt;h2&gt;Connection draining is critical&lt;/h2&gt;

&lt;p&gt;Connection draining allows a web server to gracefully wait for connections to close before it proceeds with shutting down more of the application.
HTTP connection draining shuts down the listener process that would accept new connections.&lt;/p&gt;

&lt;p&gt;Several layers of draining are required for PushEx to gracefully exit. They are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/pushex-project/pushex/blob/675c986acac5a06486d7b47e7b34e8289fda2d3b/lib/push_ex_web/channels/socket_drainer.ex" target="_blank"&gt;Socket draining&lt;/a&gt; (sockets should gracefully disconnect)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pushex-project/pushex/blob/675c986acac5a06486d7b47e7b34e8289fda2d3b/lib/push_ex/supervisor.ex#L23" target="_blank"&gt;Ranch connection draining&lt;/a&gt; (new web requests shouldn&amp;rsquo;t be handled)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pushex-project/pushex/blob/675c986acac5a06486d7b47e7b34e8289fda2d3b/lib/push_ex/push/drainer.ex" target="_blank"&gt;Push Pipeline draining&lt;/a&gt; (data in
flight should be given a chance of delivery)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You&amp;rsquo;ll notice that Socket draining is implemented separately from the connection draining. This is because the connection draining API stops
the listener from accepting new connections only. If a connection is already established, it has to be manually killed or shut down.&lt;/p&gt;

&lt;p&gt;This process was complex to get right, but now we can reboot servers without errors and without losing data. All of our data
drains out within the 30 second limit, so your mileage would vary if that&amp;rsquo;s not the case.&lt;/p&gt;

&lt;h2&gt;Big topics affect performance&lt;/h2&gt;

&lt;p&gt;We have a few large push topics in our application. It can be costly for Phoenix Tracker to have a large number of joins in a very short period of time.
The solution for PushEx is to allow certain topics to not use Tracker. When an ignored topic
is &lt;a href="https://github.com/pushex-project/pushex/blob/62efb769e6bb3472f4b4635e9e98a5f6f1c346f7/lib/push_ex_web/channels/push_tracker.ex#L32" target="_blank"&gt;tracked&lt;/a&gt;
or &lt;a href="https://github.com/pushex-project/pushex/blob/62efb769e6bb3472f4b4635e9e98a5f6f1c346f7/lib/push_ex_web/channels/push_tracker.ex#L43" target="_blank"&gt;questioned&lt;/a&gt;,
it always responds with &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The likelihood that a large topic has 0 connected clients is close to 0, so it is acceptable to treat it like it always has clients. We don&amp;rsquo;t
want to treat small topics as if they always have connected clients, or we&amp;rsquo;d do more work than necessary.&lt;/p&gt;

&lt;h2&gt;Keep-Alive to dramatically increase API throughput&lt;/h2&gt;

&lt;p&gt;I was a bit shocked when I implemented the server API calls in our Ruby app. We were used to 20ms calls to our old provider, but the new
one was hitting 200ms! We traced the root cause to DNS slowness when establishing the connection. The solution was to utilize Keep-Alive
connection pooling.&lt;/p&gt;

&lt;p&gt;The Keep-Alive header makes it so that the connection is not closed after a response is sent. The connection is open to accept more requests
and can do so without the overhead of establishing a connection. This dramatically increased the throughput of our Ruby servers to the
PushEx API endpoint. We didn&amp;rsquo;t quite hit the 20ms goal, but it was close enough to call it a win.&lt;/p&gt;

&lt;p&gt;I did hit a snag here. The connection draining for Keep-Alive connections suffers from the same problem as WebSockets do&amp;mdash;they
don&amp;rsquo;t close when the listener is closed. I had to do a bit of hackery to set a global value indicating that the Keep-Alive connection should
be &lt;a href="https://github.com/pushex-project/pushex/blob/675c986acac5a06486d7b47e7b34e8289fda2d3b/lib/push_ex_web/controllers/push_controller.ex#L32" target="_blank"&gt;closed on the next request&lt;/a&gt;.
This is a hack that I wish I didn&amp;rsquo;t have to do, but it did have the desired impact.&lt;/p&gt;

&lt;h2&gt;Wrapping Up&lt;/h2&gt;

&lt;p&gt;There were lots of challenges in rolling out the first major PushEx release, but the end result is solid. We&amp;rsquo;re running at pretty high throughput
on a low number of small servers (2GB + 2VCPU for this app) without issue.&lt;/p&gt;

&lt;p&gt;The snags we hit are not unique to our application. When building an Elixir application, you should consider both the startup and shutdown order
of your process tree. Use connection draining to avoid new connections being made to a server that is in the process of shutting down. Leverage
Keep-Alive headers for server-to-server API requests, especially if the throughput is high. These things do come with tradeoffs, however, so
your mileage may vary.&lt;/p&gt;

&lt;h2&gt;The Book Plug&lt;/h2&gt;

&lt;p&gt;My book &amp;ldquo;Real-Time Phoenix: Build Highly Scalable Systems with Channels&amp;rdquo; is now in beta
through &lt;a href="https://pragprog.com/book/sbsockets/real-time-phoenix" target="_blank"&gt;The Pragmatic Bookshelf&lt;/a&gt;. This book captures the victories and struggles that
we face when building real-time applications in Elixir.&lt;/p&gt;

&lt;div style="text-align: center"&gt;
  &lt;a href="https://pragprog.com/book/sbsockets/real-time-phoenix" target="_blank"&gt;
    &lt;img src="/images/sbsockets.jpg" alt="Real-Time Phoenix by The Pragmatic Bookshelf" height="300px" style="border: 1px solid #ccc" /&gt;
  &lt;/a&gt;
&lt;/div&gt;
</content>
  </entry>
  <entry>
    <title>Elixir Makes Testing Hard Things Easy</title>
    <link rel="alternate" href="https://stephenbussey.com/2019/06/08/elixir-makes-testing-hard-things-easy.html"/>
    <id>https://stephenbussey.com/2019/06/08/elixir-makes-testing-hard-things-easy.html</id>
    <published>2019-06-08T01:59:00-04:00</published>
    <updated>2020-04-30T21:25:00-04:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;I love Elixir. If we&amp;rsquo;ve talked at a conference or you work with me, you know this. I constantly find small nuggets
when working with Elixir that simply bring joy to me. One of these nuggets recently emerged that I thought worth
sharing.&lt;/p&gt;

&lt;h1&gt;Backstory&lt;/h1&gt;

&lt;p&gt;I&amp;rsquo;m working on some learning material right now for real-time system development. One of the key aspects of a section
is about how important measurement is. Without measurement, how do we have any confidence that our running system
is healthy?&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m using StatsD for this section because it&amp;rsquo;s fairly well-adopted and is conceptually simple to understand. However,
I kept bumping into small inefficiencies in my flow. An example of this is that I use a Node program called &lt;code&gt;statsd-logger&lt;/code&gt;
to view my metrics in development. It felt a bit strange to have a reader install this Node program to view logs in our
Elixir app.&lt;/p&gt;

&lt;p&gt;The same problem came up today when I was writing tests for the StatsD metrics. I would typically use Mockery to test that
Statix, my client, is correctly called. If I went down this path, I&amp;rsquo;d be putting a lot on the reader in terms of understanding
what the heck a mock is and when to use it. Ideally, we&amp;rsquo;ll have 1 line of code max additional setup.&lt;/p&gt;

&lt;p&gt;Okay, enough of the backstory. I wanted to fix these two very specific problems. Let&amp;rsquo;s get into the more interesting bits.&lt;/p&gt;

&lt;h1&gt;Elixir is Many Programs&lt;/h1&gt;

&lt;p&gt;Elixir runs processes in a way that makes them feel like little independent programs. We can spin up tens of thousands of these
programs without any issue at all. We can even spin up programs that we may consider expensive, such as a webserver.&lt;/p&gt;

&lt;p&gt;The real jewel of these many programs is that they are given a simple and efficient way to communicate with each other, message
passing. To take the web server example to the extreme, we could spin up 5 different servers on 5 ports and interface with them
through our browser. Behind the scenes, these &amp;ldquo;servers&amp;rdquo; would be in the same Elixir VM and could be passing messages back and forth
nearly freely.&lt;/p&gt;

&lt;p&gt;This is conceptually interesting, to me, because it means that traditionally complex or standalone applications can be run in the
same program (very easily) and can share information.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s see how this applies to StatsD.&lt;/p&gt;

&lt;h1&gt;Just run a Server&lt;/h1&gt;

&lt;p&gt;What if we just ran a StatsD server in our Elixir application? It could listen on a port and accept messages. We could do whatever
we wanted with these messages, like log them out or store them. Let&amp;rsquo;s do that in a few lines of code:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;$ iex
iex(1)&amp;gt; :gen_udp.open(8126, active: true)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;ve started a &amp;ldquo;UDP server&amp;rdquo; in our terminal. Let&amp;rsquo;s send it some messages (open new terminal):&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;# Your mileage may vary, fingers crossed
$ echo -n &amp;quot;hello:1&amp;quot; | nc -4u -w0 localhost 8126
$ echo -n &amp;quot;hello:2&amp;quot; | nc -4u -w0 localhost 8126
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s go back to the original iex terminal:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;iex(2)&amp;gt; flush()
{:udp, #Port&amp;lt;0.5&amp;gt;, {127, 0, 0, 1}, 61814, &amp;#39;hello:1&amp;#39;}
{:udp, #Port&amp;lt;0.5&amp;gt;, {127, 0, 0, 1}, 61515, &amp;#39;hello:2&amp;#39;}
:ok
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Woah! We received the UDP packets as messages to our process. That&amp;rsquo;s super neat to me because of how simple it was (due to
the great backing work of Erlang/OTP.)&lt;/p&gt;

&lt;p&gt;So what if this was the solution to problem 1 (statsd-logger)? We could start a UDP server and log out messages. I threw together
a quick library for that purpose called &lt;a href="https://github.com/sb8244/statsd_logger" target="_blank"&gt;StatsDLogger&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;That worked out nicely. What about testing, though? Let&amp;rsquo;s take what we just wrote and apply it to tests.&lt;/p&gt;

&lt;h1&gt;Test Anything Easily&lt;/h1&gt;

&lt;p&gt;Testing can sometimes be hard. Integration testing across multiple servers is usually hard. What level do we start to mock at? We can&amp;rsquo;t
actually run all of our services all of the time without having very slow tests. To help solve this, we can take what we just did and apply it to
tests. There is a key question though, how do we actually write an assertion against our server?&lt;/p&gt;

&lt;p&gt;In Elixir, message passing is champion. If you have a process&amp;rsquo;s ID, you can send it a message and it will go into that process&amp;rsquo;s mailbox.
With ExUnit, this mailbox can be asserted against by using &lt;code&gt;assert_receive&lt;/code&gt;. This means that if we can send our test process
a message, we can write a test for it.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s slightly harder to boil this concept down into an example as simple as the last. Let&amp;rsquo;s look at some code I wrote to
handle this for StatsD:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;  def handle(:valid, name, value, opts) do
    pid = Keyword.fetch!(opts, :send_to)
    send(pid, {:statsd_recv, name, value})
  end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can handle a metric by sending a message to a pid. Now the question is, &amp;ldquo;which pid?&amp;rdquo; The process that calls &lt;code&gt;start_link&lt;/code&gt;
can be referenced with &lt;code&gt;self()&lt;/code&gt;. This means we can do something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;  def start_link(opts) do
    opts = Keyword.merge([send_to: self()], opts)
    GenServer.start_link(__MODULE__, opts)
  end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can put all of this together to write a test like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;  test &amp;quot;valid / invalid messages are handled&amp;quot; do
    StatsDLogger.start_link(port: @port, formatter: :send)

    send_event(&amp;quot;a:1&amp;quot;)
    send_event(&amp;quot;a:2|c&amp;quot;)
    send_event(&amp;quot;invalid&amp;quot;)

    assert_receive {:statsd_recv, &amp;quot;a&amp;quot;, &amp;quot;1&amp;quot;}
    assert_receive {:statsd_recv, &amp;quot;a&amp;quot;, &amp;quot;2|c&amp;quot;}
    assert_receive {:statsd_recv_invalid, &amp;quot;invalid&amp;quot;}
  end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Elixir has made testing this multi-server integration a piece of cake.&lt;/p&gt;

&lt;h1&gt;In the Wild&lt;/h1&gt;

&lt;p&gt;I&amp;rsquo;m definitely not finding any new technique here. The reason I love Elixir is that gems like this are all over open-source
libraries. The one that comes to mind here is &lt;a href="https://github.com/PSPDFKit-labs/bypass" target="_blank"&gt;Bypass&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Bypass works by starting an anonymous HTTP server that runs a function when it&amp;rsquo;s invoked. That is a great example of &amp;ldquo;start a
server&amp;rdquo; to solve how to test that an application makes the right request.&lt;/p&gt;

&lt;p&gt;Phoenix Channels are a great example of &amp;ldquo;send a message&amp;rdquo; in the wild. When you write &lt;code&gt;assert_push&lt;/code&gt;, you&amp;rsquo;re actually just
&lt;a href="https://github.com/phoenixframework/phoenix/blob/master/lib/phoenix/test/channel_test.ex#L481" target="_blank"&gt;checking for a specific message&lt;/a&gt;.
The Channel test is wired up so that the test process becomes the &amp;ldquo;socket transport.&amp;rdquo; This allows it to be at the edge of the system
and capture a lot of test for very little code.&lt;/p&gt;

&lt;h1&gt;Wrapping Up&lt;/h1&gt;

&lt;p&gt;If you&amp;rsquo;re here, thanks for making it through. I love Elixir because of small things like this that would actually be huge in some
other languages I&amp;rsquo;ve worked with. If you&amp;rsquo;re looking for a way to test complex code, try out message passing as a potential solution.&lt;/p&gt;

&lt;h1&gt;Shameless Plug for ElixirConf&lt;/h1&gt;

&lt;p&gt;My colleague Grant and I are going to give a great training session at ElixirConf Tuesday course. You
can find more details at &lt;a href="https://elixirconf.com/2019/training-classes/2" target="_blank"&gt;https://elixirconf.com/2019/training-classes/2&lt;/a&gt;. We&amp;rsquo;ll
be focused on writing real-time systems, and will specifically be leveraging a lot of real world lessons from doing this at
decent scale on several pieces of our SaaS app. Come check it out!&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Useful Elixir Patterns from Real-world Side Project</title>
    <link rel="alternate" href="https://stephenbussey.com/2019/05/10/useful-elixir-patterns-from-real-world-side-project.html"/>
    <id>https://stephenbussey.com/2019/05/10/useful-elixir-patterns-from-real-world-side-project.html</id>
    <published>2019-05-10T13:00:00-04:00</published>
    <updated>2020-04-30T21:25:00-04:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;I believe that one of the best ways to push new practices is to work on a real-world project that we
can afford to experiment on. We can push the boundary in the toy project while still seeing the results
of the decisions in a production environment with real users. I&amp;rsquo;ve been fortunate to be able to ship
several applications like this at SalesLoft. The latest one that we&amp;rsquo;ll look at today is our internal OKR (Objective-Key Result) app that we use for goal tracking and alignment.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll walk through this project and see several different patterns that I like. Some of these patterns
have been used in several different projects of mine and have held up well. Others were new experiments
that I hope to use again in the future.&lt;/p&gt;

&lt;p&gt;The source code for the application can be found on &lt;a href="https://github.com/sb8244/okr_app_pub" target="_blank"&gt;Github&lt;/a&gt;. The useful
part is the code and tests, less so the specific functionality.&lt;/p&gt;

&lt;h1&gt;Context Usage&lt;/h1&gt;

&lt;p&gt;Contexts, at least as I use them, are all about establishing an application domain and keeping
the interfaces small and useful for other parts of the system to use. I&amp;rsquo;ve done this in the past but
really pushed it for this project by having no function usage other than top level modules. This
pattern allows us to have a well-defined interface for the application, which makes it simpler for both
us and others to modify the code and understand the consequences.&lt;/p&gt;

&lt;p&gt;As an example, the following would not be acceptable:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;MyApp.SomeContext.ASpecificQuery.execute(params)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Instead, I would prefer this:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;MyApp.SomeContext.a_query(params)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All of my contexts live under the &lt;code&gt;OkrApp&lt;/code&gt; module. You can see a list of them in the &lt;a href="https://github.com/sb8244/okr_app_pub/tree/master/lib/okr_app" target="_blank"&gt;okr_app folder&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I started off the application by writing code directly in the context. Some of this is still seen because
I didn&amp;rsquo;t go back through and change everything once I solidified on what I wanted. In the end, I found
that &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/analytics.ex#L5" target="_blank"&gt;function delegates&lt;/a&gt;
allowed the context to be very easy to digest and I could write tests for the underlying modules in distinct
files.&lt;/p&gt;

&lt;p&gt;The hardest part of this pattern, that I haven&amp;rsquo;t figured out yet, is how to handle Ecto schemas between
contexts. Sometimes, a context needs to leak out. For example, a &lt;code&gt;User&lt;/code&gt; schema may need to be referenced
by the &lt;code&gt;AnalyticsEvent&lt;/code&gt; schema (as seen &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/analytics/analytics_event.ex#L11" target="_blank"&gt;here&lt;/a&gt;).
I found this acceptable because I wasn&amp;rsquo;t invoking logic outside of the context interface.&lt;/p&gt;

&lt;p&gt;The advantage of Context came out when I built the &lt;a href="https://github.com/sb8244/okr_app_pub/tree/master/lib/okr_app/mailer" target="_blank"&gt;mailer&lt;/a&gt;.
I found it very simple to add in this new code without worrying about anything breaking. I was also to extract concepts
specific to the mailer (such as &lt;code&gt;Recipient&lt;/code&gt;) rather than relying on generic modules prone to change such as &lt;code&gt;User&lt;/code&gt;.&lt;/p&gt;

&lt;h1&gt;Isolated Logic (Context) for SCIM&lt;/h1&gt;

&lt;p&gt;One of the requirements for this project was SCIM (System for Cross-domain Identity Management). We use Okta internally and
they have a lot of docs on how to write a SCIM integration. I didn&amp;rsquo;t want the details of SCIM leaking into the system too
heavily. I was able to leverage a SCIM Context to achieve this goal in a way that I am pretty satisfied with.&lt;/p&gt;

&lt;p&gt;SCIM has all of its web functionality extracted into a module called &lt;code&gt;Scim.Web&lt;/code&gt;. &lt;code&gt;Phoenix.Router&lt;/code&gt; is used to provide the
appropriate endpoint definitions, but the integration happens in a simple &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/scim/web/plug.ex#L13" target="_blank"&gt;Plug&lt;/a&gt;
which is used by &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app_web/router.ex#L43" target="_blank"&gt;&lt;code&gt;OkrAppWeb.Router&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The entire &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/scim/web/users_controller.ex#L9" target="_blank"&gt;SCIM controller&lt;/a&gt; leverages
a behavior passed into it. This is the &lt;a href="https://en.wikipedia.org/wiki/Strategy_pattern" target="_blank"&gt;strategy pattern&lt;/a&gt; at work.
I didn&amp;rsquo;t create a specific &lt;code&gt;Behaviour&lt;/code&gt; requirement for this module but probably should have, because it has an expected
interface.&lt;/p&gt;

&lt;p&gt;The actual SCIM application integration happens in &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/users_scim.ex" target="_blank"&gt;UsersScim&lt;/a&gt;.
This code is a bit ugly since I didn&amp;rsquo;t clean it up too much, but it&amp;rsquo;s nice that the implementation doesn&amp;rsquo;t leak into my
application API nor does it leak into other contexts.&lt;/p&gt;

&lt;h1&gt;ListQuery Module&lt;/h1&gt;

&lt;p&gt;I really like Ecto&amp;rsquo;s API for querying data, but I have found it cumbersome to build up dynamic queries from API params easily.
I wrote a &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/query/list_query.ex" target="_blank"&gt;&lt;code&gt;ListQuery&lt;/code&gt;&lt;/a&gt; some time ago that I brought into this project.&lt;/p&gt;

&lt;p&gt;The ListQuery is used to take provided &lt;code&gt;params&lt;/code&gt; and &lt;code&gt;opts&lt;/code&gt; and turns it into an Ecto compatible query. Sometimes advanced
queries are required and I devised a small way to do that by stripping certain &lt;code&gt;params&lt;/code&gt; and then reapplying them. You can
see that &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/objectives/store/cycle_store.ex#L11" target="_blank"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m able to leverage a &lt;code&gt;ListQuery&lt;/code&gt; powered function in &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app_web/controllers/api/objective_link_controller.ex#L11" target="_blank"&gt;various controllers&lt;/a&gt;
by passing in the user params. If you use this pattern in an environment with non-friendly users, you should probably sanitize
the input to not leak any information to the client.&lt;/p&gt;

&lt;h1&gt;SimpleEctoStore Module&lt;/h1&gt;

&lt;p&gt;My contexts use &lt;code&gt;defdelegate&lt;/code&gt; to send queries to a different module. I called the module a &amp;ldquo;store&amp;rdquo; and found myself
&lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/objectives.ex#L13" target="_blank"&gt;delegating to them often&lt;/a&gt;.
Writing the same code again and again in the store became a bit cumbersome. Often, I would be writing the most
simple code possible with just the module name changed.&lt;/p&gt;

&lt;p&gt;My solution to this copy/paste problem was to build a macro powered &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/store/simple_ecto_store.ex" target="_blank"&gt;&lt;code&gt;SimpleEctoStore&lt;/code&gt;&lt;/a&gt;.
Each usage of &lt;code&gt;SimpleEctoStore&lt;/code&gt; involved passing in the schema module as well as what methods were desired to be
pulled in. You can see this &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/objectives/store/objective_link_store.ex#L2" target="_blank"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This allowed me to remove the boiler plate for a large number of ecto stores. If I used a concept like stores again,
I would definitely repeat this one.&lt;/p&gt;

&lt;h1&gt;Things that didn&amp;rsquo;t work as well&lt;/h1&gt;

&lt;p&gt;One of the biggest pain points I had was defining how I wanted my JSON API to work. At work, I follow the philosophy of very focused
endpoints that don&amp;rsquo;t preload many models together. This is for performance and flexibility reasons. Doing that in this project
made it much more complex for me and was going to take time that I didn&amp;rsquo;t want to spend. The end result here is that I
have a massive endpoint called &amp;ldquo;Okr&amp;rdquo; that &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app_web/serializers/okr.ex" target="_blank"&gt;embeds many associations&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The preload for my Okr endpoint is &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/objectives/okr_preloader.ex" target="_blank"&gt;pretty gnarly&lt;/a&gt; as
well. However, I was thrilled to be able to pass queries into the preload clauses which means that I removed the risk of
accidentally leaking data that is queried out of the system but then not handled properly by me in Elixir.&lt;/p&gt;

&lt;p&gt;Another thing that I still feel awkward about is a context&amp;rsquo;s schema referencing a different context&amp;rsquo;s schema. I wanted to
remove this but couldn&amp;rsquo;t figure out how to do so in a way I liked.&lt;/p&gt;

&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;Contexts allowed for a focused API between parts of an application. Using them saved me some mental gymnastics as I was
developing the application. I was particularly happy with how I was able to isolate the sort of whacky SCIM API via
focused context modules.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ListQuery&lt;/code&gt; and &lt;code&gt;SimpleEctoStore&lt;/code&gt; both made my life easier while defining my ecto based queries and stores. These are generically
applicable modules that could be brought into other applications if needed.&lt;/p&gt;

&lt;p&gt;I am really happy overall with how this project turned out. It is easy to read through nearly 6 months after I&amp;rsquo;ve originally
built it and has been running without any Elixir issues since that time. I&amp;rsquo;ll be applying these patterns to future projects,
for sure, and hope you are able to get some value from it!&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Thanks for reading! I&amp;rsquo;ll be teaching a class at ElixirConf US about building scalable real-time systems in Elixir. I&amp;rsquo;ve been
focused a lot on this topic so I&amp;rsquo;m pretty thrilled to share what I&amp;rsquo;ve learned. I believe that registration will open
soon.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Monolith to Microservice Without Downtime — A Production Story</title>
    <link rel="alternate" href="https://stephenbussey.com/2019/03/01/monolith-to-microservice-without-downtime-a-production-story.html"/>
    <id>https://stephenbussey.com/2019/03/01/monolith-to-microservice-without-downtime-a-production-story.html</id>
    <published>2019-03-01T17:35:00-05:00</published>
    <updated>2020-04-30T21:25:00-04:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;I am cross-posting for an article that I published on &lt;a href="https://medium.com/salesloft-engineering/monolith-to-microservice-without-downtime-a-production-story-652c9b82f03e" target="_blank"&gt;SalesLoft&amp;rsquo;s Medium blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Rather than copying the content over to this blog, I&amp;rsquo;m going to point you at the above Medium post. However, here are the lessons learned from the process:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Migrating from monolith to microservice is not easy. In fact, it involved a lot of nuance in the coding and planning that had to be a forethought and not an afterthought.&lt;/li&gt;
&lt;li&gt;Stability for customers is the most important part of a refactor like this; our product exists to serve our customers and not to be a microservice-powered entity.&lt;/li&gt;
&lt;li&gt;Finding the seam in the monolith is important when it comes to stability. If your new code, which shouldn’t be running until it’s ready, causes old requests to fail…that is a problem.&lt;/li&gt;
&lt;li&gt;The timeline was significantly longer than I anticipated. This is important because we have to prioritize refactoring old code against developing new features and fixing bugs. On the flip side…&lt;/li&gt;
&lt;li&gt;The slower timeline helped us ensure rock solid stability. We had zero stability concern on release day, which turned out to be like any other normal day.&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
</feed>

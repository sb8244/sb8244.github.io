<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Stephen Bussey's Software Engineering Blog</title>
  <id>https://stephenbussey.com</id>
  <link href="https://stephenbussey.com"/>
  <link href="https://stephenbussey.com/feed.xml" rel="self"/>
  <updated>2018-05-08T23:32:00-04:00</updated>
  <author>
    <name>Stephen Bussey</name>
  </author>
  <entry>
    <title>Elixir Memory - Not Quite Free</title>
    <link rel="alternate" href="https://stephenbussey.com/2018/05/09/elixir-memory-not-quite-free.html"/>
    <id>https://stephenbussey.com/2018/05/09/elixir-memory-not-quite-free.html</id>
    <published>2018-05-08T23:32:00-04:00</published>
    <updated>2018-05-09T00:51:32-04:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;I have been working on a shipping an Elixir service at SalesLoft to replace an existing
piece of functionality in our system with a better version. One of the core changes
is that the websocket communication of this system will be maintained by Elixir rather
than by Pusher (our Rails goto). This post is going to explore some of the surprises
and valuable lessons that I gained while debugging memory leaks in the service.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;I was doing a usual check-in on the service and noticed that the memory for only ~200
websockets connected was peaking out at over 550MB. This seemed off and meant that
connecting all users to the service would take many GB of memory. I was at a loss for
what the problem could be though, but found some help in a useful diagnostic tool.&lt;/p&gt;

&lt;h2&gt;Diagnosing Memory Culprit&lt;/h2&gt;

&lt;p&gt;When doing local development, it&amp;rsquo;s very easy to pop open &lt;code&gt;:observer.start&lt;/code&gt; and get
a wonderful interface to sift through processes and their memory consumption. However,
exposing this in production is much more difficult (too difficult in my setup to bother).
I found the tool &lt;a href="https://github.com/zhongwencool/observer_cli" target="_blank"&gt;observer_cli&lt;/a&gt; and have to
say that it is one of the greatest tools in my toolbox right now. It is based on the
well known &lt;code&gt;recon&lt;/code&gt; library, but provides a nice command line interface to visually
see and sort through process listings. I have to give props to the library maintainer
for implementing a feature I requested in a matter of days.&lt;/p&gt;

&lt;p&gt;As I was looking at the observer_cli output, I noticed that Phoenix Channel Servers were
appearing all over the top memory utilizers. Some would take up to 4MB, but averaged at about
1 MB. For 1000 sockets, this would be between 1GB - 4GB of memory!&lt;/p&gt;

&lt;h2&gt;Understanding Erlang Memory&lt;/h2&gt;

&lt;p&gt;One of my favorite posts on Erlang is by Hamidreza Soleimani, &lt;a href="https://hamidreza-s.github.io/erlang%20garbage%20collection%20memory%20layout%20soft%20realtime/2015/08/24/erlang-garbage-collection-details-and-why-it-matters.html" target="_blank"&gt;Erlang Garbage Collection Details&lt;/a&gt;.
This post goes over an important detail of how the 2-part Erlang GC works, young and old generational
heaps. The gist of it is that the major GC operation can collect both young and old heaps,
but is invoked infrequently as it is a &amp;ldquo;stop the process&amp;rdquo; type of GC. The minor GC
operation can only collect young heap items, and will mark items as old if they survive
a GC pass. What does this mean, though?&lt;/p&gt;

&lt;p&gt;In the context of a request, it is possible for an operation to take multiple
GC minor sweeps and still be referencing all of the allocated binaries (data). When
this happens, those binaries are going to be marked as old heap and require that
a full sweep happens to GC them. If the full sweep only happens in certain situations,
it is possible for the situation to not occur and the full sweep doesn&amp;rsquo;t occur. In
this case, we have a memory leak. This is exactly what was happening to my websocket.&lt;/p&gt;

&lt;p&gt;It is possible to trigger a GC on the entire node to test out if there is a possible
memory leak. Note that you don&amp;rsquo;t want to do this in a regular fashion and running
the major GC may make your debugging sessions less valuable until time passes.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# don&amp;#39;t do this often
Process.list() |&amp;gt; Enum.each(&amp;amp;:erlang.garbage_collect/1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Addressing this Memory Leak&lt;/h2&gt;

&lt;p&gt;The websocket request that I&amp;rsquo;ve been discussing was new and a bit different than
what I&amp;rsquo;ve done in the past. It is an interface to our &lt;a href="https://developers.salesloft.com/api.html" target="_blank"&gt;API&lt;/a&gt; and allows requests
to be made on the open connection. Due to it being an API request, it would return
up to 100 items at a time and require multiple pieces of data sourced from other
services to operate. This equates to 2 things: memory (1-4 MB) and time (multiple minor sweeps).&lt;/p&gt;

&lt;p&gt;&lt;a href="https://s3.us-east-2.amazonaws.com/ferd.erlang-in-anger/text.v1.1.0.pdf" target="_blank"&gt;Erlang in Anger 7.2.2&lt;/a&gt; mentions 5 different ways to fix a memory leak:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;call garbage collection manually at given intervals (icky, but somewhat efficient)&lt;/li&gt;
&lt;li&gt;stop using binaries (often not desirable)&lt;/li&gt;
&lt;li&gt;use binary:copy/1-2 if keeping only a small fragment (usually less than 64 bytes) of a larger binary&lt;/li&gt;
&lt;li&gt;move work that involves larger binaries to temporary one-off processes that will die when theyâ€™re done (a lesser form of manual GC!)&lt;/li&gt;
&lt;li&gt;or add hibernation calls when appropriate (possibly the cleanest solution for inactive processes)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;The above is copied from Erlang in Anger verbatim.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For the work that this websocket request was doing, it made most sense to utilize
a short lived process to execute and respond to the request.&lt;/p&gt;

&lt;p&gt;The initial pass at the fix involved using &lt;code&gt;Task.async&lt;/code&gt; and awaiting the response.
However, this proved to be even worse on memory because sending the response over
the process barrier was causing the same leak. The solution here ended up being to
utilize &lt;a href="https://hexdocs.pm/phoenix/Phoenix.Channel.html#socket_ref/1" target="_blank"&gt;&lt;code&gt;Phoenix.Channel socket_ref/1&lt;/code&gt;&lt;/a&gt;
and responding to the socket request in the &lt;code&gt;Task.start&lt;/code&gt;d process using &lt;code&gt;Phoenix.Channel.reply(ref, reply)&lt;/code&gt;.
The &lt;code&gt;socket_ref&lt;/code&gt; function is very useful and has some nice side effects. Due to
serialization of processes, the original approach would block access to the socket
during a request. With the new approach, the socket can handle multiple requests
at the same time.&lt;/p&gt;

&lt;h2&gt;Results from Fix&lt;/h2&gt;

&lt;div style="text-align: center"&gt;
  &lt;img src="/images/erlang-memory/step-1.png" alt="memory drop from 550MB to 250MB" /&gt;
  &lt;div&gt;
    &lt;small&gt;
      &lt;i&gt;Large fix from short lived process&lt;/i&gt;
    &lt;/small&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The results from this code change were immediate and apparent. The Phoenix.Channel.Server
processes that were from 1MB - 4MB were now at 30KB - 60KB. This lead to the massive drop
in overall memory as seen above.&lt;/p&gt;

&lt;h2&gt;Rinse and Repeat&lt;/h2&gt;

&lt;p&gt;As more sockets were connected to the system, it became clear there was still a memory
leak. By utilizing the &lt;code&gt;observer_cli&lt;/code&gt; tool, it was possible to see that the cowboy
websocket processes were hovering at 1-2MB each. Upon discussion in the community Slack, it turns
out that encoding the large payload suffers from the same type of memory leak that
was mentioned previously. However, the fix is less optimal due to that code not
being written by us.&lt;/p&gt;

&lt;p&gt;It appeared that triggering a major GC was the best option.
Phoenix even accommodates this with a special &lt;a href="https://hexdocs.pm/phoenix/Phoenix.Transports.WebSocket.html#module-garbage-collection" target="_blank"&gt;&lt;code&gt;:garbage_collect&lt;/code&gt;&lt;/a&gt;
message handler, which is marked as a solution for use after processing large messages.
We ended up triggering this 5s after the response of our large payload.&lt;/p&gt;

&lt;div style="text-align: center"&gt;
  &lt;img src="/images/erlang-memory/step-2.png" alt="memory drop from 350MB to 210MB" /&gt;
  &lt;div&gt;
    &lt;small&gt;
      &lt;i&gt;Large fix from manual GC&lt;/i&gt;
    &lt;/small&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;This memory usage is from significantly more connected sockets than step 1, and we can
clearly see how large of an impact the manual GC had. The memory is now predictable and
stable for connected sockets.&lt;/p&gt;

&lt;h2&gt;Final Thoughts&lt;/h2&gt;

&lt;p&gt;This is a very constrained use case, although possibly common, for memory leaks of
Phoenix websockets. However, the same principles apply to all of the processes we spawn
in Elixir. When we have a process, and especially with a large number of processes, it
is important to think about the life cycle of it and how it will play into garbage collection.
As our processes become longer lived, this becomes even more important as our systems
will be leaking memory over longer periods of time.&lt;/p&gt;

&lt;p&gt;It seems easiest to just slap a full system GC into every process to keep memory usage
low (and we can do that if desired), but there are other techniques related to process
lifecycle and memory consumption that may be more effective in the end.&lt;/p&gt;

&lt;p&gt;Again, I recommend reading &lt;a href="https://s3.us-east-2.amazonaws.com/ferd.erlang-in-anger/text.v1.1.0.pdf" target="_blank"&gt;Erlang in Anger 5, 7&lt;/a&gt;
and &lt;a href="https://hamidreza-s.github.io/erlang%20garbage%20collection%20memory%20layout%20soft%20realtime/2015/08/24/erlang-garbage-collection-details-and-why-it-matters.html" target="_blank"&gt;Erlang Garbage Collection Details&lt;/a&gt;
for more detailed information.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Introducing Elixir Response Snapshot Testing</title>
    <link rel="alternate" href="https://stephenbussey.com/2018/04/17/introducing-elixir-response-snapshot-testing.html"/>
    <id>https://stephenbussey.com/2018/04/17/introducing-elixir-response-snapshot-testing.html</id>
    <published>2018-04-16T21:53:00-04:00</published>
    <updated>2018-04-16T23:13:23-04:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;I&amp;rsquo;m excited to introduce an Elixir testing library that I&amp;rsquo;ve been working on, as well
as to explain the general ideas behind it. The library is called Response Snapshot and
can be found on &lt;a href="https://hex.pm/packages/response_snapshot" target="_blank"&gt;hex&lt;/a&gt;. The work is based on
my &lt;a href="https://github.com/SalesLoft/rspec-rcv" target="_blank"&gt;rspec-rcv&lt;/a&gt; gem which provides a very similar
interface and seeks to achieve the same goals.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://hex.pm/packages/response_snapshot" target="_blank"&gt;hex.pm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sb8244/elixir_response_snapshot" target="_blank"&gt;github.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;What is snapshot testing?&lt;/h1&gt;

&lt;p&gt;I&amp;rsquo;m borrowing this term from Jest, but I think that the concept is fairly simple
and true to the name. Snapshot testing is testing an API response by collecting
a valid snapshot of the output and comparing future tests against it. The entire
API response can be asserted against with a single line of code, and visually verified
as correct by a human.&lt;/p&gt;

&lt;p&gt;In addition to being able to quickly test response outputs and ensure their shape
doesn&amp;rsquo;t change over time, the snapshots can be utilized by frontend specs or other
client systems as an example of valid output. If the frontend specs always use the
most recent snapshots, any changes to the backend which would break the frontend
should fail when the frontend specs run.&lt;/p&gt;

&lt;h1&gt;Where does snapshot testing fall short?&lt;/h1&gt;

&lt;p&gt;I would hazard a guess that the idea of snapshot testing makes TDD practitioners
cringe a bit. In particular, a human has to verify that the snapshot is accurate
and then commit that into source control. Humans are prone to errors and so this
could be considered worse than standard tests on a response. I do think that there
is a place for response assertions in addition to a snapshot, although snapshots
significantly lower the barrier to entry on testing responses. Without snapshot testing,
the tests for a response can end up being long, redundant, and a chore to update.&lt;/p&gt;

&lt;p&gt;One of the big challenges in snapshot testing is handling values that change between
test runs. This is really common with ids, dates, and information generated from
a library like faker. ResponseSnapshot introduces an optional &lt;code&gt;:keys&lt;/code&gt; mode which
will not error out if any values are modified. This is most useful when the shape
of the data should be asserted, but not the exact values. Addition and removal of
keys would be an error in this mode. Another solution to avoid using keys mode is
ignored keys. With this, specific path value changes can be ignored. Paths can be
absolute or wildcards.&lt;/p&gt;

&lt;h1&gt;Getting started in Elixir&lt;/h1&gt;

&lt;p&gt;To get started in Elixir, follow the &lt;a href="https://github.com/sb8244/elixir_response_snapshot#installation" target="_blank"&gt;installation instructions&lt;/a&gt;.
Capturing your first test involves invoking &lt;code&gt;store_and_compare!/2&lt;/code&gt;. Let&amp;rsquo;s see an
example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;test &amp;quot;widgets are listed out&amp;quot;, %{conn: conn} do
  conn
  |&amp;gt; get(&amp;quot;/api/widgets&amp;quot;)
  |&amp;gt; json_response(200)
  |&amp;gt; ResponseSnapshot.store_and_compare!(path: &amp;quot;widgets/index.json&amp;quot;)
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In practice, you will want to setup your fixture path base and ignore keys that
change between test runs (such as ids and dates):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;config :response_snapshot,
  path_base: &amp;quot;test/fixtures&amp;quot;,
  ignored_keys: [
    {&amp;quot;id&amp;quot;, :any_nesting},
    {&amp;quot;created_at&amp;quot;, :any_nesting},
  ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output of the test run will be a fixture file located at &lt;code&gt;test/fixtures/widgets/index.json&lt;/code&gt;.
This file will include the test file path, the recording time, and the JSON response
of the API.&lt;/p&gt;

&lt;h1&gt;Next steps&lt;/h1&gt;

&lt;p&gt;For next steps, I&amp;rsquo;d like to bring some best practices to the library, such as using
Dialyzer. However, the core of the library seems to be operating as expected and includes
a lot of lessons learned from rspec-rcv over the past 2 years. Please reach out
on Github issues if you see anything you&amp;rsquo;d like fixed or have any questions!&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Writing Break</title>
    <link rel="alternate" href="https://stephenbussey.com/2018/02/25/writing-break.html"/>
    <id>https://stephenbussey.com/2018/02/25/writing-break.html</id>
    <published>2018-02-25T12:42:00-05:00</published>
    <updated>2018-02-25T12:43:44-05:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;I recently stopped writing for my 28 days initiative. I figured that I should at
least post a final closure on it. I ended up traveling unexpected for work in such
a way that keeping up daily writing would be incredibly hard on me mentally and
prevent enjoyment of where I am. As such, I ended my 28 days after 18.&lt;/p&gt;

&lt;p&gt;I have a lot of good content still in mind. Most of the content that I want to work
on is now longer form, requiring much more research or examples to be effective. I hope
to establish a new cadence in the upcoming weeks to get these posts out.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>28 Days - Exploring Elixir Map access</title>
    <link rel="alternate" href="https://stephenbussey.com/2018/02/18/exploring-elixir-map-access.html"/>
    <id>https://stephenbussey.com/2018/02/18/exploring-elixir-map-access.html</id>
    <published>2018-02-18T15:42:00-05:00</published>
    <updated>2018-02-18T16:08:49-05:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;I missed a day&amp;hellip;damn. I did have a great day with my dad before he left to go home,
so I&amp;rsquo;m letting it slide. I have a last minute trip coming up as well, so I&amp;rsquo;ll be working
extra hard over the next few days to come up with some ideas and execute on them while
flying over the pond to Spain.&lt;/p&gt;

&lt;p&gt;Today, I want to dive deep into a common part of Elixir: map access. This topic is one
that I have taken for granted until today, and discovering how it is all handled was
quite an interesting adventure. While the end result of today&amp;rsquo;s post won&amp;rsquo;t be ground
breaking new ideas, it should give some idea on how I approach a new foreign code base.&lt;/p&gt;

&lt;h2&gt;Digging in for the first time (and failing)&lt;/h2&gt;

&lt;p&gt;I started this adventure because my co-worker Ben brought up that &lt;a href="https://hexdocs.pm/elixir/Access.html" target="_blank"&gt;&lt;code&gt;Access&lt;/code&gt;&lt;/a&gt;
is an interesting module that we take for granted. Even further than &lt;code&gt;Access&lt;/code&gt;, the
&lt;a href="https://hexdocs.pm/elixir/Kernel.html#get_in/2" target="_blank"&gt;&lt;code&gt;Kernel&lt;/code&gt;&lt;/a&gt; module has some interesting
functions related to access. I have never used these before, but I could see them being
pretty useful.&lt;/p&gt;

&lt;p&gt;I started my journey by pulling down the elixir-lang/elixir github repo. Having this
locally allows me to do some more powerful searches than Github allows, and also
works offline. I started with some basic greps for how &lt;code&gt;Access.get&lt;/code&gt; is exposed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;âžœ  elixir git:(master) grep -R &amp;quot;Access\.get(&amp;quot; lib/elixir/lib
lib/elixir/lib/access.ex:  Internally, `data[key]` translates  to `Access.get(term, key, nil)`.
lib/elixir/lib/kernel.ex:  def get_in(data, [h]), do: Access.get(data, h)
lib/elixir/lib/kernel.ex:  def get_in(data, [h | t]), do: get_in(Access.get(data, h), t)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alright. So nothing new is really here. The Elixir language itself only uses &lt;code&gt;Access.get&lt;/code&gt;
via &lt;code&gt;Kernel.get_in/2&lt;/code&gt;. Okay&amp;hellip;so let&amp;rsquo;s find out how &lt;code&gt;get_in&lt;/code&gt; is used. I&amp;rsquo;ve excluded iex&amp;gt;
here as that is for docs and wasn&amp;rsquo;t useful for the search:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;âžœ  elixir git:(master) grep -R &amp;quot;get_in(&amp;quot; lib/elixir/lib | grep -v &amp;quot;iex&amp;quot;
lib/elixir/lib/kernel.ex:  @spec get_in(Access.t(), nonempty_list(term)) :: term
lib/elixir/lib/kernel.ex:  def get_in(data, keys)
lib/elixir/lib/kernel.ex:  def get_in(data, [h]) when is_function(h), do: h.(:get, data, &amp;amp; &amp;amp;1)
lib/elixir/lib/kernel.ex:  def get_in(data, [h | t]) when is_function(h), do: h.(:get, data, &amp;amp;get_in(&amp;amp;1, t))
lib/elixir/lib/kernel.ex:  def get_in(nil, [_]), do: nil
lib/elixir/lib/kernel.ex:  def get_in(nil, [_ | t]), do: get_in(nil, t)
lib/elixir/lib/kernel.ex:  def get_in(data, [h]), do: Access.get(data, h)
lib/elixir/lib/kernel.ex:  def get_in(data, [h | t]), do: get_in(Access.get(data, h), t)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alright&amp;hellip;.so now I&amp;rsquo;ve hit my dead end. I &lt;em&gt;know&lt;/em&gt; that Access.get is what is used for
accessing a map like &lt;code&gt;map[:key]&lt;/code&gt; as the access.ex file documents as such, but I simply
can&amp;rsquo;t find it.&lt;/p&gt;

&lt;h2&gt;Start from beginning&lt;/h2&gt;

&lt;p&gt;This is when I remembered something that I saw Chris McCord write in Slack once, he
likes to use &lt;code&gt;Macro.expand&lt;/code&gt; to see what happens when macros are processed. This gives
the AST that Elixir is going to compile into erlang, so should give a good lead. I&amp;rsquo;ve
formatted the following to read more easily:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;iex(1)&amp;gt; Macro.expand((quote do: (map[:a])), __ENV__)
{
  {:., [], [Access, :get]},
  [],
  [{:map, [], Elixir}, :a]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Okay, this is what we expected to see but also tells us a lot. When &lt;code&gt;map[:a]&lt;/code&gt; is
expanded by the Elixir compiler, somehow Access.get is added in. We can see that this
also works for &lt;code&gt;map[:a][:b]&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;iex(2)&amp;gt; Macro.expand((quote do: (map[:a][:b])), __ENV__)
{
  {:., [], [Access, :get]},
  [],
  [
    {
      {:., [], [Access, :get]},
      [],
      [{:map, [], Elixir}, :a]
    },
    :b
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This revealed to me that I missed something crucial in searching the Elixir code:
I didn&amp;rsquo;t search the compiler source code, which lives in &lt;code&gt;lib/elixir/src&lt;/code&gt;. Let&amp;rsquo;s try
a search there:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;âžœ  elixir git:(master) grep -R &amp;quot;Access&amp;quot; lib/elixir/src
lib/elixir/src/elixir_erl_pass.erl:translate_remote(&amp;#39;Elixir.Access&amp;#39; = Mod, get, Meta, [Container, Value], S) -&amp;gt;
lib/elixir/src/elixir_parser.yrl:%% Access
lib/elixir/src/elixir_parser.yrl:  {{&amp;#39;.&amp;#39;, Meta, [&amp;#39;Elixir.Access&amp;#39;, get]}, Meta, [Expr, List]}.
lib/elixir/src/elixir_rewrite.erl:-define(access, &amp;#39;Elixir.Access&amp;#39;).
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ahhhh, there we go. The &lt;a href="https://github.com/elixir-lang/elixir/blame/master/lib/elixir/src/elixir_parser.yrl#L846" target="_blank"&gt;Elixir compiler expands on map access&lt;/a&gt;.
We can then follow the dots backwards to get how this is codified:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href="https://github.com/elixir-lang/elixir/blame/master/lib/elixir/src/elixir_parser.yrl#L280" target="_blank"&gt;&lt;code&gt;build_access&lt;/code&gt; is used in &lt;code&gt;bracket_expr&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/elixir-lang/elixir/blame/master/lib/elixir/src/elixir_parser.yrl#L238" target="_blank"&gt;parser notation for different &lt;code&gt;access_expr&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Much more parser notation by looking at each variable&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I won&amp;rsquo;t pretend to know the parser expressions and how they&amp;rsquo;re put together. I suspect
that I could spend a lot of time researching that topic in particular. However, at this
point we have our answer: the bracket expressions are handled by the Elixir compiler, and
expanded into &lt;code&gt;Access.get&lt;/code&gt; calls. They are not handled as normal macros and are engrained
into the language.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Thanks for reading the 17th post in my 28 days of Elixir. Keep up through the month of February to see if I can
stand subjecting myself to &lt;a href="/tags/28-days-of-elixir.html" target="_blank"&gt;28 days of straight writing&lt;/a&gt;. I
am looking for new topics to write about, so please reach out if there&amp;rsquo;s anything you
really want to see!&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>28 Days - pg2 basics - Use process groups for orchestration across a cluster</title>
    <link rel="alternate" href="https://stephenbussey.com/2018/02/17/pg2-basics-use-process-groups-for-orchestration-across-a-cluster.html"/>
    <id>https://stephenbussey.com/2018/02/17/pg2-basics-use-process-groups-for-orchestration-across-a-cluster.html</id>
    <published>2018-02-17T02:07:00-05:00</published>
    <updated>2018-02-17T03:13:06-05:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;One of my first major Elixir projects really cared about optimization up-front,
due to high throughput. This led to my mentality of &amp;ldquo;no database&amp;rdquo;; I would try
to always keep data in local heap rather than going to a database. I quickly
encountered the biggest challenge with this: how to keep the processes that
hold data in sync with each other. I looked at using &lt;a href="http://erlang.org/doc/man/pg2.html" target="_blank"&gt;pg2&lt;/a&gt;
for this task, and have been very happy with the outcome.&lt;/p&gt;

&lt;p&gt;An example repo is up at &lt;a href="https://github.com/sb8244/pg2_demo" target="_blank"&gt;https://github.com/sb8244/pg2_demo&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;pg2&lt;/h2&gt;

&lt;p&gt;&lt;a href="http://erlang.org/doc/man/pg2.html" target="_blank"&gt;pg2&lt;/a&gt; has nothing to do with postgres,
which is one of the most common thoughts when people see the module name.
It creates process groups, so that is where the name comes from.&lt;/p&gt;

&lt;p&gt;At the most basic explanation, pg2 allows for a group to be created and then
for processes to connect to the group. This leads to a mapping of name -&amp;gt; pid list.
The pid list consists of all known processes, whether they be local or remote.
When a pg2 group is created, that group becomes visible to all connected nodes
in the system. A pg2 group can be created multiple times without error, which
means that each node can call &lt;code&gt;create&lt;/code&gt; without error.&lt;/p&gt;

&lt;h2&gt;In practice&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;ll preface this by saying that my particular problem could be solved a number of
ways, this is just the way I approached it. Also, pg2 can be used many different ways.
If you see any better approach for either the problem or solution, please let me know!&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s walk through each section of a module found in the &lt;a href="https://github.com/sb8244/pg2_demo" target="_blank"&gt;demonstration repo&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;defmodule MyWorker.Synchronization do
  use GenServer

  # The topic could simply be __MODULE__, but I like having the human name in it as well
  @topic {:human_name, __MODULE__}

  def start_link do
    GenServer.start_link(__MODULE__, [])
  end

  def topic, do: @topic
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pg2 works by grouping processes, so we need a process to group. I satisfy this by
creating a process specifically for synchronization purposes. It would be possible
to link this to the main process (MyWorker above), but there would be less performance
with the serial nature of a process.&lt;/p&gt;

&lt;p&gt;The topic is just a tuple or atom, (erlang typespec is &amp;ldquo;any&amp;rdquo;), and I generally like to
have some human readability in the topic.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  def init([]) do
    :ok = :pg2.create(@topic)
    :ok = :pg2.join(@topic, self())
    {:ok, []}
  end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When the Synchronization GenServer starts, it is going to create the pg2 topic and
then join the topic itself. This is possible due to the property pointed out earlier
that &lt;code&gt;:pg2.create&lt;/code&gt; can be called multiple times successfully.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  def update(some_param) do
    :pg2.get_members(@topic)
    |&amp;gt; Kernel.--(:pg2.get_local_members(@topic))
    |&amp;gt; Enum.each(fn(pid) -&amp;gt;
      send pid, {:broadcast, @topic, {:update_from_db, some_param}}
    end)
  end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is the public API of the Synchronization module. The process group&amp;rsquo;s members
are retrieved, which is every pid (local and remote) added to the group. In our case,
it is only Synchronization module pids. Local processes are removed from this list for
performance reasons. In my use case, the data on the local node is already correct;
the local node does not need updated.&lt;/p&gt;

&lt;p&gt;Each pid is then enumerated and is sent a broadcast message which can actually be
any atom or tuple that we like. This can be useful for passing around parameters
such as the changed data or tenant information.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  def handle_info({:broadcast, @topic, {:update_from_db, some_param}}, state) do
    MySupervisor.for_param(some_param)
      |&amp;gt; MyWorker.load_from_db()
    {:noreply, state}
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, the message we passed around is handled. In this short demo, I&amp;rsquo;m not worrying
about sending the changed messages over the wire, instead I am just loading it from the database.
This can be desirable for simplicity&amp;rsquo;s sake if the data isn&amp;rsquo;t changing often.&lt;/p&gt;

&lt;h2&gt;A note on distributing data&lt;/h2&gt;

&lt;p&gt;Keeping multiple copies of the same data in memory, and up to date, across an entire
cluster is a pretty hard problem, without going into details of &lt;a href="https://en.wikipedia.org/wiki/CAP_theorem" target="_blank"&gt;CAP theorem&lt;/a&gt;.
In this pg2 solution, the data is eventually consistent. This means that some servers
may give an incorrect answer over no answer, because they don&amp;rsquo;t have the most recent
data yet.&lt;/p&gt;

&lt;p&gt;More stringent handling of the &lt;code&gt;send&lt;/code&gt; could be coded if needed, although considering
distributed data from the beginning is worth it.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Thanks for reading the 16th post in my 28 days of Elixir. Keep up through the month of February to see if I can
stand subjecting myself to &lt;a href="/tags/28-days-of-elixir.html" target="_blank"&gt;28 days of straight writing&lt;/a&gt;. I
am looking for new topics to write about, so please reach out if there&amp;rsquo;s anything you
really want to see!&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>28 Days - My favorite Elixir testing tool - Mockery</title>
    <link rel="alternate" href="https://stephenbussey.com/2018/02/15/my-favorite-elixir-testing-tool-mockery.html"/>
    <id>https://stephenbussey.com/2018/02/15/my-favorite-elixir-testing-tool-mockery.html</id>
    <published>2018-02-15T18:23:00-05:00</published>
    <updated>2018-02-15T20:33:13-05:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;I hate to admit it, but I&amp;rsquo;ve finally started truly unit testing with Elixir. I come
from the Ruby world where our large test suite often runs slowly due to things like
data insertion / access in the tests, large object graphs, etc. It&amp;rsquo;s easy to criticize
situations like this after the fact, but I find the reasons along the way were paved
with the best intentions. As I learn and adopt Elixir at SalesLoft, I&amp;rsquo;ve been extra
careful to avoid these situations from playing out again.&lt;/p&gt;

&lt;p&gt;The biggest tool in my arsenal so far has been &lt;a href="https://github.com/appunite/mockery" target="_blank"&gt;Mockery&lt;/a&gt;, a mocking tool that
allows the test suite to be run in parallel. This seems like a natural thing to expect
from a mocking tool, but some others that I used do not have this property. I think
that the design of Mockery also leads to cleaner code, so I&amp;rsquo;ve adopted it.&lt;/p&gt;

&lt;h2&gt;Mockery Usage&lt;/h2&gt;

&lt;p&gt;The Github for Mockery lays out all different usage possibilities for it, I&amp;rsquo;ll just
cover the one or two that I use most often.&lt;/p&gt;

&lt;p&gt;The first, and most common case, is mocking out expensive operations. An example of this is network requests;
I&amp;rsquo;m able to use tools to test these requests (topic for another post?), in their specific modules,
but then keep these requests out of other places. Another example is for database access. I
can write query objects or repositories to handle the fetching / insertion of data, but
then mock out these modules in their usage throughout a system. Here&amp;rsquo;s an example usage:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;defmodule MyRequest do
  @widgets_query Mockery.of(&amp;quot;WidgetsQuery&amp;quot;)

  def call(authentication_context) do
    @widgets_query.call(authentication_context)
  end
end

defmodule MyRequestTest do
  use ExUnit.Case, async: true
  use Mockery

  test &amp;quot;call returns the widgets query&amp;quot; do
    auth = %{}
    mock WidgetsQuery, :call, [%Widget{id: 1}, %Widget{id: 2}]
    assert MyRequest.call(auth) == [%Widget{id: 1}, %Widget{id: 2}]
    assert_called WidgetsQuery, :call, [^auth]
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is about the most basic usage to Mockery possible. The module is mocked
for this process only, then used by the request. After the call, we can check that
it was called properly. This allows the important parts of a mock to be fully covered
in our test suite: params and results.&lt;/p&gt;

&lt;p&gt;There is another usage of Mockery that I&amp;rsquo;ve found useful, it&amp;rsquo;s to implement a mock that
always exists unless specified by a test. In the above mock example, if we wrote another
test, that module would not be mocked anymore, leading to a real database call. This
may not be desired if we end up mocking a module in a lot of places. Enter &lt;code&gt;by:&lt;/code&gt; keyword:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@widgets_query Mockery.of(&amp;quot;WidgetsQuery&amp;quot;, by: &amp;quot;WidgetsQuery.Mock&amp;quot;)

defmodule WidgetsQuery do
  defmodule Mock do
    def call(_auth), do: []
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the above usage of Mockery, any call in a normal test will return an empty list.
This may not be desirable for certain cases, so your mileage will vary with such a
technique.&lt;/p&gt;

&lt;h2&gt;Mockery for Processes&lt;/h2&gt;

&lt;p&gt;Another really powerful use case of mocking, that I&amp;rsquo;ve found and think warrants a
separate mention, is for process based communication. In practice, testing processes
can get a bit hairy. Timing, process ownership, race conditions are all easily possible
in tests due to tests running in a process and our code being able to run in processes.
Mockery can help out here by mocking out the explicit boundaries between processes.&lt;/p&gt;

&lt;p&gt;While I find a ton of power in Mockery for this use case, I urge even more caution than
others. It would be possible to devise an incorrect process tree that works in tests
solely because of mocking. Don&amp;rsquo;t shy away from mocking, but don&amp;rsquo;t take to it immediately
either.&lt;/p&gt;

&lt;h2&gt;Dangers of mocking&lt;/h2&gt;

&lt;p&gt;Mocking is not a silver bullet for tests. It can certainly help greatly with common
expensive operations and allows for testing of boundaries rather than going past
the boundaries in each test. However, what if our boundaries are not correct?&lt;/p&gt;

&lt;p&gt;A few common issues arise with mocking. It is possible to setup a mock on a function
that doesn&amp;rsquo;t exist, on params that are not reflective of reality, and on results that
are not part of the type signature of the function. Each of these scenarios has the
potential for a worst case testing scenario: code that passes locally but fails in
QA or production testing.&lt;/p&gt;

&lt;p&gt;Mockery does help with the function not existing, as it requires mocking out functions
that do exist in the target module. This can even work on arity (ensure that the right
arity is mocked out). It cannot, unfortunately, help with the input or output not being
real. Engineers must be on the lookout for these issues and &lt;em&gt;diligently&lt;/em&gt; use mocking,
ensuring that the usage is correct at write-time and also after any refactors.&lt;/p&gt;

&lt;p&gt;I haven&amp;rsquo;t done so, but my guess is that typespecs might be able to help out here. I don&amp;rsquo;t
see any obvious integrations with Mockery, but anything is possible and could be implemented
custom based on the typespec metadata.&lt;/p&gt;

&lt;h2&gt;Why Mockery over others?&lt;/h2&gt;

&lt;p&gt;I had some problems with other mocking tools that dynamically switch out modules
globally. These types of tools require that test processes run synchronously rather
than asynchronously. This is not a big deal for small test suites, but could be a huge
limiter for a large test suite. At the scale of our product, I have to assume that a
service could become large over time, and so I&amp;rsquo;m very careful about test speed.&lt;/p&gt;

&lt;p&gt;I also find that defining what is mockable in the module (rather than anything being
mockable via the test suite) allows my code to be more readable and explicit. All of
a sudden, I can tell exactly what is mockable rather than assuming that anything is
mockable. If I saw someone mocking an &lt;code&gt;Enum&lt;/code&gt; function, for example, I would have
an immediate red flag raised. Seeing this play out in the module rather than the
test really does help.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Thanks for reading the 15th post in my 28 days of Elixir. Keep up through the month of February to see if I can
stand subjecting myself to &lt;a href="/tags/28-days-of-elixir.html" target="_blank"&gt;28 days of straight writing&lt;/a&gt;. I
am looking for new topics to write about, so please reach out if there&amp;rsquo;s anything you
really want to see!&lt;/p&gt;
</content>
  </entry>
</feed>

<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Stephen Bussey's Software Engineering Blog</title>
  <id>https://stephenbussey.com</id>
  <link href="https://stephenbussey.com"/>
  <link href="https://stephenbussey.com/feed.xml" rel="self"/>
  <updated>2020-05-06T17:21:00-04:00</updated>
  <author>
    <name>Stephen Bussey</name>
  </author>
  <entry>
    <title>My Experiences Writing Real-Time Phoenix</title>
    <link rel="alternate" href="https://stephenbussey.com/2020/05/06/my-experiences-writing-real-time-phoenix.html"/>
    <id>https://stephenbussey.com/2020/05/06/my-experiences-writing-real-time-phoenix.html</id>
    <published>2020-05-06T17:21:00-04:00</published>
    <updated>2020-05-15T16:02:47-04:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;The initial itch to write a book formed over a couple of weeks around October of 2018. Of course, I was very naïve at the time. I thought that a book could be written in three weeks if one simply battened down the hatches and got it done&amp;mdash;surely it would be done by Christmas. This may be true for some people, but the reality for me was quite different. This post goes through many of the different aspects of my writing journey.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m sure that everyone has a very different journey when writing a book, so take mine with a grain of salt. I&amp;rsquo;m hoping that you will find it useful if you&amp;rsquo;re thinking about writing, or maybe this post will help plant that seed. Maybe you&amp;rsquo;re just curious at the process of writing.&lt;/p&gt;

&lt;p&gt;This post will be a bit longer than usual as I want to make sure that the different aspects are explained in-depth.&lt;/p&gt;

&lt;p&gt;There is a 50% off code at the end of this post that is good until May 17, 2020. It&amp;rsquo;s the best deal you&amp;rsquo;re going to find!&lt;/p&gt;

&lt;h2&gt;Why I Decided to Write&lt;/h2&gt;

&lt;p&gt;I was working on a project to extract the &amp;ldquo;live feed&amp;rdquo; feature out of the SalesLoft Rails monolith. The actual API surface and moving parts are very simple, so this was a good candidate for a rewrite into a Phoenix-backed application. I spent a couple of weeks on the implementation and things were going well—it was time to roll it out.&lt;/p&gt;

&lt;p&gt;Despite the initial speed of developing the application, the rollout process took about 5 months. This was due to all of the little things that I took for granted coming to light. To be clear, the issues I ran into were not things that Phoenix lacked. Instead, they were the little nuances of writing a real-time app that I just didn&amp;rsquo;t know about. For example, I brought down the main application at least twice by overwhelming the monolith with thousands of simultaneous requests from the Elixir backend. (If you read the book, this is an example of an unintentional data pipeline, from Chapter 6.)&lt;/p&gt;

&lt;p&gt;I emerged from that project with a lot more Elixir knowledge and a good primer for how to write real-time apps. I was pretty convinced that my experiences on the project would be good to share with the world. I wrote some blog posts about it and got good reception on those, but I kept seeing people running into similar issues on Slack and the Forums. This cemented the idea as a good one for writing about. After writing a few more real-time features for SalesLoft, I knew it would be a good time to write.&lt;/p&gt;

&lt;h2&gt;Time to Find a Publisher&lt;/h2&gt;

&lt;p&gt;It seems that there are only a few major publishers in the Elixir world. The most prominent ones are Pragmatic Bookshelf and Manning Publications. Each publisher has an ideas email where you can shoot them an idea and they&amp;rsquo;ll let you know if they want a formal proposal or not. I submitted first to Pragmatic and&amp;hellip;the results were bad. I tried a few more times with them but I just couldn&amp;rsquo;t make it work. However, my call with Manning seemed to go well and I sent them a formal proposal.&lt;/p&gt;

&lt;p&gt;Writing a proposal is hard work. It was really the first dive into the book&amp;rsquo;s content that I took. I wrote pretty extensive outlines (I think something like 12 pages of just outlining) and had great feedback from the ten reviewers. The process of doing the proposal was one of the most valuable early stage things that I did though, because it forced me to flesh out my idea more and to realize what would and would not work. Despite the good feedback from Manning, I still really wanted to try to get something going with Pragmatic.&lt;/p&gt;

&lt;p&gt;I ran into Bruce Tate at Lonestar Elixir 2019. I asked him about writing and explained where I was at in the process with Manning. He gave me some great advice on how to tweak my initial proposal that I sent to Pragmatic:&lt;/p&gt;

&lt;p&gt;&lt;i&gt;&lt;strong&gt;Focus on what you can do with the technology, and less on the technology itself.&lt;/strong&gt;&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;My initial proposal was all about WebSocket and writing apps with them. This was a bad proposal. He got me to flip this a bit into building scalable real-time systems (the final product vs the technology used).&lt;/p&gt;

&lt;p&gt;The new short-form proposal was well-received by the team. I was asked to send a formal proposal, which involves writing a meaty chapter from the book. This ended up being thirty pages of content. They reviewed this and it was accepted! I was going to write with Pragmatic! I was actually on the ski chairlift with my dad when the call from Bruce came in about it. I was so happy that I&amp;rsquo;m pretty sure I cried a bit.&lt;/p&gt;

&lt;p&gt;The process between Lonestar Elixir to a signed contract was only about three weeks. I was incredibly impressed by Pragmatic&amp;rsquo;s speed here, and knew it would be a sign of a good relationship.&lt;/p&gt;

&lt;h2&gt;The First Few Weeks&lt;/h2&gt;

&lt;p&gt;I was sent credentials to the Pragmatic build system, my book&amp;rsquo;s repository, a guide on how to write in their style (written using the same formatting as their books of course), and an introduction to my editor—Jackie. Everything came together quickly, so this felt very surreal and I wasn&amp;rsquo;t sure where to start.&lt;/p&gt;

&lt;p&gt;They coached through the beginning of the writing process. I put together a more built-out outline for every chapter and stubbed out chapters with some basic flow information. The initial ask was to put together chapters 2-4, with a sync up on style and writing after each. I buckled down and got to writing. I was given a bit of guidance (and had the writing style guide), but was told to predominately just stay in my own style for these initial chapters.&lt;/p&gt;

&lt;p&gt;The first chapters were difficult to write, to say the least. More difficult, however, was going through the chapters with Jackie and figuring out how to make them better. Each chapter would receive about fifty comments, so there was a lot to deal with. Like a PR, it is difficult to sent multiple weeks worth of work to someone else and have it broken down line by line. To anyone that hits this wall, it&amp;rsquo;s tough to deal with. Just keep learning, taking notes, adjusting, and pressing forward.&lt;/p&gt;

&lt;p&gt;One humorous anecdote during the initial weeks is that I sent Jackie an initial timeline which had me finished with the book in August, only a few months out. She let me know that this was quite ambitious and to basically double or triple that estimate to get to the final book. Engineers are always under or over estimating, it seems.&lt;/p&gt;

&lt;h2&gt;The Form of a Technical Book&lt;/h2&gt;

&lt;p&gt;One of the things that I hear most often when discussing writing a book is, &amp;ldquo;I could never do that, I can&amp;rsquo;t write 6 pages let alone 25, then 10+ chapters!&amp;rdquo; It is true that books are long and take time, but there&amp;rsquo;s a formula that honestly really helps out with the writing process.&lt;/p&gt;

&lt;p&gt;Every single one of my chapters, without fail, started with a basic outline that went 2 levels deep. The top heading would be the section&amp;rsquo;s title in the table of contents, and subheadings would be solely for breaking down content for the reader. Establishing this outline in a single sitting let me really clearly construct where I wanted the chapter to go, without anything context switching me away from it. I would note things that I wanted to especially hit on, as well. Once the outline was written, I would walk away from writing for the day.&lt;/p&gt;

&lt;p&gt;Each time that I sat down to write, I would start by establishing which of the subsections I wanted to write before calling it for the day. This gave me a goal that I could push for. To be honest, I wasn&amp;rsquo;t super great at hitting this all of the time, which I&amp;rsquo;ll touch on in the next section. I would try to complete a subheading in a single sitting to ensure continuity in my thoughts.&lt;/p&gt;

&lt;p&gt;When writing a subheading, it follows a pretty predictable formula again and again. There is an introduction paragraph that goes into what the section will be about. This sets the expectations for the reader. There are then 3-5 meaty paragraphs going over the content of the section. Sometimes the section would get broken down into another level of headings, if it was an especially meaty one. There is then a final paragraph that sets the reader up for the next section they&amp;rsquo;re going to read.&lt;/p&gt;

&lt;p&gt;This formula repeats for every section and every chapter of the entire book. I think that writing this way really helped me push through any mental barriers, and it also gives the reader knowledge of what they&amp;rsquo;re going to learn in the chapter and the sections of the chapter.&lt;/p&gt;

&lt;p&gt;A chapter follows this same formula. There is an introduction section of roughly four paragraphs. This is followed by the various sections (as mentioned above), and then a &amp;ldquo;Wrapping Up&amp;rdquo; section that closes out the chapter, and more importantly preps the reader for the next chapter. When you write this way, all of a sudden hitting 20+ pages per chapter doesn&amp;rsquo;t seem so bad.&lt;/p&gt;

&lt;h2&gt;The Long Haul&lt;/h2&gt;

&lt;p&gt;I was getting a pretty good rhythm going. Jackie was pleased with my writing, ability to quickly address feedback, and to reduce the number of repeat mistakes I would make. The number of times that I would get a chapter back from review, and the number of comments in the review, started to drop for each chapter. At the beginning, it would take at least three review rounds with 50, 35, 10 comments or so. By the end, it would take maybe two review rounds of 20, 5 comments.&lt;/p&gt;

&lt;p&gt;However, I also hit a pretty big mental wall. I made good progress on Part 1 of the book (6 chapters), and started on Part 2. Part 2 was so much harder because the project that we make in the book continues for the rest of the book. It turns out that writing a coherent and useful example that spans many chapters is really hard. I went from giving Jackie a chapter every two weeks to about one every four weeks. That wasn&amp;rsquo;t good.&lt;/p&gt;

&lt;p&gt;Writing during this time would often involve me sitting down at my computer and either doing nothing or doing very little for two hours. I felt pretty defeated, but I just couldn&amp;rsquo;t get it going. Another thing that I noticed is that I had a really hard time starting a new chapter, but I would push through a chapter pretty quickly once I started it. This is all pretty synonymous with burnout, which I&amp;rsquo;m sure there was a bit of.&lt;/p&gt;

&lt;p&gt;The thing that saved me the most during this time was ElixirConf 2019. I was doing a training on real-time application development there, and I wanted to have the book in a beta release by then. A beta release is reserved for books that are ~60% done and have a good velocity to the finish. I pushed really hard to get the necessary chapters done for the beta release and was successful at it. Pragmatic really did me some favors during this time, though, as another author gave up their beta spot by two weeks to help make sure that mine hit the goal. I appreciated that a ton.&lt;/p&gt;

&lt;p&gt;This long haul definitely ties into the hardest thing, for me, that happened when writing.&lt;/p&gt;

&lt;h2&gt;The Hardest Part of Writing a Book&lt;/h2&gt;

&lt;p&gt;The hardest part of me writing this book was not the technical or writing aspect of it. Instead, it was the gnawing feeling that I should be writing. Hanging out with friends on the weekend? (You should really be writing right now.) Christmas party for work? (Don&amp;rsquo;t drink much, you need to hit that writing goal, and you took a different day off.) Cycling on my Tuesday night ride? (You need to get done early so you can get an hour of writing in.) Basically everything that I did came with this feeling in the back of my head. Even if I wasn&amp;rsquo;t planning on writing that night, it was still prevalent. I only didn&amp;rsquo;t feel this way when I had just completed a chapter and sent it to Jackie.&lt;/p&gt;

&lt;p&gt;I put this here mainly as a warning and for something to consider if you&amp;rsquo;re looking to write a book. I haven&amp;rsquo;t really talked to many other authors about this feeling, so I&amp;rsquo;m curious if you wrote a book and the same thing happened to you. I have never felt this experience when it came to coding or other work, so it was new for me.&lt;/p&gt;

&lt;h2&gt;Refactoring&lt;/h2&gt;

&lt;p&gt;I have typically not been great at refactoring my writing. I will proofread this post once and ship it. I think it&amp;rsquo;s important to not get bogged down in the little nuances, as the alternative can be paralyzing and make it difficult to deliver content. However, this is clearly not how a book works.&lt;/p&gt;

&lt;p&gt;The story I want to share here is about chapter 3, 4, and 5 of the book. Believe it or not, these chapters started life as a single chapter. It was quite the monster, coming in at nearly 45 pages. The sample chapter that I wrote is closest to these chapters, which is part of why it may have ended up this way.&lt;/p&gt;

&lt;p&gt;I worked hard with my editor to find all of the seams in the monster chapter to see how it could be broken apart. Doing this involves changing almost all of the transition text (introduction and transition paragraphs in the earlier described form) and also requires checking continuity. So it&amp;rsquo;s not a small thing. I was also able to expand more on the content in these chapters because they no longer were crammed in way too tight. I am proud of the final result of this work, though, as I&amp;rsquo;ve gotten compliments on the way that the material in these chapters is delivered.&lt;/p&gt;

&lt;p&gt;I felt this same satisfaction anytime that I removed content. Removing content from a book is great, as it usually means that the concept can be described more simply and more succinctly somewhere else. Or it means that the content wasn&amp;rsquo;t important and that the reader no longer has to deal with carrying it in their head.&lt;/p&gt;

&lt;p&gt;Jackie was great at helping me see opportunities for refactoring my chapters. It&amp;rsquo;s her job to deliver a well-flowing book, and she does a great job at coaching through that.&lt;/p&gt;

&lt;h2&gt;Shipping The Book&lt;/h2&gt;

&lt;p&gt;The first release of the book came in August 2019 with the Beta release. This was definitely a big success for me, as it gave me motivation to keep pushing forward and also vetted the content with errata submissions. The final version released in April and has been available on &lt;a href="https://www.amazon.com/gp/product/1680507192" target="_blank"&gt;Amazon&lt;/a&gt; and &lt;a href="https://pragprog.com/book/sbsockets/real-time-phoenix" target="_blank"&gt;Pragmatic Bookshelf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m very happy with the final result. I&amp;rsquo;ve gotten some unsolicited praise from people that I&amp;rsquo;ve never met on the way that the book flows and the information presented in it. I think that there is always room for improvement, but it is important to be satisfied with a job well done.&lt;/p&gt;

&lt;div style="text-align: center"&gt;
  &lt;img src="/images/holding-book.jpg" alt="Steve Bussey holding Real-Time Phoenix" style="border: 1px solid #ccc" /&gt;
&lt;/div&gt;

&lt;h2&gt;Thanks&lt;/h2&gt;

&lt;p&gt;Many people helped make this book possible. In particular, a decent number of technical reviewers contributed to vetting the material and helping to reduce the number of errors. This book would not have been the same without their suggestions and reviews. A large thanks to each of you: Amos King, Ben Olive, Chris Keathley, Dan Dresselhaus, Grant Powell, Gábor László Hajba, Johanna Larsson, John Oxford, Ray Gesualdo, Stefan Turalski, and Ulisses H. F. de Almeida.&lt;/p&gt;

&lt;p&gt;I also want to thank the community and maintainers of libraries. Phoenix is amazing and brings me joy. Part of the reason for this is that it is so well-maintained by Chris McCord and others who contribute to it. Chris and others also contributed with their one-one conversations and forum posts.&lt;/p&gt;

&lt;h2&gt;What&amp;rsquo;s Next?&lt;/h2&gt;

&lt;p&gt;Who knows, especially right now. I&amp;rsquo;m hoping to be able to continue trainings and presentations around the content in the book at conferences and meetups once things get back to normal. While I enjoyed the experience, I can say for sure that I&amp;rsquo;m not writing another book for a little bit. :)&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m excited to get back into writing some greenfield software. I have a few ideas in my mind that I&amp;rsquo;m excited to flesh out more over the next few months.&lt;/p&gt;

&lt;h2&gt;A Discount for the Weekend&lt;/h2&gt;

&lt;p&gt;There is a discount that is active on Pragmatic right now. You can get the Real-Time Phoenix ebook for 50% off, which is insane. You will not get a better price for as long as I can anticipate.&lt;/p&gt;

&lt;p&gt;You can use the coupon code &lt;strong&gt;Frameworks2020&lt;/strong&gt; for 50% off. This expires on May 17, 2020, although I&amp;rsquo;m not sure at what time. Purchase the book on &lt;a href="https://pragprog.com/book/sbsockets/real-time-phoenix" target="_blank"&gt;Pragmatic Bookshelf&lt;/a&gt; with this coupon code to redeem your 50% off! You can see which other books are available for this deal in the &lt;a href="https://media.pragprog.com/newsletters/2020-05-11.html" target="_blank"&gt;Pragmatic Newsletter&lt;/a&gt;.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Verifying Queries with Ecto's prepare_query Callback</title>
    <link rel="alternate" href="https://stephenbussey.com/2019/12/30/verifying-queries-with-ecto-s-prepare-query.html"/>
    <id>https://stephenbussey.com/2019/12/30/verifying-queries-with-ecto-s-prepare-query.html</id>
    <published>2019-12-30T13:28:00-05:00</published>
    <updated>2019-12-30T14:35:52-05:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;You can use many different techniques to build and scale Software as a Service applications. One technique that is very popular is
to use a single database for multiple paying customers. This multi-tenant approach to SaaS works well for many people, but there are
a few dangers to look out for. The biggest danger is the risk of cross-tenant data leaking. I consider this the worst possible scenario
for a multi-tenant SaaS application, even beyond a full system outage.&lt;/p&gt;

&lt;p&gt;In this post, we&amp;rsquo;re going to look at a technique to guarantee that cross-tenant leaks don&amp;rsquo;t happen in an Elixir application. We&amp;rsquo;ll be
looking at Ecto&amp;rsquo;s new(ish) &lt;code&gt;prepare_query&lt;/code&gt; callback and how it can be used to inspect (almost) every query that goes through your
application. I&amp;rsquo;ll discuss how I test drove a query inspector to inspect every query for tenancy.&lt;/p&gt;

&lt;p&gt;The ultimate goal of this post is to serve as a light reference for how I navigated the &lt;code&gt;Ecto.Query&lt;/code&gt; struct to implement
the tenancy enforcer. The biggest challenge that I faced was understanding what went into the structure and how to walk it.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s first look at what &lt;code&gt;prepare_query&lt;/code&gt; is.&lt;/p&gt;

&lt;h2&gt;Ecto&amp;rsquo;s &lt;code&gt;prepare_query&lt;/code&gt; Callback&lt;/h2&gt;

&lt;p&gt;The &lt;a href="https://hexdocs.pm/ecto/Ecto.Repo.html#c:prepare_query/3" target="_blank"&gt;&lt;code&gt;prepare_query&lt;/code&gt;&lt;/a&gt; callback was introduced in September of 2019. You
can define a function (&lt;code&gt;prepare_query&lt;/code&gt;) in your &lt;code&gt;Application.Repo&lt;/code&gt; module. The function is invoked before a query is executed, and you
are provided the full query structure as well as some metadata.&lt;/p&gt;

&lt;p&gt;With &lt;code&gt;prepare_query&lt;/code&gt;, you can inspect the query or even modify it. The Ecto documentation gives an example where a Repo filters out
&amp;ldquo;soft deleted&amp;rdquo; records unless the user is an admin. It looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;# From https://hexdocs.pm/ecto/Ecto.Repo.html#c:prepare_query/3

@impl true
def prepare_query(_operation, query, opts) do
  if opts[:admin] do
    {query, opts}
  else
    query = from(x in query, where: is_nil(x.deleted_at))
    {query, opts}
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can use this to detect whether a query has tenancy set correctly. We won&amp;rsquo;t actually modify the query in this post, due to
concerns I have with how that approach removes multi-tenancy awareness.&lt;/p&gt;

&lt;p&gt;In order to get started, we&amp;rsquo;ll need to define a function that shows us the &lt;code&gt;Ecto.Query&lt;/code&gt; structure. Let&amp;rsquo;s do that next.&lt;/p&gt;

&lt;h2&gt;Understanding Ecto Query Structure from Inspect Protocol&lt;/h2&gt;

&lt;p&gt;In order to get started with a runnable example, I modified
&lt;a href="https://github.com/sb8244/ecto_tenancy_enforcer/blob/master/test/support/tenancy/repo.ex" target="_blank"&gt;&lt;code&gt;test/support/tenancy/repo.ex&lt;/code&gt;&lt;/a&gt;
by replacing &lt;code&gt;prepare_query&lt;/code&gt; with the following empty function.&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;  def prepare_query(_operation, query, opts) do
    IO.inspect query
    {query, opts}
  end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I can then run the test &lt;code&gt;&amp;quot;valid tenancy is only condition&amp;quot;&lt;/code&gt; with &lt;code&gt;mix test test/integration/prepare_test.exs:26&lt;/code&gt;, and see the following
output:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;➜  ecto_tenancy_enforcer git:(master) ✗ mix test test/integration/prepare_test.exs:26
#Ecto.Query&amp;lt;from s0 in &amp;quot;schema_migrations&amp;quot;, lock: &amp;quot;FOR UPDATE&amp;quot;,
 select: type(s0.version, :integer)&amp;gt;
Including tags: [line: &amp;quot;26&amp;quot;]
Excluding tags: [:test]

#Ecto.Query&amp;lt;from c0 in Tenancy.Company, where: c0.tenant_id == 1, select: c0&amp;gt;
.

Finished in 0.2 seconds
43 tests, 0 failures, 42 excluded
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s not quite helpful, because the &lt;code&gt;Ecto.Query&lt;/code&gt; is printed out in text form. We can add &lt;code&gt;structs: false&lt;/code&gt; to the &lt;code&gt;IO.inspect&lt;/code&gt; call
and we get a different result.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;%{
  __struct__: Ecto.Query,
  aliases: %{},
  assocs: [],
  combinations: [],
  distinct: nil,
  from: %{
    __struct__: Ecto.Query.FromExpr,
    as: nil,
    hints: [],
    prefix: nil,
    source: {&amp;quot;companies&amp;quot;, Tenancy.Company}
  },
  group_bys: [],
  havings: [],
  joins: [],
  limit: nil,
  lock: nil,
  offset: nil,
  order_bys: [],
  prefix: nil,
  preloads: [],
  select: %{
    __struct__: Ecto.Query.SelectExpr,
    expr: {:&amp;amp;, [], [0]},
    fields: nil,
    file: &amp;quot;/Users/stephenbussey/src/ecto_tenancy_enforcer/deps/ecto/lib/ecto/query/planner.ex&amp;quot;,
    line: 814,
    params: [],
    take: %{}
  },
  sources: nil,
  updates: [],
  wheres: [
    %{
      __struct__: Ecto.Query.BooleanExpr,
      expr: {:==, [],
       [
         {{:., [], [{:&amp;amp;, [], [0]}, :tenant_id]}, [], []},
         %{
           __struct__: Ecto.Query.Tagged,
           tag: nil,
           type: {0, :tenant_id},
           value: 1
         }
       ]},
      file: &amp;quot;/Users/stephenbussey/src/ecto_tenancy_enforcer/test/integration/prepare_test.exs&amp;quot;,
      line: 27,
      op: :and,
      params: []
    }
  ],
  windows: [],
  with_ctes: nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alright, now we have the actual struct that we can work with. We are capable of writing a query, inspecting the struct, and then figuring
out how to walk / enforce that query. If we continued with TDD at this point, we&amp;rsquo;d eventually hit a snag. &lt;code&gt;Ecto.Query&lt;/code&gt; represents
referenced tables (joins, some wheres) with a positional index system. However, the structure doesn&amp;rsquo;t include
the list of positional references&amp;mdash;you have to build it yourself.&lt;/p&gt;

&lt;p&gt;This led to a bit of a pickle, because it&amp;rsquo;s not documented anywhere and is considered an internal query structure. However, we know
of at least one place that knows how to resolve a positional index into a table / module name (Ecto.Query inspect). Let&amp;rsquo;s find that
and see what it&amp;rsquo;s doing.&lt;/p&gt;

&lt;p&gt;A search for &lt;code&gt;defimpl&lt;/code&gt; in Ecto brings us to &lt;a href="https://github.com/elixir-ecto/ecto/blob/b3ee240e91deecfd6d8727946bfc8ed5de752e4f/lib/ecto/query/inspect.ex#L21" target="_blank"&gt;this line&lt;/a&gt;,
which is the implementation of &lt;code&gt;Inspect&lt;/code&gt; for &lt;code&gt;Ecto.Query&lt;/code&gt;. In particular, the following code is of interest to us:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;  defp to_list(query) do
    names =
      query
      |&amp;gt; collect_sources
      |&amp;gt; generate_letters
      |&amp;gt; generate_names
      |&amp;gt; List.to_tuple()

    ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With this lead, it&amp;rsquo;s possible to use these same functions to create a positional index lookup. You can see a finished example of this
in &lt;a href="https://github.com/sb8244/ecto_tenancy_enforcer/blob/master/lib/ecto_tenancy_enforcer/source_collector.ex" target="_blank"&gt;&lt;code&gt;EctoTenancyEnforcer.SourceCollector&lt;/code&gt;&lt;/a&gt;. I modified this to return the schema module name, rather than a letter or table name, so that I could use my
&lt;code&gt;enforced_schemas&lt;/code&gt; option to check if a referenced table needs checked or not.&lt;/p&gt;

&lt;p&gt;We have about as much as we&amp;rsquo;re going to get out of Ecto. At this point, it&amp;rsquo;s possible to jump in and start coding a query verifier to
do whatever you want. I found myself still a bit lost at this point, but I did know what I wanted to pass / fail. I&amp;rsquo;ll walk through
the steps I took to test drive a query enforcer.&lt;/p&gt;

&lt;h2&gt;Test Driving EctoTenancyEnforcer&lt;/h2&gt;

&lt;p&gt;I don&amp;rsquo;t practice test driven development often, but I find it valuable when I have no clue what the solution is going to be, but I
know what I want it to look like. That is the case in a query enforcer, because it&amp;rsquo;s easy to write out queries that should pass and
fail into a test suite. I can then go one-by-one to make the incorrect ones fail, while still having the valid ones pass.&lt;/p&gt;

&lt;p&gt;I started with a set of very simple tests, like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;test &amp;quot;no filters at all&amp;quot; do
  assert_raise(TenancyViolation, fn -&amp;gt;
    Repo.all(Company)
  end)
end

test &amp;quot;valid tenancy is only condition&amp;quot; do
  valid = from c in Company, where: c.tenant_id == 1
  assert Repo.all(valid) |&amp;gt; length == 1
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I started with these to build some simple confidence. Once they were passing, I listed out about 40 queries that I knew should work
or not work. I chipped my way through these and eventually found the patterns in the &lt;code&gt;Ecto.Query&lt;/code&gt; struct that led to fairly clean code to
walk them.&lt;/p&gt;

&lt;p&gt;The final result can be seen in the &lt;a href="https://github.com/sb8244/ecto_tenancy_enforcer/blob/master/lib/ecto_tenancy_enforcer/query_verifier.ex" target="_blank"&gt;&lt;code&gt;EctoTenancyEnforcer.QueryVerifier&lt;/code&gt;&lt;/a&gt; module. I&amp;rsquo;m sure that there are cases I missed in my TDD, but I&amp;rsquo;m happy enough with this to
use it in production applications. I&amp;rsquo;ll add new tests as cases are encountered in the wild.&lt;/p&gt;

&lt;h2&gt;Wrapping Up&lt;/h2&gt;

&lt;p&gt;Ecto&amp;rsquo;s &lt;code&gt;prepare_query&lt;/code&gt; callback is incredibly powerful for query inspection and modification. It&amp;rsquo;s a bit dense to get started with,
due to the &lt;code&gt;Ecto.Query&lt;/code&gt; structure being undocumented, but TDD helped me out significantly. The &lt;code&gt;Ecto.Query&lt;/code&gt; walking is undocumented,
but I&amp;rsquo;m anticipating stability in the Query structure going into the future. That may or may not pan out, but I think it&amp;rsquo;s a decent
bet.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re looking for tenancy enforcement in Ecto queries, try out &lt;a href="https://github.com/sb8244/ecto_tenancy_enforcer" target="_blank"&gt;EctoTenancyEnforcer&lt;/a&gt;.
You can refer to this repo as a complete example of query enforcement.&lt;/p&gt;

&lt;h2&gt;The Book Plug&lt;/h2&gt;

&lt;p&gt;My book &amp;ldquo;Real-Time Phoenix: Build Highly Scalable Systems with Channels&amp;rdquo; is now in beta
through &lt;a href="http://bit.ly/rtp-exq" target="_blank"&gt;The Pragmatic Bookshelf&lt;/a&gt;. This book explores using Phoenix Channels, GenStage, and more to build
real-time applications in Elixir. The first draft has been completed for a little bit and the book should be in production by
February, with print coming at the end of the production process.&lt;/p&gt;

&lt;div style="text-align: center"&gt;
  &lt;a href="http://bit.ly/rtp-ecto-tenancy" target="_blank"&gt;
    &lt;img src="/images/sbsockets.jpg" alt="Real-Time Phoenix by The Pragmatic Bookshelf" height="300px" style="border: 1px solid #ccc" /&gt;
  &lt;/a&gt;
&lt;/div&gt;
</content>
  </entry>
  <entry>
    <title>Improve Exq Writes With Pooling</title>
    <link rel="alternate" href="https://stephenbussey.com/2019/10/01/improve-exq-writes-with-pooling.html"/>
    <id>https://stephenbussey.com/2019/10/01/improve-exq-writes-with-pooling.html</id>
    <published>2019-10-01T00:26:00-04:00</published>
    <updated>2019-10-01T01:57:19-04:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;&lt;a href="https://hex.pm/packages/exq" target="_blank"&gt;Exq&lt;/a&gt; is a background job processing library written in Elixir. It uses Redis, via the
Redix library, to store and then retrieve jobs. In this post, we&amp;rsquo;ll look at the performance of writing jobs into Redis
via the &lt;code&gt;Exq.Enqueuer&lt;/code&gt; API. You&amp;rsquo;ll see several benchmarks that utilize a single &lt;code&gt;Enqueuer&lt;/code&gt;, a poolboy queue, and a named
process pool.&lt;/p&gt;

&lt;p&gt;The repo for the benchmark and sample application is at &lt;a href="https://github.com/sb8244/exq-throughput" target="_blank"&gt;https://github.com/sb8244/exq-throughput&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;The Problem&lt;/h2&gt;

&lt;p&gt;Background job processing libraries write their jobs into a persistent storage mechanism and then retrieve those jobs
in the future. If you&amp;rsquo;ve used Ruby, you may be familiar with Sidekiq. The act of writing to Redis is very fast, but there
can be overhead at multiple levels. If the overhead is too high, then writing jobs to Redis becomes slow and the application
may become backed up. This can lead to errors or even a loss of service, if acknowledged persistence of a job is required.&lt;/p&gt;

&lt;h2&gt;Types of Overhead&lt;/h2&gt;

&lt;p&gt;The most common overhead that I&amp;rsquo;ve seen is the backup of Redis commands being executed end-to-end serially. This happens
when you use a single connection to write to Redis, and can occur in any language. The issue arises because a single connection can only
send one command at a time. It must then wait for the response before another command can occur. Redis is single-threaded, so it may not
be obvious why this is an issue. The problem is that the network overhead is done serially in this type of system—each write has to
go over the network and back before the next starts.&lt;/p&gt;

&lt;p&gt;The following diagram shows the speed of three hypothetical Redis requests:&lt;/p&gt;

&lt;div style="text-align: center"&gt;
  &lt;img src="/images/ExqPool/redis_serialization.svg"
       alt="Redis single connection versus pooled connection. Pooled connection completes 3 requests much faster."
       height="375px" /&gt;
  &lt;p&gt;
    &lt;small&gt;&lt;i&gt;Redis single connection versus pooled connection. Pooled connection completes 3 requests much faster.&lt;/i&gt;&lt;/small&gt;
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Each connection sends a command that goes over the network to Redis, which processes the command. A response is returned
and also goes over the network. In the real-world, this network latency might be 1ms or less. However, the end result is that
the requests complete much faster when multiple commands can be simultaneously sent via multiple connections.&lt;/p&gt;

&lt;p&gt;Another type of overhead is the fact that an Elixir process handles messages serially. If a job is enqueued via a single
process, the same problem as a single connection emerges.&lt;/p&gt;

&lt;h2&gt;The Problem in Exq&lt;/h2&gt;

&lt;p&gt;Exq enqueues jobs through the &lt;code&gt;Exq.Enqueuer&lt;/code&gt; process. This is a single process that holds a single redis connection. Each enqueue
task goes through this one process, serially. If serial processes and single connections lead to less throughput, then this is
will limit the throughput of Exq enqueueing.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s move into what we can do about it, and then benchmarks.&lt;/p&gt;

&lt;h2&gt;Pool Processes to Increase Throughput&lt;/h2&gt;

&lt;p&gt;The solution to the problem above is to pool processes, so that multiple Redis commands can be sent to Redis in the same moment
of time. There are two main ways that I&amp;rsquo;ve done this in Elixir: poolboy and named pools.&lt;/p&gt;

&lt;h4&gt;Poolboy&lt;/h4&gt;

&lt;p&gt;&lt;a href="https://github.com/devinus/poolboy" target="_blank"&gt;Poolboy&lt;/a&gt; is a nifty Erlang library that can create a pool of any process you want. We could
pool &lt;code&gt;Exq.Enqueuer&lt;/code&gt; processes and then enqueue jobs by using the poolboy functions. Let&amp;rsquo;s see how we&amp;rsquo;d do that:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;defmodule ExqThroughput.Application do
  use Application

  def start(_type, _args) do
    children =
      [
        :poolboy.child_spec(:worker, poolboy_config())
      ]

    opts = [strategy: :one_for_one, name: ExqThroughput.Supervisor]
    Supervisor.start_link(children, opts)
  end

  def enqueuer_pool_size(), do: :erlang.system_info(:schedulers_online)

  def poolboy_config() do
    [
      {:name, {:local, :enqueuer}},
      {:worker_module, ExqThroughput.PooledEnqueuer},
      {:size, enqueuer_pool_size()}
    ]
  end
end

defmodule ExqThroughput.PooledEnqueuer do
  def start_link(_) do
    # Hack to make Exq happy with running
    num = :rand.uniform(100_000_000) + 100
    name = :&amp;quot;Elixir.Exq#{num}&amp;quot;
    Exq.Enqueuer.start_link(name: name)

    # We need to put the enqueuer instance into the pool
    {:ok, Process.whereis(:&amp;quot;#{name}.Enqueuer&amp;quot;)}
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There is a bit of a hack in the &lt;code&gt;PooledEnqueuer&lt;/code&gt; module to make Exq happy. There may be another way to get around this, but I went
for a quick solution for the purpose of this benchmark. There is also a bit of working around the Exq process tree to get access
directly to the Enqueuer process.&lt;/p&gt;

&lt;p&gt;We can now enqueue a job by first checking out the poolboy process:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;:poolboy.transaction(:enqueuer, fn pid -&amp;gt;
  Exq.enqueue(pid, &amp;quot;throughput_queue&amp;quot;, Worker, [])
end)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Named process pooling looks a bit different than this.&lt;/p&gt;

&lt;h4&gt;Named Processes&lt;/h4&gt;

&lt;p&gt;You can start multiple processes in Elixir and give them a name like &lt;code&gt;MyProcess1&lt;/code&gt;, &lt;code&gt;MyProcess2&lt;/code&gt;, etc. When you want to send a
message to the process, you would send a message to &lt;code&gt;:&amp;quot;Elixir.MyProcess#{:rand.uniform(2)}&amp;quot;&lt;/code&gt;. This is named process pooling, and is
conceptually very simple—this makes it easier to setup.&lt;/p&gt;

&lt;p&gt;We have to start the pool of processes in the application:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;defmodule ExqThroughput.Application do
  use Application

  def start(_type, _args) do
    children = named_enqueuer_pool(enqueuer_pool_size())
    opts = [strategy: :one_for_one, name: ExqThroughput.Supervisor]
    Supervisor.start_link(children, opts)
  end

  def enqueuer_pool_size(), do: :erlang.system_info(:schedulers_online)

  defp named_enqueuer_pool(count) do
    for i &amp;lt;- 1..count do
      name = :&amp;quot;Elixir.Exq#{i}&amp;quot;

      %{
        id: name,
        start: {Exq.Enqueuer, :start_link, [[name: name]]}
      }
    end
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can then enqueue work by directly using these processes:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;def named_enqueue() do
  num = :rand.uniform(ExqThroughput.Application.enqueuer_pool_size())
  Exq.enqueue(:&amp;quot;Elixir.Exq#{num}.Enqueuer&amp;quot;, &amp;quot;throughput_queue&amp;quot;, Worker, [])
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I love this approach due to its simplicity. Let&amp;rsquo;s see how all of the approaches stack up.&lt;/p&gt;

&lt;h2&gt;Benchmark&lt;/h2&gt;

&lt;p&gt;Benchee is used to benchmark three scenarios: single process, poolboy, named processes. Benchee is ran with various
parallelism amounts to simulate how you might run Exq in production. For example, if you are enqueueing from a web tier,
then your parallelism will be quite high. If you&amp;rsquo;re enqueueing from a single process, you would have no parallelism.&lt;/p&gt;

&lt;p&gt;The redis queues are cleaned up before/after each test. The Exq work processor is not running—this test is purely around
speed of enqueueing. These tests are all running locally, and Redis is not running through any type of virtualization. The performance
would be significantly different depending on how redis is setup and the network speed between your application and redis.&lt;/p&gt;

&lt;p&gt;When Benchee was run with a single runner, all of the approaches came out roughly the same. This is expected because we
won&amp;rsquo;t see parallelism benefits without multiple processes trying to enqueue.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Name                       ips        average  deviation         median         99th %
named enqueuer          9.05 K      110.52 μs    ±42.61%          99 μs         210 μs
poolboy enqueuer        8.73 K      114.51 μs    ±57.05%         102 μs         240 μs
default enqueuer        8.30 K      120.54 μs    ±51.87%         110 μs         249 μs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The difference with 6 parallel testers was quite different. We can see that the pool approaches have significantly higher
throughput:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;total ips is these numbers * 6
Name                       ips        average  deviation         median         99th %
poolboy enqueuer        4.40 K      227.14 μs    ±39.15%         216 μs         417 μs
named enqueuer          3.95 K      253.41 μs    ±45.96%         227 μs         605 μs
default enqueuer        1.05 K      954.02 μs    ±21.91%         951 μs     1446.13 μs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now for 12:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;total ips is these numbers * 12
Name                       ips        average  deviation         median         99th %
poolboy enqueuer        2.83 K      352.86 μs    ±26.97%         339 μs         655 μs
named enqueuer          2.78 K      359.24 μs    ±53.25%         302 μs        1004 μs
default enqueuer        0.84 K     1187.04 μs    ±21.96%        1121 μs     1882.19 μs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and 24:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;total ips is these numbers * 24
Name                       ips        average  deviation         median         99th %
named enqueuer          1.48 K      675.58 μs    ±66.26%      541.98 μs     2198.98 μs
poolboy enqueuer        1.06 K      942.92 μs    ±51.20%      845.98 μs     2470.98 μs
default enqueuer        0.34 K     2900.89 μs    ±19.05%     2765.98 μs     4482.25 μs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That one surprised me because the named enqueuer was significantly more performant. I tried it over 10 times and
consistently got the same results. The tests were run in different order each time.&lt;/p&gt;

&lt;p&gt;That disappeared for 48:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;total ips is these numbers * 48
Name                       ips        average  deviation         median         99th %
poolboy enqueuer        912.30        1.10 ms    ±30.56%        1.01 ms        2.35 ms
named enqueuer          896.40        1.12 ms    ±77.47%        0.86 ms        4.06 ms
default enqueuer        203.05        4.92 ms    ±18.66%        4.65 ms        8.84 ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Interpreting the Results&lt;/h2&gt;

&lt;p&gt;These results show, clearly, that pooling the &lt;code&gt;Exq.Enqueuer&lt;/code&gt; process significantly increases throughput. This might be
even more pronounced when Redis is accessed over the network.&lt;/p&gt;

&lt;p&gt;Each test increased the parallelism, and the gap between pooled and single got even larger. With 48 processes enqueueing jobs, the total
throughput per second is ~43,000 versus ~9,600. With 12 processes enqueueing jobs, the throughput per second is still ~33,000 versus
~10,000.&lt;/p&gt;

&lt;h2&gt;Action the Results&lt;/h2&gt;

&lt;p&gt;If you are using Exq in production, consider pooling the enqueuer processes to increase throughput capacity. You may also increase
your enqueue speeds even if you&amp;rsquo;re not at capacity. You can use any pooling approach you want, they are roughly the same and
have a substantial impact to throughput.&lt;/p&gt;

&lt;p&gt;Exq already has an open issue to discuss adding some type of parallelism to the enqueuer process. Thanks to my colleague Marco for
opening that issue and for letting me look at this problem with him.&lt;/p&gt;

&lt;h2&gt;The Book Plug&lt;/h2&gt;

&lt;p&gt;My book &amp;ldquo;Real-Time Phoenix: Build Highly Scalable Systems with Channels&amp;rdquo; is now in beta
through &lt;a href="http://bit.ly/rtp-exq" target="_blank"&gt;The Pragmatic Bookshelf&lt;/a&gt;. This book explores using Phoenix Channels, GenStage, and more to build
real-time applications in Elixir.&lt;/p&gt;

&lt;div style="text-align: center"&gt;
  &lt;a href="http://bit.ly/rtp-exq" target="_blank"&gt;
    &lt;img src="/images/sbsockets.jpg" alt="Real-Time Phoenix by The Pragmatic Bookshelf" height="300px" style="border: 1px solid #ccc" /&gt;
  &lt;/a&gt;
&lt;/div&gt;
</content>
  </entry>
  <entry>
    <title>Lessons Learned from Shipping PushEx</title>
    <link rel="alternate" href="https://stephenbussey.com/2019/08/30/lessons-learned-from-shipping-pushex.html"/>
    <id>https://stephenbussey.com/2019/08/30/lessons-learned-from-shipping-pushex.html</id>
    <published>2019-08-30T19:55:00-04:00</published>
    <updated>2019-08-30T21:46:28-04:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;It&amp;rsquo;s been a while since I announced the initial open-source release of PushEx. We had a few small challenges to solve
in order to roll it out, and then I let it bake for several months to ensure that it is stable in production. It has
been running for several months now at production scale, and I cut the first official release of it
on &lt;a href="https://hex.pm/packages/push_ex" target="_blank"&gt;hex.pm&lt;/a&gt; today. These are some of the challenges and lessons learned from running
it in production.&lt;/p&gt;

&lt;h2&gt;The project was a big success&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s start with the good news.
Our goal with the project was to replace our existing push provider, in order to have more control over an important
part of our tech stack. By &amp;ldquo;push provider&amp;rdquo;, I mean a service that sends data from servers to clients in real-time.
We needed to replace multiple JavaScript clients and we needed to ensure that the
pushes were getting queued up from our servers properly.&lt;/p&gt;

&lt;p&gt;This process went very well, due to careful planning and testing on our end. We ran both systems in parallel
for some time, and slowly ramped the number of pushes from 0 to 100%. Throughout this process, we pushed data to clients
but did not use it there. We became confident enough to consume the data on the clients through a slow rollout process.
At this point, the connections to our old provider dropped.&lt;/p&gt;

&lt;p&gt;We were able to avoid downtime or problems throughout this process by very carefully monitoring the application. We
stopped the rollout at the first sign of trouble, and then resumed once we understood the root cause.&lt;/p&gt;

&lt;h2&gt;The order of process shutdown matters&lt;/h2&gt;

&lt;p&gt;We encountered a large number of errors during the deployment process of the application. This never occurred during
normal operation, so it seemed a bit odd. The root cause of the errors was that the application would still receive
requests as it was shutting down. This caused messages to try to push to clients, but the processes involved in that
were already shut down. This can happen with a Supervisor structure like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[
  PushExWeb.Endpoint,
  PushEx.Pipeline
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the pipeline shuts down while the Endpoint is still online, web requests will encounter errors. A better layout
might look like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[
  PushEx.Pipeline,
  PushExWeb.Endpoint,
  PushExWeb.ConnectionDrainer
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to gain control over the shutdown process of PushEx, I had to make changes to the application supervision tree. I separated
the &lt;a href="https://github.com/pushex-project/pushex/blob/675c986acac5a06486d7b47e7b34e8289fda2d3b/lib/push_ex/application.ex#L10" target="_blank"&gt;starting of PushEx&amp;rsquo;s core system&lt;/a&gt;
from the &lt;a href="https://github.com/pushex-project/pushex/blob/675c986acac5a06486d7b47e7b34e8289fda2d3b/lib/push_ex/supervisor.ex#L11" target="_blank"&gt;web portion&lt;/a&gt;.
These changes allow the entire web stack to be gracefully go offline (complete with connection draining) before the data
pipeline goes offline.&lt;/p&gt;

&lt;p&gt;This leads into the next problem&amp;mdash;how to prevent data loss during application shutdown.&lt;/p&gt;

&lt;h2&gt;Connection draining is critical&lt;/h2&gt;

&lt;p&gt;Connection draining allows a web server to gracefully wait for connections to close before it proceeds with shutting down more of the application.
HTTP connection draining shuts down the listener process that would accept new connections.&lt;/p&gt;

&lt;p&gt;Several layers of draining are required for PushEx to gracefully exit. They are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/pushex-project/pushex/blob/675c986acac5a06486d7b47e7b34e8289fda2d3b/lib/push_ex_web/channels/socket_drainer.ex" target="_blank"&gt;Socket draining&lt;/a&gt; (sockets should gracefully disconnect)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pushex-project/pushex/blob/675c986acac5a06486d7b47e7b34e8289fda2d3b/lib/push_ex/supervisor.ex#L23" target="_blank"&gt;Ranch connection draining&lt;/a&gt; (new web requests shouldn&amp;rsquo;t be handled)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pushex-project/pushex/blob/675c986acac5a06486d7b47e7b34e8289fda2d3b/lib/push_ex/push/drainer.ex" target="_blank"&gt;Push Pipeline draining&lt;/a&gt; (data in
flight should be given a chance of delivery)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You&amp;rsquo;ll notice that Socket draining is implemented separately from the connection draining. This is because the connection draining API stops
the listener from accepting new connections only. If a connection is already established, it has to be manually killed or shut down.&lt;/p&gt;

&lt;p&gt;This process was complex to get right, but now we can reboot servers without errors and without losing data. All of our data
drains out within the 30 second limit, so your mileage would vary if that&amp;rsquo;s not the case.&lt;/p&gt;

&lt;h2&gt;Big topics affect performance&lt;/h2&gt;

&lt;p&gt;We have a few large push topics in our application. It can be costly for Phoenix Tracker to have a large number of joins in a very short period of time.
The solution for PushEx is to allow certain topics to not use Tracker. When an ignored topic
is &lt;a href="https://github.com/pushex-project/pushex/blob/62efb769e6bb3472f4b4635e9e98a5f6f1c346f7/lib/push_ex_web/channels/push_tracker.ex#L32" target="_blank"&gt;tracked&lt;/a&gt;
or &lt;a href="https://github.com/pushex-project/pushex/blob/62efb769e6bb3472f4b4635e9e98a5f6f1c346f7/lib/push_ex_web/channels/push_tracker.ex#L43" target="_blank"&gt;questioned&lt;/a&gt;,
it always responds with &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The likelihood that a large topic has 0 connected clients is close to 0, so it is acceptable to treat it like it always has clients. We don&amp;rsquo;t
want to treat small topics as if they always have connected clients, or we&amp;rsquo;d do more work than necessary.&lt;/p&gt;

&lt;h2&gt;Keep-Alive to dramatically increase API throughput&lt;/h2&gt;

&lt;p&gt;I was a bit shocked when I implemented the server API calls in our Ruby app. We were used to 20ms calls to our old provider, but the new
one was hitting 200ms! We traced the root cause to DNS slowness when establishing the connection. The solution was to utilize Keep-Alive
connection pooling.&lt;/p&gt;

&lt;p&gt;The Keep-Alive header makes it so that the connection is not closed after a response is sent. The connection is open to accept more requests
and can do so without the overhead of establishing a connection. This dramatically increased the throughput of our Ruby servers to the
PushEx API endpoint. We didn&amp;rsquo;t quite hit the 20ms goal, but it was close enough to call it a win.&lt;/p&gt;

&lt;p&gt;I did hit a snag here. The connection draining for Keep-Alive connections suffers from the same problem as WebSockets do&amp;mdash;they
don&amp;rsquo;t close when the listener is closed. I had to do a bit of hackery to set a global value indicating that the Keep-Alive connection should
be &lt;a href="https://github.com/pushex-project/pushex/blob/675c986acac5a06486d7b47e7b34e8289fda2d3b/lib/push_ex_web/controllers/push_controller.ex#L32" target="_blank"&gt;closed on the next request&lt;/a&gt;.
This is a hack that I wish I didn&amp;rsquo;t have to do, but it did have the desired impact.&lt;/p&gt;

&lt;h2&gt;Wrapping Up&lt;/h2&gt;

&lt;p&gt;There were lots of challenges in rolling out the first major PushEx release, but the end result is solid. We&amp;rsquo;re running at pretty high throughput
on a low number of small servers (2GB + 2VCPU for this app) without issue.&lt;/p&gt;

&lt;p&gt;The snags we hit are not unique to our application. When building an Elixir application, you should consider both the startup and shutdown order
of your process tree. Use connection draining to avoid new connections being made to a server that is in the process of shutting down. Leverage
Keep-Alive headers for server-to-server API requests, especially if the throughput is high. These things do come with tradeoffs, however, so
your mileage may vary.&lt;/p&gt;

&lt;h2&gt;The Book Plug&lt;/h2&gt;

&lt;p&gt;My book &amp;ldquo;Real-Time Phoenix: Build Highly Scalable Systems with Channels&amp;rdquo; is now in beta
through &lt;a href="https://pragprog.com/book/sbsockets/real-time-phoenix" target="_blank"&gt;The Pragmatic Bookshelf&lt;/a&gt;. This book captures the victories and struggles that
we face when building real-time applications in Elixir.&lt;/p&gt;

&lt;div style="text-align: center"&gt;
  &lt;a href="https://pragprog.com/book/sbsockets/real-time-phoenix" target="_blank"&gt;
    &lt;img src="/images/sbsockets.jpg" alt="Real-Time Phoenix by The Pragmatic Bookshelf" height="300px" style="border: 1px solid #ccc" /&gt;
  &lt;/a&gt;
&lt;/div&gt;
</content>
  </entry>
  <entry>
    <title>Elixir Makes Testing Hard Things Easy</title>
    <link rel="alternate" href="https://stephenbussey.com/2019/06/08/elixir-makes-testing-hard-things-easy.html"/>
    <id>https://stephenbussey.com/2019/06/08/elixir-makes-testing-hard-things-easy.html</id>
    <published>2019-06-08T01:59:00-04:00</published>
    <updated>2019-07-02T13:07:46-04:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;I love Elixir. If we&amp;rsquo;ve talked at a conference or you work with me, you know this. I constantly find small nuggets
when working with Elixir that simply bring joy to me. One of these nuggets recently emerged that I thought worth
sharing.&lt;/p&gt;

&lt;h1&gt;Backstory&lt;/h1&gt;

&lt;p&gt;I&amp;rsquo;m working on some learning material right now for real-time system development. One of the key aspects of a section
is about how important measurement is. Without measurement, how do we have any confidence that our running system
is healthy?&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m using StatsD for this section because it&amp;rsquo;s fairly well-adopted and is conceptually simple to understand. However,
I kept bumping into small inefficiencies in my flow. An example of this is that I use a Node program called &lt;code&gt;statsd-logger&lt;/code&gt;
to view my metrics in development. It felt a bit strange to have a reader install this Node program to view logs in our
Elixir app.&lt;/p&gt;

&lt;p&gt;The same problem came up today when I was writing tests for the StatsD metrics. I would typically use Mockery to test that
Statix, my client, is correctly called. If I went down this path, I&amp;rsquo;d be putting a lot on the reader in terms of understanding
what the heck a mock is and when to use it. Ideally, we&amp;rsquo;ll have 1 line of code max additional setup.&lt;/p&gt;

&lt;p&gt;Okay, enough of the backstory. I wanted to fix these two very specific problems. Let&amp;rsquo;s get into the more interesting bits.&lt;/p&gt;

&lt;h1&gt;Elixir is Many Programs&lt;/h1&gt;

&lt;p&gt;Elixir runs processes in a way that makes them feel like little independent programs. We can spin up tens of thousands of these
programs without any issue at all. We can even spin up programs that we may consider expensive, such as a webserver.&lt;/p&gt;

&lt;p&gt;The real jewel of these many programs is that they are given a simple and efficient way to communicate with each other, message
passing. To take the web server example to the extreme, we could spin up 5 different servers on 5 ports and interface with them
through our browser. Behind the scenes, these &amp;ldquo;servers&amp;rdquo; would be in the same Elixir VM and could be passing messages back and forth
nearly freely.&lt;/p&gt;

&lt;p&gt;This is conceptually interesting, to me, because it means that traditionally complex or standalone applications can be run in the
same program (very easily) and can share information.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s see how this applies to StatsD.&lt;/p&gt;

&lt;h1&gt;Just run a Server&lt;/h1&gt;

&lt;p&gt;What if we just ran a StatsD server in our Elixir application? It could listen on a port and accept messages. We could do whatever
we wanted with these messages, like log them out or store them. Let&amp;rsquo;s do that in a few lines of code:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;$ iex
iex(1)&amp;gt; :gen_udp.open(8126, active: true)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;ve started a &amp;ldquo;UDP server&amp;rdquo; in our terminal. Let&amp;rsquo;s send it some messages (open new terminal):&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;# Your mileage may vary, fingers crossed
$ echo -n &amp;quot;hello:1&amp;quot; | nc -4u -w0 localhost 8126
$ echo -n &amp;quot;hello:2&amp;quot; | nc -4u -w0 localhost 8126
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s go back to the original iex terminal:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;iex(2)&amp;gt; flush()
{:udp, #Port&amp;lt;0.5&amp;gt;, {127, 0, 0, 1}, 61814, &amp;#39;hello:1&amp;#39;}
{:udp, #Port&amp;lt;0.5&amp;gt;, {127, 0, 0, 1}, 61515, &amp;#39;hello:2&amp;#39;}
:ok
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Woah! We received the UDP packets as messages to our process. That&amp;rsquo;s super neat to me because of how simple it was (due to
the great backing work of Erlang/OTP.)&lt;/p&gt;

&lt;p&gt;So what if this was the solution to problem 1 (statsd-logger)? We could start a UDP server and log out messages. I threw together
a quick library for that purpose called &lt;a href="https://github.com/sb8244/statsd_logger" target="_blank"&gt;StatsDLogger&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;That worked out nicely. What about testing, though? Let&amp;rsquo;s take what we just wrote and apply it to tests.&lt;/p&gt;

&lt;h1&gt;Test Anything Easily&lt;/h1&gt;

&lt;p&gt;Testing can sometimes be hard. Integration testing across multiple servers is usually hard. What level do we start to mock at? We can&amp;rsquo;t
actually run all of our services all of the time without having very slow tests. To help solve this, we can take what we just did and apply it to
tests. There is a key question though, how do we actually write an assertion against our server?&lt;/p&gt;

&lt;p&gt;In Elixir, message passing is champion. If you have a process&amp;rsquo;s ID, you can send it a message and it will go into that process&amp;rsquo;s mailbox.
With ExUnit, this mailbox can be asserted against by using &lt;code&gt;assert_receive&lt;/code&gt;. This means that if we can send our test process
a message, we can write a test for it.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s slightly harder to boil this concept down into an example as simple as the last. Let&amp;rsquo;s look at some code I wrote to
handle this for StatsD:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;  def handle(:valid, name, value, opts) do
    pid = Keyword.fetch!(opts, :send_to)
    send(pid, {:statsd_recv, name, value})
  end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can handle a metric by sending a message to a pid. Now the question is, &amp;ldquo;which pid?&amp;rdquo; The process that calls &lt;code&gt;start_link&lt;/code&gt;
can be referenced with &lt;code&gt;self()&lt;/code&gt;. This means we can do something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;  def start_link(opts) do
    opts = Keyword.merge([send_to: self()], opts)
    GenServer.start_link(__MODULE__, opts)
  end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can put all of this together to write a test like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;  test &amp;quot;valid / invalid messages are handled&amp;quot; do
    StatsDLogger.start_link(port: @port, formatter: :send)

    send_event(&amp;quot;a:1&amp;quot;)
    send_event(&amp;quot;a:2|c&amp;quot;)
    send_event(&amp;quot;invalid&amp;quot;)

    assert_receive {:statsd_recv, &amp;quot;a&amp;quot;, &amp;quot;1&amp;quot;}
    assert_receive {:statsd_recv, &amp;quot;a&amp;quot;, &amp;quot;2|c&amp;quot;}
    assert_receive {:statsd_recv_invalid, &amp;quot;invalid&amp;quot;}
  end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Elixir has made testing this multi-server integration a piece of cake.&lt;/p&gt;

&lt;h1&gt;In the Wild&lt;/h1&gt;

&lt;p&gt;I&amp;rsquo;m definitely not finding any new technique here. The reason I love Elixir is that gems like this are all over open-source
libraries. The one that comes to mind here is &lt;a href="https://github.com/PSPDFKit-labs/bypass" target="_blank"&gt;Bypass&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Bypass works by starting an anonymous HTTP server that runs a function when it&amp;rsquo;s invoked. That is a great example of &amp;ldquo;start a
server&amp;rdquo; to solve how to test that an application makes the right request.&lt;/p&gt;

&lt;p&gt;Phoenix Channels are a great example of &amp;ldquo;send a message&amp;rdquo; in the wild. When you write &lt;code&gt;assert_push&lt;/code&gt;, you&amp;rsquo;re actually just
&lt;a href="https://github.com/phoenixframework/phoenix/blob/master/lib/phoenix/test/channel_test.ex#L481" target="_blank"&gt;checking for a specific message&lt;/a&gt;.
The Channel test is wired up so that the test process becomes the &amp;ldquo;socket transport.&amp;rdquo; This allows it to be at the edge of the system
and capture a lot of test for very little code.&lt;/p&gt;

&lt;h1&gt;Wrapping Up&lt;/h1&gt;

&lt;p&gt;If you&amp;rsquo;re here, thanks for making it through. I love Elixir because of small things like this that would actually be huge in some
other languages I&amp;rsquo;ve worked with. If you&amp;rsquo;re looking for a way to test complex code, try out message passing as a potential solution.&lt;/p&gt;

&lt;h1&gt;Shameless Plug for ElixirConf&lt;/h1&gt;

&lt;p&gt;My colleague Grant and I are going to give a great training session at ElixirConf Tuesday course. You
can find more details at &lt;a href="https://elixirconf.com/2019/training-classes/2" target="_blank"&gt;https://elixirconf.com/2019/training-classes/2&lt;/a&gt;. We&amp;rsquo;ll
be focused on writing real-time systems, and will specifically be leveraging a lot of real world lessons from doing this at
decent scale on several pieces of our SaaS app. Come check it out!&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Useful Elixir Patterns from Real-world Side Project</title>
    <link rel="alternate" href="https://stephenbussey.com/2019/05/10/useful-elixir-patterns-from-real-world-side-project.html"/>
    <id>https://stephenbussey.com/2019/05/10/useful-elixir-patterns-from-real-world-side-project.html</id>
    <published>2019-05-10T13:00:00-04:00</published>
    <updated>2019-07-02T13:07:45-04:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;I believe that one of the best ways to push new practices is to work on a real-world project that we
can afford to experiment on. We can push the boundary in the toy project while still seeing the results
of the decisions in a production environment with real users. I&amp;rsquo;ve been fortunate to be able to ship
several applications like this at SalesLoft. The latest one that we&amp;rsquo;ll look at today is our internal OKR (Objective-Key Result) app that we use for goal tracking and alignment.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll walk through this project and see several different patterns that I like. Some of these patterns
have been used in several different projects of mine and have held up well. Others were new experiments
that I hope to use again in the future.&lt;/p&gt;

&lt;p&gt;The source code for the application can be found on &lt;a href="https://github.com/sb8244/okr_app_pub" target="_blank"&gt;Github&lt;/a&gt;. The useful
part is the code and tests, less so the specific functionality.&lt;/p&gt;

&lt;h1&gt;Context Usage&lt;/h1&gt;

&lt;p&gt;Contexts, at least as I use them, are all about establishing an application domain and keeping
the interfaces small and useful for other parts of the system to use. I&amp;rsquo;ve done this in the past but
really pushed it for this project by having no function usage other than top level modules. This
pattern allows us to have a well-defined interface for the application, which makes it simpler for both
us and others to modify the code and understand the consequences.&lt;/p&gt;

&lt;p&gt;As an example, the following would not be acceptable:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;MyApp.SomeContext.ASpecificQuery.execute(params)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Instead, I would prefer this:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;MyApp.SomeContext.a_query(params)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All of my contexts live under the &lt;code&gt;OkrApp&lt;/code&gt; module. You can see a list of them in the &lt;a href="https://github.com/sb8244/okr_app_pub/tree/master/lib/okr_app" target="_blank"&gt;okr_app folder&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I started off the application by writing code directly in the context. Some of this is still seen because
I didn&amp;rsquo;t go back through and change everything once I solidified on what I wanted. In the end, I found
that &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/analytics.ex#L5" target="_blank"&gt;function delegates&lt;/a&gt;
allowed the context to be very easy to digest and I could write tests for the underlying modules in distinct
files.&lt;/p&gt;

&lt;p&gt;The hardest part of this pattern, that I haven&amp;rsquo;t figured out yet, is how to handle Ecto schemas between
contexts. Sometimes, a context needs to leak out. For example, a &lt;code&gt;User&lt;/code&gt; schema may need to be referenced
by the &lt;code&gt;AnalyticsEvent&lt;/code&gt; schema (as seen &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/analytics/analytics_event.ex#L11" target="_blank"&gt;here&lt;/a&gt;).
I found this acceptable because I wasn&amp;rsquo;t invoking logic outside of the context interface.&lt;/p&gt;

&lt;p&gt;The advantage of Context came out when I built the &lt;a href="https://github.com/sb8244/okr_app_pub/tree/master/lib/okr_app/mailer" target="_blank"&gt;mailer&lt;/a&gt;.
I found it very simple to add in this new code without worrying about anything breaking. I was also to extract concepts
specific to the mailer (such as &lt;code&gt;Recipient&lt;/code&gt;) rather than relying on generic modules prone to change such as &lt;code&gt;User&lt;/code&gt;.&lt;/p&gt;

&lt;h1&gt;Isolated Logic (Context) for SCIM&lt;/h1&gt;

&lt;p&gt;One of the requirements for this project was SCIM (System for Cross-domain Identity Management). We use Okta internally and
they have a lot of docs on how to write a SCIM integration. I didn&amp;rsquo;t want the details of SCIM leaking into the system too
heavily. I was able to leverage a SCIM Context to achieve this goal in a way that I am pretty satisfied with.&lt;/p&gt;

&lt;p&gt;SCIM has all of its web functionality extracted into a module called &lt;code&gt;Scim.Web&lt;/code&gt;. &lt;code&gt;Phoenix.Router&lt;/code&gt; is used to provide the
appropriate endpoint definitions, but the integration happens in a simple &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/scim/web/plug.ex#L13" target="_blank"&gt;Plug&lt;/a&gt;
which is used by &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app_web/router.ex#L43" target="_blank"&gt;&lt;code&gt;OkrAppWeb.Router&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The entire &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/scim/web/users_controller.ex#L9" target="_blank"&gt;SCIM controller&lt;/a&gt; leverages
a behavior passed into it. This is the &lt;a href="https://en.wikipedia.org/wiki/Strategy_pattern" target="_blank"&gt;strategy pattern&lt;/a&gt; at work.
I didn&amp;rsquo;t create a specific &lt;code&gt;Behaviour&lt;/code&gt; requirement for this module but probably should have, because it has an expected
interface.&lt;/p&gt;

&lt;p&gt;The actual SCIM application integration happens in &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/users_scim.ex" target="_blank"&gt;UsersScim&lt;/a&gt;.
This code is a bit ugly since I didn&amp;rsquo;t clean it up too much, but it&amp;rsquo;s nice that the implementation doesn&amp;rsquo;t leak into my
application API nor does it leak into other contexts.&lt;/p&gt;

&lt;h1&gt;ListQuery Module&lt;/h1&gt;

&lt;p&gt;I really like Ecto&amp;rsquo;s API for querying data, but I have found it cumbersome to build up dynamic queries from API params easily.
I wrote a &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/query/list_query.ex" target="_blank"&gt;&lt;code&gt;ListQuery&lt;/code&gt;&lt;/a&gt; some time ago that I brought into this project.&lt;/p&gt;

&lt;p&gt;The ListQuery is used to take provided &lt;code&gt;params&lt;/code&gt; and &lt;code&gt;opts&lt;/code&gt; and turns it into an Ecto compatible query. Sometimes advanced
queries are required and I devised a small way to do that by stripping certain &lt;code&gt;params&lt;/code&gt; and then reapplying them. You can
see that &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/objectives/store/cycle_store.ex#L11" target="_blank"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m able to leverage a &lt;code&gt;ListQuery&lt;/code&gt; powered function in &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app_web/controllers/api/objective_link_controller.ex#L11" target="_blank"&gt;various controllers&lt;/a&gt;
by passing in the user params. If you use this pattern in an environment with non-friendly users, you should probably sanitize
the input to not leak any information to the client.&lt;/p&gt;

&lt;h1&gt;SimpleEctoStore Module&lt;/h1&gt;

&lt;p&gt;My contexts use &lt;code&gt;defdelegate&lt;/code&gt; to send queries to a different module. I called the module a &amp;ldquo;store&amp;rdquo; and found myself
&lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/objectives.ex#L13" target="_blank"&gt;delegating to them often&lt;/a&gt;.
Writing the same code again and again in the store became a bit cumbersome. Often, I would be writing the most
simple code possible with just the module name changed.&lt;/p&gt;

&lt;p&gt;My solution to this copy/paste problem was to build a macro powered &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/store/simple_ecto_store.ex" target="_blank"&gt;&lt;code&gt;SimpleEctoStore&lt;/code&gt;&lt;/a&gt;.
Each usage of &lt;code&gt;SimpleEctoStore&lt;/code&gt; involved passing in the schema module as well as what methods were desired to be
pulled in. You can see this &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/objectives/store/objective_link_store.ex#L2" target="_blank"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This allowed me to remove the boiler plate for a large number of ecto stores. If I used a concept like stores again,
I would definitely repeat this one.&lt;/p&gt;

&lt;h1&gt;Things that didn&amp;rsquo;t work as well&lt;/h1&gt;

&lt;p&gt;One of the biggest pain points I had was defining how I wanted my JSON API to work. At work, I follow the philosophy of very focused
endpoints that don&amp;rsquo;t preload many models together. This is for performance and flexibility reasons. Doing that in this project
made it much more complex for me and was going to take time that I didn&amp;rsquo;t want to spend. The end result here is that I
have a massive endpoint called &amp;ldquo;Okr&amp;rdquo; that &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app_web/serializers/okr.ex" target="_blank"&gt;embeds many associations&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The preload for my Okr endpoint is &lt;a href="https://github.com/sb8244/okr_app_pub/blob/master/lib/okr_app/objectives/okr_preloader.ex" target="_blank"&gt;pretty gnarly&lt;/a&gt; as
well. However, I was thrilled to be able to pass queries into the preload clauses which means that I removed the risk of
accidentally leaking data that is queried out of the system but then not handled properly by me in Elixir.&lt;/p&gt;

&lt;p&gt;Another thing that I still feel awkward about is a context&amp;rsquo;s schema referencing a different context&amp;rsquo;s schema. I wanted to
remove this but couldn&amp;rsquo;t figure out how to do so in a way I liked.&lt;/p&gt;

&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;Contexts allowed for a focused API between parts of an application. Using them saved me some mental gymnastics as I was
developing the application. I was particularly happy with how I was able to isolate the sort of whacky SCIM API via
focused context modules.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ListQuery&lt;/code&gt; and &lt;code&gt;SimpleEctoStore&lt;/code&gt; both made my life easier while defining my ecto based queries and stores. These are generically
applicable modules that could be brought into other applications if needed.&lt;/p&gt;

&lt;p&gt;I am really happy overall with how this project turned out. It is easy to read through nearly 6 months after I&amp;rsquo;ve originally
built it and has been running without any Elixir issues since that time. I&amp;rsquo;ll be applying these patterns to future projects,
for sure, and hope you are able to get some value from it!&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Thanks for reading! I&amp;rsquo;ll be teaching a class at ElixirConf US about building scalable real-time systems in Elixir. I&amp;rsquo;ve been
focused a lot on this topic so I&amp;rsquo;m pretty thrilled to share what I&amp;rsquo;ve learned. I believe that registration will open
soon.&lt;/p&gt;
</content>
  </entry>
</feed>

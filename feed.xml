<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Stephen Bussey's Software Engineering Blog</title>
  <id>https://stephenbussey.com</id>
  <link href="https://stephenbussey.com"/>
  <link href="https://stephenbussey.com/feed.xml" rel="self"/>
  <updated>2019-01-29T17:04:00-05:00</updated>
  <author>
    <name>Stephen Bussey</name>
  </author>
  <entry>
    <title>Distributed In-Memory Caching in Elixir</title>
    <link rel="alternate" href="https://stephenbussey.com/2019/01/29/distributed-in-memory-caching-in-elixir.html"/>
    <id>https://stephenbussey.com/2019/01/29/distributed-in-memory-caching-in-elixir.html</id>
    <published>2019-01-29T17:04:00-05:00</published>
    <updated>2019-01-29T20:31:33-05:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;I&amp;rsquo;ve been working with implementing our PushEx server at SalesLoft (a bit later than I thought I&amp;rsquo;d have time to) and one
of the challenges has been to map user identities in our authentication token to a legacy identity that we use in our
push channels. This is as simple as an HTTP call, but it is also something that could potentially burst very hard and cause
a large number of downstream HTTP calls. In order to help alleviate this, the identity -&amp;gt; secondary identity will be cached.&lt;/p&gt;

&lt;p&gt;I would typically just throw a cache in redis or memcached, but I really wanted to reach for something simpler in this situation.
My goals for this project are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;No external databases introduced (there are none currently on our PushEx implementation)&lt;/li&gt;
&lt;li&gt;Caching can be persistent between deploys (we roll our deploys so some pods are always online)&lt;/li&gt;
&lt;li&gt;Caching is conceptually simple in case it needs debugged by another engineer in the future&lt;/li&gt;
&lt;li&gt;Identity mappings will be stable between services, so time variations are not required. However:&lt;/li&gt;
&lt;li&gt;Caching will use a TTL to prevent really stale values from being used&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I&amp;rsquo;m going to walk through the different options I looked at and what I ended up on. The options I passed on aren&amp;rsquo;t necessarily bad,
they just ended up being operationally or conceptually more difficult than I needed.&lt;/p&gt;

&lt;h1&gt;Swarm&lt;/h1&gt;

&lt;p&gt;There was a great talk in the Atlanta Elixir Meetup recently about using Swarm to manage processes across a cluster. I really liked
a few things that the presentation demonstrated:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Processes are stable such that restarting a node with the same identity will put the process back on that node&lt;/li&gt;
&lt;li&gt;Processes are capable of handing themselves off to another node&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My initial plan was to put Cachex or another store in sharded processes that are distributed across the cluster. However, I soon
realized that passing off this ets state may be fairly complex and so I ended up looking at the cache as a simple in-process memory
map. That is actually fine with me, but some other things didn&amp;rsquo;t work out for it.&lt;/p&gt;

&lt;p&gt;The biggest issue that I ran into was that process hand-off became fairly complex to implement. I never actually got that working
properly after ~4 hours of work on it. Another bigger issue is that there is no process state replication. This means that my shard
values would have to pass themselves off fully to another node between the time that the node is told to shut down and the near future.
If the node was forcibly killed before then, the data would be lost and the cache would be re-populated.&lt;/p&gt;

&lt;p&gt;I ended up moving on from this to trying out the next solution because it seemed like I was getting myself into a solution I didn&amp;rsquo;t need.
That will be a theme in this blog post: there is nothing particularly wrong with technology X, but the trade-offs it brings may be more
than worth it for the particular use case I&amp;rsquo;m working with.&lt;/p&gt;

&lt;h1&gt;Delta CRDT&lt;/h1&gt;

&lt;p&gt;After talking with my colleague Dan about this topic (he will be presenting on distributed state at Lonestar Elixir Conf and it will be awesome),
he suggested looking at DeltaCRDT as a potential solution. I really liked this library for a few reasons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;State is replicated across the cluster, so shutdowns are not frantic&lt;/li&gt;
&lt;li&gt;CRDT gives a lot of benefit around time variation in the cluster (node A has a different value than node B at a point in time)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We ended up getting a working solution that used the DeltaCRDT library. Our code looked something like this (don&amp;rsquo;t use this code):&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;defmodule SalesloftPusher.AccountLookupCache.Monitor do
  use GenServer

  def start_link(_) do
    GenServer.start_link(__MODULE__, [], name: __MODULE__)
  end

  def init(_) do
    :net_kernel.monitor_nodes(true)

    {neighbors, []} = :rpc.multicall(Node.list, Process, :whereis, [AccountLookupCache])
    DeltaCrdt.add_neighbours(AccountLookupCache, neighbors)

    {:ok, []}
  end

  # Callbacks

  def handle_info({:nodeup, node}, state) do
    handle_info({:retrynodeup, node, 0}, state)
  end

  def handle_info({:retrynodeup, node, count}, state) do
    pid = :rpc.call(node, Process, :whereis, [AccountLookupCache])

    if pid == nil do
      IO.puts &amp;quot;Node is up, but app not booted, retry = #{count}&amp;quot;
      Process.send_after(self(), {:retrynodeup, node, count + 1}, 500)
    else
      IO.puts &amp;quot;Node is now up #{node} #{inspect(pid)}&amp;quot;
      DeltaCrdt.add_neighbours(AccountLookupCache, [pid])
    end

    {:noreply, state}
  end

  def handle_info({:nodedown, _node}, state) do
    {:noreply, state}
  end
end

usage:

DeltaCrdt.read(AccountLookupCache)
DeltaCrdt.mutate(AccountLookupCache, :add, [&amp;quot;1111/2&amp;quot;, &amp;quot;4&amp;quot;])
DeltaCrdt.read(AccountLookupCache)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a pretty slick library and the setup was fairly simple. This code may have some edge cases in it but we ran into
some performance issues with larger data sets and moved onto another solution. While we did end up moving on from it, the
author has been working hard on a refactor to improve the speed of the data structure. What&amp;rsquo;s he is doing is way beyond my
understanding of CRDTs and is pretty inspiring open-source work.&lt;/p&gt;

&lt;p&gt;This issue got me thinking and I realized that I didn&amp;rsquo;t need a lot of the benefits of the CRDT. I wanted replication across
the cluster, but my values are stable and so time variations won&amp;rsquo;t be a factor. I would most likely have stuck with DeltaCRDT
at this point if that was a factor, but I ended up moving onto my current solution.&lt;/p&gt;

&lt;h1&gt;Cachex + pg2 Replication&lt;/h1&gt;

&lt;p&gt;My final solution involves a tried and true solution around Cachex + pg2. I&amp;rsquo;ve &lt;a href="/2018/02/17/pg2-basics-use-process-groups-for-orchestration-across-a-cluster.html" target="_blank"&gt;written about pg2&lt;/a&gt;
in the past and have used it successfully in production on several projects. It essentially lets us
place our cache processes in a group across the cluster and reference the remote cache processes as a pid list.&lt;/p&gt;

&lt;p&gt;The solution presented utilizes Cachex for all local set/get/stat, and passes messages containing
sets to the cluster using &lt;code&gt;send&lt;/code&gt;. When the cache process starts it notifies all neighbors in pg2
that it would like a dump of the state and then writes that into Cachex. Here are some strengths
and weaknesses of the solution:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;+ Boot based (rather than shutdown) based replication so that nodes do not lose data when they go down&lt;/li&gt;
&lt;li&gt;+ Cachex for local cache management (so we get all of that library&amp;rsquo;s benefits)&lt;/li&gt;
&lt;li&gt;+ Efficient writing and loading of an export. In testing it took less than 3 seconds for a 1,000,000 key cache locally (higher across network)&lt;/li&gt;
&lt;li&gt;- The entire cluster being down will cause cache data loss and an increase in misses&lt;/li&gt;
&lt;li&gt;- There is no consensus on what the right cache is, it&amp;rsquo;s best attempt&lt;/li&gt;
&lt;li&gt;- Possible flood of binary across network on boot with many nodes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The biggest disadvantage is the last one and I think it will be fixed before I take this code into production. It
is a purely effort based blocker (I need to write the code) and conceptually will work just fine.&lt;/p&gt;

&lt;p&gt;The code is on a gist due to being fairly long: https://gist.github.com/sb8244/371335946d444bd8c5786571cacef4d6&lt;/p&gt;

&lt;p&gt;The end result is a very simple set/get interface which handles all of the distribution and caching.
I did a few basic performance tests of the system by throwing 10k, 50k, 500k, 1000k k/v
pairs into the cache and seeing how it performed. Writes and distribution were incredibly fast and rebooting
the application caused cache availability within a few seconds, well before the app would finish booting for
kubernetes health checks. There was one caveat I noticed which is that memory usage spiked while loading
the dumps from remote servers. I believe that the best solution here will involve me changing to a system
that collects the size of each remote cache and selects the top 1 or 2 sized caches. That will prevent N
servers from sending full cache dumps and only 1 or 2.&lt;/p&gt;

&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;In summary, three different potential solutions were evaluated for this distributed caching problem. While
the first two options utilize great libraries and would be possible to build on, the trade-offs were too much
for the simplicity of my needs.&lt;/p&gt;

&lt;p&gt;When working on a software project like this, considering what your exact needs are is important and may actually
lead you away from the typical libraries into a different solution. It may seem obvious for many, but it is very
easy to get caught up in the libraries that we&amp;rsquo;re hearing about rather than what is best for our particular use case.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Thanks for reading! I&amp;rsquo;ll be speaking at &lt;a href="https://lonestarelixir.com/2019/" target="_blank"&gt;Lonestar ElixirConf&lt;/a&gt; about bringing Elixir to production,
looking at both human and tech challenges in doing so.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Understanding Compile Time Dependencies in Elixir - A Bug Hunt</title>
    <link rel="alternate" href="https://stephenbussey.com/2019/01/03/understanding-compile-time-dependencies-in-elixir-a-bug-hunt.html"/>
    <id>https://stephenbussey.com/2019/01/03/understanding-compile-time-dependencies-in-elixir-a-bug-hunt.html</id>
    <published>2019-01-02T21:59:00-05:00</published>
    <updated>2019-01-11T16:18:07-05:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;This blog post will cover a fairly trivial but still interesting problem that I encountered at work today. I think it&amp;rsquo;s worth writing about simply because it&amp;rsquo;s a bit non-obvious and will probably happen to other people as well. It also is a good time to reinforce our understanding of how the Elixir compiler works and why the order of things matters.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;The issue started after upgrading one of our repositories to the latest Elixir version (from 1.6) and upgrading Distillery from 1.5 -&amp;gt; 2.0. The problem manifested itself as our instrumentation disappearing in DataDog. The graphs was there before the deployment, then immediately became empty after.&lt;/p&gt;

&lt;p&gt;The immediate thought is that something could have been wrong with 1.7 or the upgrade libraries (and not something we did). However, this seems unlikely given that Elixir 1.7 has been out in the wild for a while now and has had time to get any kinks worked out (and something this major would be unlikely anyways). We power our instrumentation with &lt;a href="https://github.com/discordapp/instruments/" target="_blank"&gt;Instruments&lt;/a&gt; as seen in a &lt;a href="/2018/09/24/elixir-probes-replacing-elixometer.html" target="_blank"&gt;past post&lt;/a&gt; and so we verified that the application was started and configured with all of our probes&amp;hellip;everything was visibly okay on this front.&lt;/p&gt;

&lt;p&gt;After confirming that instruments should be working, we noticed that DataDog was reporting some stats for our application that looked an awful lot like what we wanted&amp;hellip;it was just missing our &amp;ldquo;.probes&amp;rdquo; namespace. The problem isn&amp;rsquo;t that the stats weren&amp;rsquo;t reporting, but that they were reporting without our &lt;code&gt;probe_prefix&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;The Code Problem&lt;/h2&gt;

&lt;p&gt;Inside of Instruments, there is a line of code which loads the provided probe prefix and places it into a module attribute:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# https://github.com/discordapp/instruments/blob/89a620d181ba4a04ed0ac01c47057c018d645428/lib/probe/definitions.ex#L11
@probe_prefix Application.get_env(:instruments, :probe_prefix)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This line of code happens inside of a &lt;code&gt;defmodule&lt;/code&gt;, which means that it is read from at compile time and not at run-time. It would be possible to use &lt;code&gt;Application.get_env/2&lt;/code&gt; at run-time using something like &lt;code&gt;def probe_prefix&lt;/code&gt;, but there is nothing wrong with the module attribute approach.&lt;/p&gt;

&lt;p&gt;In Distillery 1, all config was provided at compile time and &lt;code&gt;probe_prefix&lt;/code&gt; would always be present. In Distillery 2 (with the awesome new config providers), config can optionally be provided at run time; this simplifies and fixes quite a few deployment quirks. Our standard for Distillery 2 config providers has been to move &lt;code&gt;config/prod.exs&lt;/code&gt; to &lt;code&gt;rel/config/runtime.exs&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;The Fix&lt;/h2&gt;

&lt;p&gt;In hindsight, we should not have moved the entire contents of &lt;code&gt;config/prod.exs&lt;/code&gt; into our config provider. As Paul mentions in his &lt;a href="https://dockyard.com/blog/2018/08/23/announcing-distillery-2-0" target="_blank"&gt;release blog post&lt;/a&gt;, &amp;ldquo;You can still use all of the config files under config/ in your project, but you should use those for compile-time config and default values only&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;The fix is as simple as making sure our &lt;code&gt;runtime.exs&lt;/code&gt; file only has dynamic values in it. This means that we provide a &lt;code&gt;config/prod.exs&lt;/code&gt; file for static configuration, and a &lt;code&gt;rel/config/runtime.exs&lt;/code&gt; file for dynamic configuration.&lt;/p&gt;

&lt;h2&gt;Compilation vs Run-time&lt;/h2&gt;

&lt;p&gt;The reason that I liked this small bug is that it reinforces understanding of compilation and how we can very quickly end up blurring the line between run-time and compile-time. As users and authors of code, we have to be cognizant of where are values are coming from and where provided values are going. It often will be perfectly &amp;ldquo;okay&amp;rdquo; to do things incorrectly (because there are many situations where it wouldn&amp;rsquo;t be a problem), but it can be difficult to track down when it is not okay. In particular, this issue looked like &lt;code&gt;probe_prefix&lt;/code&gt; was setup correctly, and only become clear through reading of the source code that it was not setup correctly.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Thanks for reading! I&amp;rsquo;ll be speaking at &lt;a href="https://lonestarelixir.com/2019/" target="_blank"&gt;Lonestar ElixirConf&lt;/a&gt; about bringing Elixir to production, looking at both human and tech challenges in doing so.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Announcing PushEx: Open-Source WebSocket Push Server in Elixir</title>
    <link rel="alternate" href="https://stephenbussey.com/2018/12/02/announcing-pushex-open-source-websocket-push-server-in-elixir.html"/>
    <id>https://stephenbussey.com/2018/12/02/announcing-pushex-open-source-websocket-push-server-in-elixir.html</id>
    <published>2018-12-02T15:00:00-05:00</published>
    <updated>2019-01-02T21:43:58-05:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;I&amp;rsquo;m happy to announce the open-sourcing of a project that I&amp;rsquo;ve been putting a good deal of time and effort into, &lt;a href="https://github.com/pushex-project/pushex" target="_blank"&gt;PushEx&lt;/a&gt;. PushEx is a WebSocket push server built in Elixir; it allows you to send payloads from your servers to your users in real-time. In this post, I will go in-depth about the problem PushEx solves, why I&amp;rsquo;m excited about it, and why Elixir is perfect for the problem space.&lt;/p&gt;

&lt;h1&gt;The Problem&lt;/h1&gt;

&lt;p&gt;As consumers of an online web application use the application, asynchronous processes may take place that the user needs alerted to. Some examples of this include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Notifying a user when they get a new message&lt;/li&gt;
&lt;li&gt;Alerting the user when their multi-minute import finishes&lt;/li&gt;
&lt;li&gt;Updating the frontend website as backend data models are updated&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;WebSockets allow an elegant solution to this problem because they enable the backend servers to maintain a direct connection to the user. The servers can then send data to the connected users nearly instantly.&lt;/p&gt;

&lt;p&gt;However, WebSockets bring about their own problems. The web server must be capable of handling all of the connections and keeping them alive every so often. In a production deployment, multiple servers must also be able to have knowledge about each other, because the user may be connected to server A but the request for a push goes through server B. Phoenix and Elixir, the foundation that PushEx leverages, provide excellent solutions to these problems.&lt;/p&gt;

&lt;h1&gt;Elixir, A Great Solution&lt;/h1&gt;

&lt;p&gt;Elixir is an excellent language for solving the problem of WebSockets. Some properties of Elixir that are great for this problem are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Parallel processing power for efficient handling of many connections&lt;/li&gt;
&lt;li&gt;Built-in networking between multiple servers in a deployment&lt;/li&gt;
&lt;li&gt;Ability to hold very large sets of processes (supports many connections)&lt;/li&gt;
&lt;li&gt;Incremental garbage collection per process (many connections won&amp;rsquo;t freeze at the same time)&lt;/li&gt;
&lt;li&gt;Error separation between processes (connections won&amp;rsquo;t crash each other)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Phoenix leverages the power of Elixir to provide an excellent WebSocket solution, and PushEx provides these same benefits because it is built using Phoenix Channels. In fact, Phoenix has been benchmarked at being able to support over 2 million connections on a single large box. While this is a fun benchmark, real world implementations will often have many servers networked together for efficiency, scalability, and redundancy. Phoenix provides a solution to the problems that come with distributed servers through &amp;ldquo;Presence&amp;rdquo;, a mathematically efficient solution to knowing the state of the distributed WebSockets.&lt;/p&gt;

&lt;h1&gt;Challenging to Bring to Production&lt;/h1&gt;

&lt;p&gt;Despite the effectiveness of Elixir and Phoenix for WebSockets, developing a production-ready push server is not a trivial task. Throughout the development of PushEx, an &lt;a href="https://github.com/phoenixframework/phoenix/pull/3141" target="_blank"&gt;improvement&lt;/a&gt; was found/developed to improve Phoenix&amp;rsquo;s JS client availability and the properties of many simultaneous connections were &lt;a href="https://elixirforum.com/t/phoenix-presence-mailbox-full/15139/13" target="_blank"&gt;examined&lt;/a&gt;. This work helps make PushEx production-ready and scalable to a large number of clients.&lt;/p&gt;

&lt;p&gt;While there are large companies using WebSockets in production successfully, it is easy to see that small details can be missed. The goal with PushEx is to identify, solve, document, and educate on these small details.&lt;/p&gt;

&lt;p&gt;PushEx seeks to make developing your production-ready push server easy! The &lt;a href="https://hexdocs.pm/push_ex/standalone.html#content" target="_blank"&gt;Getting Started - Standalone Installation guide&lt;/a&gt; allows you to get up and running in less than 10 steps, with only a bit of application-specific code needing developed.&lt;/p&gt;

&lt;h1&gt;Using PushEx&lt;/h1&gt;

&lt;p&gt;PushEx has been designed to be easy to setup and will be comfortable for Elixir developers. There are, however, quite a few guides written that will help make the process of developing and deploying PushEx easier. You can find these guides on &lt;a href="https://hexdocs.pm/push_ex/readme.html" target="_blank"&gt;hexdocs.pm&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;PushEx is currently in a 1.0.0 release candidate. An implementation has been developed for use in production at SalesLoft, and will be tested there within 30 days. Once it is in production, the 1.0.0 release will be shipped. I do not anticipate much changing between now and then, as the core system is similar to what is already in use on another internal project.&lt;/p&gt;

&lt;h1&gt;Giving Back to Elixir Community&lt;/h1&gt;

&lt;p&gt;Elixir and Phoenix are providing the solid backbone of PushEx and help to make it great. To that end, I want to invite the community to ask any questions they have on the PushEx project so that they may have some of their challenges solved. Please feel free to ask &lt;strong&gt;any&lt;/strong&gt; question on the &lt;a href="https://github.com/pushex-project/pushex/issues" target="_blank"&gt;PushEx Issue Tracker&lt;/a&gt; regarding any code or ideas in the project. I will personally read and answer each one to help provide back to the community.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Explore PushEx on:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/pushex-project" target="_blank"&gt;Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://hex.pm/packages/push_ex" target="_blank"&gt;Hex&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  <entry>
    <title>Elixir Probes - Replacing Elixometer</title>
    <link rel="alternate" href="https://stephenbussey.com/2018/09/24/elixir-probes-replacing-elixometer.html"/>
    <id>https://stephenbussey.com/2018/09/24/elixir-probes-replacing-elixometer.html</id>
    <published>2018-09-23T21:44:00-04:00</published>
    <updated>2019-01-02T21:43:58-05:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;It&amp;rsquo;s been a while since my last post. I&amp;rsquo;ve been chipping away at Elixir still, fortunately, focusing on making sure that other teams at SalesLoft are best equipped to build Elixir microservices. One of the most common items that need implemented on every Elixir project is a set of Datadog metrics. These metrics provide both VM health and application specific info.&lt;/p&gt;

&lt;h1&gt;Elixometer&lt;/h1&gt;

&lt;p&gt;Until now, all apps at SalesLoft have shipped with using Elixometer as an instrumentation service in our Elixir apps. Elixometer has a variety of methods that allow collection of stats and can report to a StatsD server. It also has a variety of problems:&lt;/p&gt;

&lt;h4&gt;1. The mix.exs entry for Elixometer is quite involved.&lt;/h4&gt;

&lt;p&gt;There seem to be some incompatiblies in the various libraries that have been released over time. Here is my current Elixometer mix.exs entry:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# start exometer; force &amp;quot;correct&amp;quot; modules due to elixometer not compiling properly
{:elixometer, &amp;quot;~&amp;gt; 1.2&amp;quot;},
{:lager, &amp;quot;&amp;gt;= 3.2.1&amp;quot;, override: true},
{:exometer, github: &amp;quot;Feuerlabs/exometer&amp;quot;},
{:exometer_core, &amp;quot;~&amp;gt;1.4.0&amp;quot;, override: true},
{:amqp_client, git: &amp;quot;https://github.com/dsrosario/amqp_client.git&amp;quot;, branch: &amp;quot;erlang_otp_19&amp;quot;, override: true},
# end exometer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I haven&amp;rsquo;t even tried updating these because it took me several days to get a working config.&lt;/p&gt;

&lt;h4&gt;2. Elixometer includes libraries that are outside of instrumentation.&lt;/h4&gt;

&lt;p&gt;For instance, &lt;code&gt;lager&lt;/code&gt; is a listed dependency. This may not manifest as a problem in your project, but it could also be hiding from you. I discovered that including lager&amp;rsquo;s error logger module would clear out all of the other SASL error loggers, which is how Bugsnag was included. This meant that Bugsnag was being forcibly removed without my knowledge. The solution here was to disable lager&amp;rsquo;s error logger.&lt;/p&gt;

&lt;h4&gt;3. Elixometer is fairly complex to setup and use.&lt;/h4&gt;

&lt;p&gt;This has been a complaint both across the team and also on my own projects.&lt;/p&gt;

&lt;p&gt;I once wanted to report a simple count to Datadog and plot a rate of change. After 1-2 days of trying to figuring out why it was not working, I discovered that all stats report as gauges unless a complex setup is used to specify the reporting type&amp;hellip;I ended up using a different StatsD library (Statix) at that point.&lt;/p&gt;

&lt;h1&gt;Instruments - Elixometer&amp;rsquo;s Replacement&lt;/h1&gt;

&lt;p&gt;Going forward, I will be using the &lt;a href="https://github.com/discordapp/instruments" target="_blank"&gt;Instruments&lt;/a&gt; library to report probed metrics to Datadog. I will be using &lt;a href="https://github.com/lexmag/statix" target="_blank"&gt;Statix&lt;/a&gt; to report non-probed application metrics to Datadog.&lt;/p&gt;

&lt;p&gt;A probe is a bit of code that runs on a defined interval and reports the statistics on each run. An example of this is asking for VM memory utilization every 1s and sending that to StatsD. Instruments makes defining probes very easy, and I&amp;rsquo;m going to share my standard configuration.&lt;/p&gt;

&lt;h2&gt;Instruments Setup&lt;/h2&gt;

&lt;p&gt;I followed the guide on Github to setup Instruments in my application. Outside of the recommended config, I did find that reporting to the standard &lt;code&gt;Logger&lt;/code&gt; module during testing makes a ton of sense. To do that, I placed the following in &lt;code&gt;config/text.exs&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;config :instruments, reporter_module: Instruments.StatsReporter.Logger
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;My other config looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;config :statix,
  prefix: &amp;quot;okr_app.#{Mix.env}&amp;quot;,
  host: System.get_env(&amp;quot;STATSD_HOST&amp;quot;),
  port: String.to_integer(System.get_env(&amp;quot;STATSD_PORT&amp;quot;) || &amp;quot;8125&amp;quot;),
  disabled: System.get_env(&amp;quot;STATSD_HOST&amp;quot;) == nil

config :instruments,
  fast_counter_report_interval: 100,
  probe_prefix: &amp;quot;probes&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Instruments Probe Definition&lt;/h2&gt;

&lt;p&gt;Probes are defined in Instruments a bit differently than in Elixometer. Elixometer utilizes a static configuration, but I cannot find such an option for Instruments. I defined probes in my &lt;code&gt;application.ex&lt;/code&gt; file:&lt;/p&gt;

&lt;pre&gt;&lt;code class="elixir"&gt;def setup_probes() do
  # I allow instruments to be disabled as this is an open source application and StatsD isn&amp;#39;t required
  if Application.get_env(:instruments, :disabled) != true do
    {:ok, _} = Application.ensure_all_started(:instruments)
    interval = 1_000

    Instruments.Probe.define(
      &amp;quot;erlang.process_count&amp;quot;,
      :gauge,
      mfa: {:erlang, :system_info, [:process_count]},
      report_interval: interval
    )

    Instruments.Probe.define(
      &amp;quot;erlang.memory&amp;quot;,
      :gauge,
      mfa: {:erlang, :memory, []},
      keys: [:total, :atom, :processes],
      report_interval: interval
    )

    Instruments.Probe.define(
      &amp;quot;erlang.statistics.run_queue&amp;quot;,
      :gauge,
      mfa: {:erlang, :statistics, [:run_queue]},
      report_interval: interval
    )

    Instruments.Probe.define(
      &amp;quot;erlang.system_info.process_count&amp;quot;,
      :gauge,
      mfa: {:erlang, :system_info, [:process_count]},
      report_interval: interval
    )
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Instruments&amp;rsquo; documentation discusses how custom probes can be designed for your application specifically. In addition to these probes, you can utilize Statix library to send StatsD metrics. That is outside of the scope of this post, but it is useful to note that Instruments defines &lt;a href="https://github.com/discordapp/instruments/blob/master/lib/instruments.ex#L78" target="_blank"&gt;various functions&lt;/a&gt; for sending data to the underlying Statix module. You can also use &lt;code&gt;Instruments.Statix&lt;/code&gt; directly, if that suits your needs better.&lt;/p&gt;

&lt;h1&gt;Final Thoughts&lt;/h1&gt;

&lt;p&gt;I haven&amp;rsquo;t seen any downside with Instruments yet that would make me use Elixometer again. It seems much easier to setup, is more obvious to read, and doesn&amp;rsquo;t involve interfacing with an erlang module in a sometimes confusing fashion. Due to all of this, it seems like a great package for setting up StatsD probes.&lt;/p&gt;

&lt;p&gt;I hinted at an open source implementation that uses Instruments. I&amp;rsquo;m working on putting the final touches on the repo and open-sourcing it under the SalesLoft Github org. There will be a blog post when that happens, as I&amp;rsquo;ve been trying to utilize best practices (of the current moment) when building it.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Elixir Memory - Not Quite Free</title>
    <link rel="alternate" href="https://stephenbussey.com/2018/05/09/elixir-memory-not-quite-free.html"/>
    <id>https://stephenbussey.com/2018/05/09/elixir-memory-not-quite-free.html</id>
    <published>2018-05-08T23:32:00-04:00</published>
    <updated>2019-01-02T21:43:58-05:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;I have been working on a shipping an Elixir service at SalesLoft to replace an existing
piece of functionality in our system with a better version. One of the core changes
is that the websocket communication of this system will be maintained by Elixir rather
than by Pusher (our Rails goto). This post is going to explore some of the surprises
and valuable lessons that I gained while debugging memory leaks in the service.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;I was doing a usual check-in on the service and noticed that the memory for only ~200
websockets connected was peaking out at over 550MB. This seemed off and meant that
connecting all users to the service would take many GB of memory. I was at a loss for
what the problem could be though, but found some help in a useful diagnostic tool.&lt;/p&gt;

&lt;h2&gt;Diagnosing Memory Culprit&lt;/h2&gt;

&lt;p&gt;When doing local development, it&amp;rsquo;s very easy to pop open &lt;code&gt;:observer.start&lt;/code&gt; and get
a wonderful interface to sift through processes and their memory consumption. However,
exposing this in production is much more difficult (too difficult in my setup to bother).
I found the tool &lt;a href="https://github.com/zhongwencool/observer_cli" target="_blank"&gt;observer_cli&lt;/a&gt; and have to
say that it is one of the greatest tools in my toolbox right now. It is based on the
well known &lt;code&gt;recon&lt;/code&gt; library, but provides a nice command line interface to visually
see and sort through process listings. I have to give props to the library maintainer
for implementing a feature I requested in a matter of days.&lt;/p&gt;

&lt;p&gt;As I was looking at the observer_cli output, I noticed that Phoenix Channel Servers were
appearing all over the top memory utilizers. Some would take up to 4MB, but averaged at about
1 MB. For 1000 sockets, this would be between 1GB - 4GB of memory!&lt;/p&gt;

&lt;h2&gt;Understanding Erlang Memory&lt;/h2&gt;

&lt;p&gt;One of my favorite posts on Erlang is by Hamidreza Soleimani, &lt;a href="https://hamidreza-s.github.io/erlang%20garbage%20collection%20memory%20layout%20soft%20realtime/2015/08/24/erlang-garbage-collection-details-and-why-it-matters.html" target="_blank"&gt;Erlang Garbage Collection Details&lt;/a&gt;.
This post goes over an important detail of how the 2-part Erlang GC works, young and old generational
heaps. The gist of it is that the major GC operation can collect both young and old heaps,
but is invoked infrequently as it is a &amp;ldquo;stop the process&amp;rdquo; type of GC. The minor GC
operation can only collect young heap items, and will mark items as old if they survive
a GC pass. What does this mean, though?&lt;/p&gt;

&lt;p&gt;In the context of a request, it is possible for an operation to take multiple
GC minor sweeps and still be referencing all of the allocated binaries (data). When
this happens, those binaries are going to be marked as old heap and require that
a full sweep happens to GC them. If the full sweep only happens in certain situations,
it is possible for the situation to not occur and the full sweep doesn&amp;rsquo;t occur. In
this case, we have a memory leak. This is exactly what was happening to my websocket.&lt;/p&gt;

&lt;p&gt;It is possible to trigger a GC on the entire node to test out if there is a possible
memory leak. Note that you don&amp;rsquo;t want to do this in a regular fashion and running
the major GC may make your debugging sessions less valuable until time passes.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# don&amp;#39;t do this often
Process.list() |&amp;gt; Enum.each(&amp;amp;:erlang.garbage_collect/1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Addressing this Memory Leak&lt;/h2&gt;

&lt;p&gt;The websocket request that I&amp;rsquo;ve been discussing was new and a bit different than
what I&amp;rsquo;ve done in the past. It is an interface to our &lt;a href="https://developers.salesloft.com/api.html" target="_blank"&gt;API&lt;/a&gt; and allows requests
to be made on the open connection. Due to it being an API request, it would return
up to 100 items at a time and require multiple pieces of data sourced from other
services to operate. This equates to 2 things: memory (1-4 MB) and time (multiple minor sweeps).&lt;/p&gt;

&lt;p&gt;&lt;a href="https://s3.us-east-2.amazonaws.com/ferd.erlang-in-anger/text.v1.1.0.pdf" target="_blank"&gt;Erlang in Anger 7.2.2&lt;/a&gt; mentions 5 different ways to fix a memory leak:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;call garbage collection manually at given intervals (icky, but somewhat efficient)&lt;/li&gt;
&lt;li&gt;stop using binaries (often not desirable)&lt;/li&gt;
&lt;li&gt;use binary:copy/1-2 if keeping only a small fragment (usually less than 64 bytes) of a larger binary&lt;/li&gt;
&lt;li&gt;move work that involves larger binaries to temporary one-off processes that will die when they’re done (a lesser form of manual GC!)&lt;/li&gt;
&lt;li&gt;or add hibernation calls when appropriate (possibly the cleanest solution for inactive processes)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;The above is copied from Erlang in Anger verbatim.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For the work that this websocket request was doing, it made most sense to utilize
a short lived process to execute and respond to the request.&lt;/p&gt;

&lt;p&gt;The initial pass at the fix involved using &lt;code&gt;Task.async&lt;/code&gt; and awaiting the response.
However, this proved to be even worse on memory because sending the response over
the process barrier was causing the same leak. The solution here ended up being to
utilize &lt;a href="https://hexdocs.pm/phoenix/Phoenix.Channel.html#socket_ref/1" target="_blank"&gt;&lt;code&gt;Phoenix.Channel socket_ref/1&lt;/code&gt;&lt;/a&gt;
and responding to the socket request in the &lt;code&gt;Task.start&lt;/code&gt;d process using &lt;code&gt;Phoenix.Channel.reply(ref, reply)&lt;/code&gt;.
The &lt;code&gt;socket_ref&lt;/code&gt; function is very useful and has some nice side effects. Due to
serialization of processes, the original approach would block access to the socket
during a request. With the new approach, the socket can handle multiple requests
at the same time.&lt;/p&gt;

&lt;h2&gt;Results from Fix&lt;/h2&gt;

&lt;div style="text-align: center"&gt;
  &lt;img src="/images/erlang-memory/step-1.png" alt="memory drop from 550MB to 250MB" /&gt;
  &lt;div&gt;
    &lt;small&gt;
      &lt;i&gt;Large fix from short lived process&lt;/i&gt;
    &lt;/small&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The results from this code change were immediate and apparent. The Phoenix.Channel.Server
processes that were from 1MB - 4MB were now at 30KB - 60KB. This lead to the massive drop
in overall memory as seen above.&lt;/p&gt;

&lt;h2&gt;Rinse and Repeat&lt;/h2&gt;

&lt;p&gt;As more sockets were connected to the system, it became clear there was still a memory
leak. By utilizing the &lt;code&gt;observer_cli&lt;/code&gt; tool, it was possible to see that the cowboy
websocket processes were hovering at 1-2MB each. Upon discussion in the community Slack, it turns
out that encoding the large payload suffers from the same type of memory leak that
was mentioned previously. However, the fix is less optimal due to that code not
being written by us.&lt;/p&gt;

&lt;p&gt;It appeared that triggering a major GC was the best option.
Phoenix even accommodates this with a special &lt;a href="https://hexdocs.pm/phoenix/Phoenix.Transports.WebSocket.html#module-garbage-collection" target="_blank"&gt;&lt;code&gt;:garbage_collect&lt;/code&gt;&lt;/a&gt;
message handler, which is marked as a solution for use after processing large messages.
We ended up triggering this 5s after the response of our large payload.&lt;/p&gt;

&lt;div style="text-align: center"&gt;
  &lt;img src="/images/erlang-memory/step-2.png" alt="memory drop from 350MB to 210MB" /&gt;
  &lt;div&gt;
    &lt;small&gt;
      &lt;i&gt;Large fix from manual GC&lt;/i&gt;
    &lt;/small&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;This memory usage is from significantly more connected sockets than step 1, and we can
clearly see how large of an impact the manual GC had. The memory is now predictable and
stable for connected sockets.&lt;/p&gt;

&lt;h2&gt;Final Thoughts&lt;/h2&gt;

&lt;p&gt;This is a very constrained use case, although possibly common, for memory leaks of
Phoenix websockets. However, the same principles apply to all of the processes we spawn
in Elixir. When we have a process, and especially with a large number of processes, it
is important to think about the life cycle of it and how it will play into garbage collection.
As our processes become longer lived, this becomes even more important as our systems
will be leaking memory over longer periods of time.&lt;/p&gt;

&lt;p&gt;It seems easiest to just slap a full system GC into every process to keep memory usage
low (and we can do that if desired), but there are other techniques related to process
lifecycle and memory consumption that may be more effective in the end.&lt;/p&gt;

&lt;p&gt;Again, I recommend reading &lt;a href="https://s3.us-east-2.amazonaws.com/ferd.erlang-in-anger/text.v1.1.0.pdf" target="_blank"&gt;Erlang in Anger 5, 7&lt;/a&gt;
and &lt;a href="https://hamidreza-s.github.io/erlang%20garbage%20collection%20memory%20layout%20soft%20realtime/2015/08/24/erlang-garbage-collection-details-and-why-it-matters.html" target="_blank"&gt;Erlang Garbage Collection Details&lt;/a&gt;
for more detailed information.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Introducing Elixir Response Snapshot Testing</title>
    <link rel="alternate" href="https://stephenbussey.com/2018/04/17/introducing-elixir-response-snapshot-testing.html"/>
    <id>https://stephenbussey.com/2018/04/17/introducing-elixir-response-snapshot-testing.html</id>
    <published>2018-04-16T21:53:00-04:00</published>
    <updated>2019-01-02T21:43:58-05:00</updated>
    <author>
      <name>Stephen Bussey</name>
    </author>
    <content type="html">&lt;p&gt;I&amp;rsquo;m excited to introduce an Elixir testing library that I&amp;rsquo;ve been working on, as well
as to explain the general ideas behind it. The library is called Response Snapshot and
can be found on &lt;a href="https://hex.pm/packages/response_snapshot" target="_blank"&gt;hex&lt;/a&gt;. The work is based on
my &lt;a href="https://github.com/SalesLoft/rspec-rcv" target="_blank"&gt;rspec-rcv&lt;/a&gt; gem which provides a very similar
interface and seeks to achieve the same goals.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://hex.pm/packages/response_snapshot" target="_blank"&gt;hex.pm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sb8244/elixir_response_snapshot" target="_blank"&gt;github.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;What is snapshot testing?&lt;/h1&gt;

&lt;p&gt;I&amp;rsquo;m borrowing this term from Jest, but I think that the concept is fairly simple
and true to the name. Snapshot testing is testing an API response by collecting
a valid snapshot of the output and comparing future tests against it. The entire
API response can be asserted against with a single line of code, and visually verified
as correct by a human.&lt;/p&gt;

&lt;p&gt;In addition to being able to quickly test response outputs and ensure their shape
doesn&amp;rsquo;t change over time, the snapshots can be utilized by frontend specs or other
client systems as an example of valid output. If the frontend specs always use the
most recent snapshots, any changes to the backend which would break the frontend
should fail when the frontend specs run.&lt;/p&gt;

&lt;h1&gt;Where does snapshot testing fall short?&lt;/h1&gt;

&lt;p&gt;I would hazard a guess that the idea of snapshot testing makes TDD practitioners
cringe a bit. In particular, a human has to verify that the snapshot is accurate
and then commit that into source control. Humans are prone to errors and so this
could be considered worse than standard tests on a response. I do think that there
is a place for response assertions in addition to a snapshot, although snapshots
significantly lower the barrier to entry on testing responses. Without snapshot testing,
the tests for a response can end up being long, redundant, and a chore to update.&lt;/p&gt;

&lt;p&gt;One of the big challenges in snapshot testing is handling values that change between
test runs. This is really common with ids, dates, and information generated from
a library like faker. ResponseSnapshot introduces an optional &lt;code&gt;:keys&lt;/code&gt; mode which
will not error out if any values are modified. This is most useful when the shape
of the data should be asserted, but not the exact values. Addition and removal of
keys would be an error in this mode. Another solution to avoid using keys mode is
ignored keys. With this, specific path value changes can be ignored. Paths can be
absolute or wildcards.&lt;/p&gt;

&lt;h1&gt;Getting started in Elixir&lt;/h1&gt;

&lt;p&gt;To get started in Elixir, follow the &lt;a href="https://github.com/sb8244/elixir_response_snapshot#installation" target="_blank"&gt;installation instructions&lt;/a&gt;.
Capturing your first test involves invoking &lt;code&gt;store_and_compare!/2&lt;/code&gt;. Let&amp;rsquo;s see an
example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;test &amp;quot;widgets are listed out&amp;quot;, %{conn: conn} do
  conn
  |&amp;gt; get(&amp;quot;/api/widgets&amp;quot;)
  |&amp;gt; json_response(200)
  |&amp;gt; ResponseSnapshot.store_and_compare!(path: &amp;quot;widgets/index.json&amp;quot;)
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In practice, you will want to setup your fixture path base and ignore keys that
change between test runs (such as ids and dates):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;config :response_snapshot,
  path_base: &amp;quot;test/fixtures&amp;quot;,
  ignored_keys: [
    {&amp;quot;id&amp;quot;, :any_nesting},
    {&amp;quot;created_at&amp;quot;, :any_nesting},
  ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output of the test run will be a fixture file located at &lt;code&gt;test/fixtures/widgets/index.json&lt;/code&gt;.
This file will include the test file path, the recording time, and the JSON response
of the API.&lt;/p&gt;

&lt;h1&gt;Next steps&lt;/h1&gt;

&lt;p&gt;For next steps, I&amp;rsquo;d like to bring some best practices to the library, such as using
Dialyzer. However, the core of the library seems to be operating as expected and includes
a lot of lessons learned from rspec-rcv over the past 2 years. Please reach out
on Github issues if you see anything you&amp;rsquo;d like fixed or have any questions!&lt;/p&gt;
</content>
  </entry>
</feed>
